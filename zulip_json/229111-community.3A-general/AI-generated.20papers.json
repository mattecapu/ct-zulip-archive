[
    {
        "content": "<p>I hate to do this, but the four arXiv papers by this author, uploaded in the last week, all look wholly AI-generated to me: <a href=\"https://arxiv.org/search/?query=Reizi&amp;searchtype=author\">https://arxiv.org/search/?query=Reizi&amp;searchtype=author</a></p>",
        "id": 507901615,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1742862792
    },
    {
        "content": "<p>In particular the appendix of <a href=\"https://arxiv.org/abs/2503.16555\">https://arxiv.org/abs/2503.16555</a> there is a promise of proofs, examples and so on, but the entire text of the appendix is this:</p>\n<blockquote>\n<p>In this appendix, we present additional proofs, detailed calculations, and further examples<br>\nthat complement the results in the main text. In particular, the appendix includes:  <br>\n* A complete proof of the back-and-forth construction used in Lemma 5.8.  <br>\n* Detailed verifications of the functoriality of the Henkin and compactness-based model constructions.<br>\n* Concrete examples illustrating the construction of models for specific theories.  </p>\n<p>These supplementary materials are provided to offer deeper insight into the technical details and to demonstrate how our unified framework can be applied to various logical systems.</p>\n</blockquote>\n<p>The next text is the bibliography and that's it. The content is also extremely banal.</p>",
        "id": 507901831,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1742862941
    },
    {
        "content": "<p>After a cursory inspection of <a href=\"https://arxiv.org/abs/2503.16570\">https://arxiv.org/abs/2503.16570</a>, I agree.</p>",
        "id": 507940187,
        "sender_full_name": "Chad Nester",
        "timestamp": 1742885332
    },
    {
        "content": "<p>I can't find any information about this supposed person online except an affiliation via their email, but I've made a report to the Arxiv.</p>",
        "id": 507946343,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1742888034
    },
    {
        "content": "<p><a href=\"/user_uploads/21317/2XH9nNv7Uy7EppX4uo7NT3lQ/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/21317/2XH9nNv7Uy7EppX4uo7NT3lQ/image.png\" title=\"image.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"991x806\" src=\"/user_uploads/thumbnail/21317/2XH9nNv7Uy7EppX4uo7NT3lQ/image.png/840x560.webp\"></a></div><p>yep, no way a human wrote this</p>",
        "id": 507956956,
        "sender_full_name": "fosco",
        "timestamp": 1742891617
    },
    {
        "content": "<p>Stupid LLM forgetting the syntax for bold in TeX and falling back on Markdown...</p>",
        "id": 507966779,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1742894228
    },
    {
        "content": "<p>I'm proud to say I called bullshit from the titles alone in my feed lol glad I wasn't wrong</p>",
        "id": 508059186,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1742917155
    },
    {
        "content": "<p>Heh, we did an experiment on LLMs that produce SQL code, and for many of them, no matter how much you tell them not to format the output, they still do it. Stripping extra comments and markdown/html out of responses turned out to be the hardest part of interacting with the LLM in an automated flow.</p>",
        "id": 508067115,
        "sender_full_name": "Ryan Wisnesky",
        "timestamp": 1742919054
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275932\">Matteo Capucci (he/him)</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/508059186\">said</a>:</p>\n<blockquote>\n<p>I'm proud to say I called bullshit from the titles alone in my feed lol glad I wasn't wrong</p>\n</blockquote>\n<p>Right, natural transformations between theorems.</p>",
        "id": 508136651,
        "sender_full_name": "Joe Moeller",
        "timestamp": 1742940391
    },
    {
        "content": "<p>I noticed there are two orders of the names used. Two of the papers are JRB, and two are BJR. What could be the point of that?</p>",
        "id": 508136715,
        "sender_full_name": "Joe Moeller",
        "timestamp": 1742940437
    },
    {
        "content": "<p>The email address seems to be attached to Open University Japan, so name-order may have been auto-generated differently for the different papers?</p>",
        "id": 508152007,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1742948826
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"282822\">fosco</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/507956956\">said</a>:</p>\n<blockquote>\n<p>yep, no way a human wrote this</p>\n</blockquote>\n<p>To be fair, I have seen researchers who <em>just</em> learned about category theory writing this way. </p>\n<p>Anyway, the AI-generated slop CT papers are coming. I've noticed that <a href=\"http://chat.qwen.ai\">Qwen</a> 2.5 is trained on a lot of higher/formal category theory. It's fun to play with and it can produce approximately accurate references to results, which can sometimes cut down on search time. It's not yet good enough to generate any meaningfully creative results, and is not enough to fool a half-keen eye, but I can imagine an undergrad using qwen to write a undergrad thesis that nobody reads.</p>",
        "id": 508787020,
        "sender_full_name": "Noah Chrein",
        "timestamp": 1743176694
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"277976\">Noah Chrein</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/508787020\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"282822\">fosco</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/507956956\">said</a>:</p>\n<blockquote>\n<p>yep, no way a human wrote this</p>\n</blockquote>\n<p>To be fair, I have seen researchers who <em>just</em> learned about category theory writing this way. </p>\n<p>Anyway, the AI-generated slop CT papers are coming. I've noticed that <a href=\"http://chat.qwen.ai\">Qwen</a> 2.5 is trained on a lot of higher/formal category theory. It's fun to play with and it can produce approximately accurate references to results, which can sometimes cut down on search time. It's not yet good enough to generate any meaningfully creative results, and is not enough to fool a half-keen eye, but I can imagine an undergrad using qwen to write a undergrad thesis that nobody reads.</p>\n</blockquote>\n<p>What is Qwen and how happen it was trained on so much category theory?</p>",
        "id": 508788675,
        "sender_full_name": "Ivan Di Liberti",
        "timestamp": 1743177114
    },
    {
        "content": "<p>Qwen appears to be Alibaba's language model. I hadn't heard of it till now.</p>",
        "id": 508792542,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1743178049
    },
    {
        "content": "<p>Perhaps the Chinese understand the importance of category theory to mathematics and hence to generalized cognition</p>",
        "id": 508792840,
        "sender_full_name": "Noah Chrein",
        "timestamp": 1743178117
    },
    {
        "content": "<p>There’s an interesting fake paper on the ArXiv today. I can’t really tell if it’s AI crankery or just the old fashioned kind. Did anybody glance at it? <a href=\"https://arxiv.org/abs/2505.22558\">https://arxiv.org/abs/2505.22558</a></p>",
        "id": 521179894,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1748557099
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"609515\">Kevin Carlson</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521179894\">said</a>:</p>\n<blockquote>\n<p>There’s an interesting fake paper on the ArXiv today. I can’t really tell if it’s AI crankery or just the old fashioned kind. Did anybody glance at it? <a href=\"https://arxiv.org/abs/2505.22558\">https://arxiv.org/abs/2505.22558</a></p>\n</blockquote>\n<p>The excessive use of lists suggests AI</p>",
        "id": 521180392,
        "sender_full_name": "Cole Comfort",
        "timestamp": 1748557341
    },
    {
        "content": "<p>Right, that makes sense. It was harder to find obvious local absurdities than in papers further up this thread, which is disappointing.</p>",
        "id": 521182914,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1748558845
    },
    {
        "content": "<p>There's a whole bunch recently that I have been <del>complaining about</del> pointing out elsewhere. The author is uploading a new paper every couple of days, and the title names something after himself. I'm happy to see today that they've been moved to math.GM ! (as I suggested)</p>\n<p><a href=\"https://export.arxiv.org/find/math/1/au:+Alpay_F/0/1/0/all/0/1\">https://export.arxiv.org/find/math/1/au:+Alpay_F/0/1/0/all/0/1</a></p>",
        "id": 521187274,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1748561523
    },
    {
        "content": "<p>And in the case at the top of the thread, namely <a href=\"https://arxiv.org/search/?query=Reizi&amp;searchtype=author\">https://arxiv.org/search/?query=Reizi&amp;searchtype=author</a> all these are also math.GM classified now, not math.CT.</p>",
        "id": 521187635,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1748561716
    },
    {
        "content": "<p>Seems like in theory the arXiv \"endorsement system\" should deal with AI generated papers just like any other spam, but I guess it doesn't work in practice?    <a href=\"https://info.arxiv.org/help/endorsement.html\">https://info.arxiv.org/help/endorsement.html</a></p>",
        "id": 521189500,
        "sender_full_name": "Ryan Wisnesky",
        "timestamp": 1748562959
    },
    {
        "content": "<p>Yes, I'm a bit confused how all these people are getting endorsements.</p>",
        "id": 521194635,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1748565330
    },
    {
        "content": "<p>At the very least it should be possible to \"un-endorse\" them after they've demonstrated their crankiness.</p>",
        "id": 521194732,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1748565372
    },
    {
        "content": "<p>Another one! <a href=\"https://arxiv.org/abs/2505.22931\">https://arxiv.org/abs/2505.22931</a></p>",
        "id": 521215571,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1748578166
    },
    {
        "content": "<p>Maybe the arXiv needs to appoint a category theorist to the team of moderators...</p>",
        "id": 521216374,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1748578738
    },
    {
        "content": "<p>I thought arXiv had a strong stance against crackpottery, so why are these papers allowed to remain under math.GM, rather than being removed entirely?</p>",
        "id": 521233951,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1748588545
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"609515\">Kevin Carlson</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521182914\">said</a>:</p>\n<blockquote>\n<p>Right, that makes sense. It was harder to find obvious local absurdities than in papers further up this thread, which is disappointing.</p>\n</blockquote>\n<p>The phrase \"discrete conformal field theory\" in the abstract made me raise my eyebrows.  As if that were a known thing.  Given how much people try everything, there probably <em>is</em> <strong>some</strong> work on something called discrete conformal field theory, but....</p>\n<p>Yeah, there's a paper <a href=\"https://link.springer.com/article/10.1007/s00220-022-04475-x.\">Conformal Field Theory at the Lattice Level: Discrete Complex Analysis and Virasoro Structure</a> trying to understand how conformal field theory is related to field theory on a lattice.  But most conformal transformations don't map a lattice to itself, so this is bound to be rough, and the idea that \"Recursive Difference Categories and Topos-Theoretic Universality\" would have something to say about it is, umm, questionable.</p>",
        "id": 521235022,
        "sender_full_name": "John Baez",
        "timestamp": 1748589023
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276092\">Nathanael Arkor</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521233951\">said</a>:</p>\n<blockquote>\n<p>I thought arXiv had a strong stance against crackpottery, so why are these papers allowed to remain under math.GM, rather than being removed entirely?</p>\n</blockquote>\n<p>It can be hard to tell whether a math paper is crazy, and people whose papers are rejected entirely complain a lot, so it seems the arXiv folks find it convenient to put borderline papers into math.GM,  expecting people 'in the know' to beware of such papers.  That's my impression anyway.</p>",
        "id": 521236395,
        "sender_full_name": "John Baez",
        "timestamp": 1748589669
    },
    {
        "content": "<p>It's more diplomatic than having math.CP for crackpot math.</p>",
        "id": 521236804,
        "sender_full_name": "John Baez",
        "timestamp": 1748589868
    },
    {
        "content": "<p>This is a truly beautiful era to witness first-hand.</p>",
        "id": 521244418,
        "sender_full_name": "fosco",
        "timestamp": 1748592972
    },
    {
        "content": "<p>ViXra appears to be <a href=\"https://ai.vixra.org/\">embracing</a> the future...</p>",
        "id": 521246836,
        "sender_full_name": "Areeb SM",
        "timestamp": 1748593894
    },
    {
        "content": "<p>But not unreservedly:</p>\n<blockquote>\n<p><a href=\"http://viXra.org\">viXra.org</a> only accept scholarly articles written without AI assistance. Please go to <a href=\"http://ai.viXra.org\">ai.viXra.org</a> to submit new scholarly article written with AI assistance.</p>\n</blockquote>",
        "id": 521248709,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1748594574
    },
    {
        "content": "<p>arxiv could use the exact same disclaimer, only changing the first instance of \"vixra\".</p>",
        "id": 521385995,
        "sender_full_name": "Joe Moeller",
        "timestamp": 1748648324
    },
    {
        "content": "<p><a href=\"http://ai.viXra.org\">ai.viXra.org</a> sounds like a fascinating crackpot sociology experiment.  They have 343 papers so far.   Within the subject of physics, most of the papers are on \"relativity and cosmology\", so we can guess that part of physics attracts crackpots the most.     Within mathematics, 75% of the papers are on number theory.</p>",
        "id": 521423955,
        "sender_full_name": "John Baez",
        "timestamp": 1748681374
    },
    {
        "content": "<p>Yesterday's first submitted paper on general relativity and cosmology:</p>\n<p><strong>The Pi-Periodic 22/7ths Dimension: A Quantum Gravity Framework for Dark Energy</strong></p>\n<blockquote>\n<p>We propose a novel 4+1-dimensional quantum gravity framework incorporating a compactified extra dimension, τ , with a periodicity of π (to 22 decimal places), symbolically tied to the rational approximation 22/7.</p>\n</blockquote>\n<p>Someone is taking this 22/7 stuff very seriously!  I believe Archimedes came up with this approximation to pi, and it was good enough that by the Middle Ages a bunch of mathematicians believed <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>π</mi><mo>=</mo></mrow><annotation encoding=\"application/x-tex\">\\pi = </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span></span></span></span> 22/7.</p>",
        "id": 521424158,
        "sender_full_name": "John Baez",
        "timestamp": 1748681545
    },
    {
        "content": "<blockquote>\n<p>by the Middle Ages a bunch of mathematicians believed <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>π</mi><mo>=</mo></mrow><annotation encoding=\"application/x-tex\">\\pi =</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span></span></span></span>  22/7.</p>\n</blockquote>\n<p><span aria-label=\"surprise\" class=\"emoji emoji-1f62e\" role=\"img\" title=\"surprise\">:surprise:</span>  Wait, is it not? /s</p>",
        "id": 521425529,
        "sender_full_name": "fosco",
        "timestamp": 1748682819
    },
    {
        "content": "<p>Archimedes squared the circle with this ONE WEIRD TRICK!  Geometers hate him!</p>",
        "id": 521426011,
        "sender_full_name": "James Deikun",
        "timestamp": 1748683270
    },
    {
        "content": "<p>Actually I learned this when reading about the mathematician <a href=\"https://johncarlosbaez.wordpress.com/2025/04/24/civilizational-collapse-part-5/\">Franco of Liège</a>.  In 1020 he got interested in the ancient Greek problem of squaring the circle. But since he believed that pi is 22/7, he started studying the square root of 22/7.  I don't know if he figured out how to construct the square root of 22/7 with straightedge and compass.  But he did manage to prove that the square root of 22/7 is irrational!</p>\n<p>Now, this is better than it sounds, because I believe the old Greek proof that <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msqrt><mn>2</mn></msqrt></mrow><annotation encoding=\"application/x-tex\">\\sqrt{2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.04em;vertical-align:-0.1328em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9072em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\">2</span></span></span><span style=\"top:-2.8672em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"400em\" height=\"1.08em\" viewBox=\"0 0 400000 1080\" preserveAspectRatio=\"xMinYMin slice\"><path d=\"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z\"/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1328em;\"><span></span></span></span></span></span></span></span></span> is irrational had been lost in western Europe at this time.   So it took some serious ingenuity.  </p>\n<p>Still, it's a sad reflection on the sorry state of mathematical knowledge in western Europe from around 500 AD to 1000 AD.   It was better elsewhere at that time.   I find this local collapse of civilization, and how people recovered, quite fascinating.</p>",
        "id": 521426330,
        "sender_full_name": "John Baez",
        "timestamp": 1748683563
    },
    {
        "content": "<p>Could AI slop prompt some loss of collective intelligence now?</p>",
        "id": 521426636,
        "sender_full_name": "John Baez",
        "timestamp": 1748683850
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521426636\">said</a>:</p>\n<blockquote>\n<p>Could AI slop prompt some loss of collective intelligence now?</p>\n</blockquote>\n<p>In general any tool that helps you thinking makes you sloppier in some respect. So yes. For instance, ancient languages are often way more complicated grammatically than new languages. One reason for this is that being able to say \"Go around the mammoth, without being heard, by exactly half of a circle\" in fewer words may have been a big advantage when we were hunter-gatherers, so languages tended to be more expressive. With civilization, inception of written support etc we lost the need to formulate such complicated statements in a compact way, languages became less expressive, and we probably lost some of our cognitive ability in the process as well. It's always a tradeoff.</p>",
        "id": 521429490,
        "sender_full_name": "Fabrizio Romano Genovese",
        "timestamp": 1748686516
    },
    {
        "content": "<p>I'm thinking more about how successive generations of Roman summaries of Greek scientific texts watered them down to a homeopathic dilution of their original strength.  Then many of the originals were lost, at least in western Europe.</p>",
        "id": 521434376,
        "sender_full_name": "John Baez",
        "timestamp": 1748691002
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"679887\">@Fabrizio Romano Genovese</span>: could you share a reference for the claim that older languages have higher entropy than modern languages?</p>",
        "id": 521434597,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1748691176
    },
    {
        "content": "<p>13 messages were moved from this topic to <a class=\"stream-topic\" data-stream-id=\"229451\" href=\"/#narrow/channel/229451-meta.3A-off-topic/topic/language.3A.20the.20rise.20and.20fall.20of.20complex.20grammars/with/521434597\">#meta: off-topic &gt; language: the rise and fall of complex grammars</a> by <span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span>.</p>",
        "id": 521525145,
        "sender_full_name": "Notification Bot",
        "timestamp": 1748765102
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"679887\">Fabrizio Romano Genovese</span> <a href=\"#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/521429490\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521426636\">said</a>:</p>\n<blockquote>\n<p>Could AI slop prompt some loss of collective intelligence now?</p>\n</blockquote>\n<p>In general any tool that helps you thinking makes you sloppier in some respect. So yes. For instance, ancient languages are often way more complicated grammatically than new languages. One reason for this is that being able to say \"Go around the mammoth, without being heard, by exactly half of a circle\" in fewer words may have been a big advantage when we were hunter-gatherers, so languages tended to be more expressive. With civilization, inception of written support etc we lost the need to formulate such complicated statements in a compact way, languages became less expressive, and we probably lost some of our cognitive ability in the process as well. It's always a tradeoff.</p>\n</blockquote>\n<p>uuuhmm what's a reference for this? smells really funny to me...</p>",
        "id": 521805518,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1748885576
    },
    {
        "content": "<p><a href=\"#narrow/channel/229451-meta.3A-off-topic/topic/language.3A.20the.20rise.20and.20fall.20of.20complex.20grammars/with/521434597\">https://categorytheory.zulipchat.com/#narrow/channel/229451-meta.3A-off-topic/topic/language.3A.20the.20rise.20and.20fall.20of.20complex.20grammars/with/521434597</a></p>",
        "id": 521829583,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1748893478
    },
    {
        "content": "<p>New AI paper up:</p>\n<ol>\n<li><a href=\"https://arxiv.org/abs/2506.06885\">https://arxiv.org/abs/2506.06885</a></li>\n</ol>",
        "id": 523707907,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1749718438
    },
    {
        "content": "<p>This one is funny because it outs itself<br>\n<a href=\"/user_uploads/21317/g4CqaUqHyjbQrd8X6oR_oXBq/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/21317/g4CqaUqHyjbQrd8X6oR_oXBq/image.png\" title=\"image.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"818x398\" src=\"/user_uploads/thumbnail/21317/g4CqaUqHyjbQrd8X6oR_oXBq/image.png/840x560.webp\"></a></div>",
        "id": 523708003,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1749718466
    },
    {
        "content": "<p>I actually approve of this way of approaching AI tools: personally, I don't think they automatically disqualify a paper. The principle should be the the author is ultimately responsible to check their results, and remains fully accountable.</p>",
        "id": 523708584,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1749718621
    },
    {
        "content": "<blockquote>\n<p>Our results offer a formal justification for this procedure, suggesting that the analytic<br>\ncontinuation is not arbitrary but is in fact forced by the underlying principles of symmetry<br>\nand normalization.</p>\n</blockquote>\n<p>Kind of a funny quote because the analytic continuation of a function is one of the most rigidly determined and least arbitrary constructions in mathematics</p>",
        "id": 523708600,
        "sender_full_name": "Patrick Nicodemus",
        "timestamp": 1749718626
    },
    {
        "content": "<p>Indeed, the paper is (from a quick skim) likely formally correct but basically insubtantials, it's a big cargo-cult regurgitation. The whole thing seems circular.</p>",
        "id": 523708805,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1749718687
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275932\">Matteo Capucci (he/him)</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523708584\">said</a>:</p>\n<blockquote>\n<p>I actually approve of this way of approaching AI tools: personally, I don't think they automatically disqualify a paper. The principle should be the the author is ultimately responsible to check their results, and remains fully accountable.</p>\n</blockquote>\n<p>Yeah, I think I agree. At the very least we might have to get used to seeing that writing style everywhere, I can imagine a non-native speaker feeling a lot of pressure to use it to make their wording seem natural. It doesn't inherently disqualify the paper. But, on the other hand, it makes me suspicious and vigilant of errors, and at that point even a small error would be enough to cause me to discard it.</p>",
        "id": 523709393,
        "sender_full_name": "Patrick Nicodemus",
        "timestamp": 1749718859
    },
    {
        "content": "<p>I suspect at some point someone will try to mass-produce papers and submit them everywhere (it's almost trivial to compile a list of journal inside the math-cs area; let the machine prepare a different paper for each item of the list; let the machine submit, let the machine handle the rebuttals and modify the paper accordingly, resubmit...), relying on small probability of success after a high number of trial. </p>\n<p>It's the academia equivalent of asking out 100 girls, one of them will say yes.</p>\n<p>These are very interesting times to witness. Especially if you're an irredeemable nihilist.</p>",
        "id": 523712713,
        "sender_full_name": "fosco",
        "timestamp": 1749719899
    },
    {
        "content": "<p>I think there's a real risk of the image of CT being tarnished if this type of stuff becomes too common. The number theory people know how to funnel cranks away from their arXiv category, if category theorists can't do this, it's not a good look.</p>",
        "id": 523740601,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1749729409
    },
    {
        "content": "<p>Also, mathematicians generally are conscious of the circle squarers and the number theory cranks and so on, and can spot this stuff pretty easily, because it's on a hot-button topic and shows the usual obvious signs. But something in category theory applied to other areas (not Applied Category Theory, but to an outsider it's not necessarily obvious) that plays to the stereotypes of CT's abstract nonsense moniker just looks like another silly CT paper that claims to revolutionise our understanding of a piece of classical mathematics when really it's empty of real content.</p>",
        "id": 523741056,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1749729575
    },
    {
        "content": "<p>Perhaps not among hardcore mathematicians, who would almost surely recognise the problem and commiserate, but anyone merely adjacent, for instance someone with money who might be needed to be convinced to fund some real and good ACT may get wind of this AI nonsense. </p>\n<p>Maybe I'm being too pessimistic here. But these are ideas that occur to me</p>",
        "id": 523742554,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1749730100
    },
    {
        "content": "<blockquote>\n<p>The number theory people know how to funnel cranks away from their arXiv category</p>\n</blockquote>\n<p>How?</p>",
        "id": 523756995,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1749734780
    },
    {
        "content": "<p>Well, I get math.NT daily announcements and I've never seen a crank number theory paper, and yet I know they do turn up in math.GM.</p>",
        "id": 523757655,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1749734998
    },
    {
        "content": "<p>So somehow they manage it.</p>",
        "id": 523757689,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1749735008
    },
    {
        "content": "<p>Have we taken any action about these papers?  Contacted anyone at arXiv about removing them and un-endorsing the submitters?  That seems to me to be the obvious first step.  I'd be willing to help if needed, although I don't have the time to filter the daily submissions for them myself.</p>",
        "id": 523782577,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1749742159
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276422\">David Michael Roberts</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523741056\">said</a>:</p>\n<blockquote>\n<p>But something in category theory applied to other areas (not Applied Category Theory, but to an outsider it's not necessarily obvious) that plays to the stereotypes of CT's abstract nonsense moniker just looks like another silly CT paper that claims to revolutionise our understanding of a piece of classical mathematics when really it's empty of real content.</p>\n</blockquote>\n<p>I wonder if an effect like this could be what's causing the problem by making it easier for cranks to get endorsed with CT papers by lazy non-category-theorists.</p>",
        "id": 523783104,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1749742325
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276422\">David Michael Roberts</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523757655\">said</a>:</p>\n<blockquote>\n<p>Well, I get math.NT daily announcements and I've never seen a crank number theory paper, and yet I know they do turn up in math.GM.</p>\n</blockquote>\n<p>To me, that just suggests that the arXiv editors are better at detecting crank NT papers than crank CT papers, likely because they have had more practice at it.</p>",
        "id": 523786721,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1749743466
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276777\">Mike Shulman</span> <a href=\"#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/523782577\">said</a>:</p>\n<blockquote>\n<p>Have we taken any action about these papers?  Contacted anyone at arXiv about removing them and un-endorsing the submitters?  That seems to me to be the obvious first step.  I'd be willing to help if needed, although I don't have the time to filter the daily submissions for them myself.</p>\n</blockquote>\n<p>I’ve contacted the ArXiv about the first batch of these that came up. They said they’d look into it but don’t share results of investigations. I haven’t checked whether the papers are down. It feels like fingers in a dike if we can’t figure out who is endorsing these authors though!</p>",
        "id": 523792785,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1749745600
    },
    {
        "content": "<p>Did your first batch include <a href=\"https://arxiv.org/abs/2505.22931\">Recursive Difference Categories and Topos-Theoretic Universality</a> by <a href=\"https://arxiv.org/search/math?searchtype=author&amp;query=Santacana,+A+B\">Andreu Ballus Santacana</a>?  That was a crank paper discussed here earlier.   It's still up!   Santacana is also responsible for the new one you folks are talking about today, <a href=\"https://arxiv.org/abs/2506.06885\">Analytic Uniqueness of Ball Volume Interpolation: Categorical Invariance and Universal Characterization</a>.</p>",
        "id": 523793705,
        "sender_full_name": "John Baez",
        "timestamp": 1749745928
    },
    {
        "content": "<p>I checked earlier, and Santacana appears to be in the <a href=\"https://portalrecerca.uab.cat/en/persons/andreu-ballus-santacana\">department of philosophy of UAB Barcelona</a>.</p>\n<p>(He's definitely got the Grothendieck bald-head thing going on.)</p>",
        "id": 523793881,
        "sender_full_name": "John Baez",
        "timestamp": 1749745997
    },
    {
        "content": "<p>I reported the papers of Barreto that David Roberts opened this thread with. Unfortunately they're still up and there have been two more since then. They're all in GM now, though, which I guess is the best it seems we can generally hope for.</p>",
        "id": 523801760,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1749749184
    },
    {
        "content": "<p>The moving to math.GM has been patchy. Some of the ones by one author whose primary listing is CS.lo haven't moved, while those that were listed under math.CT have. Presumably because computer scientists are even less well-equipped than a generic mathematician to judge what CT is actually AI-generated crank material.</p>",
        "id": 523847620,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1749770853
    },
    {
        "content": "<p>If the people submitting these things are actually employed by reputable institutions, perhaps we should contact their employers.</p>",
        "id": 523848772,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1749771840
    },
    {
        "content": "<p>The first person I reported is unlocatable online, IIRC. But that’s apparently not the case for everyone.</p>",
        "id": 523860779,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1749781521
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276422\">David Michael Roberts</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523740601\">said</a>:</p>\n<blockquote>\n<p>I think there's a real risk of the image of CT being tarnished if this type of stuff becomes too common. The number theory people know how to funnel cranks away from their arXiv category, if category theorists can't do this, it's not a good look.</p>\n</blockquote>\n<p>Every week, <em>one or two</em> of these papers make it into the <code>math.LO</code>/<code>cs.LO</code> announcements, which is frankly ridiculous.  We had a person who just had a couple of their articles GM-holed last week get through to <code>cs.LO</code> <em>again</em> this week. Especially disappointing since at the same time, I know multiple people with solid academic affiliations, long records in logic, and academic email addresses who've seen their announcements blocked/delayed while they appealed (e.g. conference extended abstracts misclassified and rejected as \"abstract-only submissions\", or a PhD thesis randomly rejected) :/</p>\n<p>I don't think this tarnishes the image of logic itself, but it's certainly a big source of noise and not a good look for arXiv moderation.</p>",
        "id": 523916691,
        "sender_full_name": "Zoltan A. Kocsis (Z.A.K.)",
        "timestamp": 1749810585
    },
    {
        "content": "<p>A possible solution is to set up a small website that collects these papers and flag them as \"probably bollocks\". A small number of us, committed to express a judgment evaluate these submissions pointing out \"this passage is AI generated\" \"the second sentence at page 2 doesn't make any sense\" etc</p>\n<p>It takes a lot of work, but we all know what's the rule here:<br>\n<a href=\"/user_uploads/21317/0PygcTIJuTOeZlzGYmIK5wIM/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/21317/0PygcTIJuTOeZlzGYmIK5wIM/image.png\" title=\"image.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"640x600\" src=\"/user_uploads/thumbnail/21317/0PygcTIJuTOeZlzGYmIK5wIM/image.png/840x560.webp\"></a></div><p>I agree that this state of affairs tarnishes the reputation of category theory/ists and I think there is only one way to nip the problem in the bud, that is taking responsibility and vehemently assert that \"yeah, no, we do not recognize this shit as category theory or even as decent math\"</p>",
        "id": 523927191,
        "sender_full_name": "fosco",
        "timestamp": 1749814408
    },
    {
        "content": "<p>I'm not sure this kind of \"negative curation\", in which we maintain lists of things that are <em>bad</em>, is the way to go. </p>\n<p>In an ideal world the function of journals is to be lists of things that are <em>good</em>, or at least probably not bad.</p>",
        "id": 523930411,
        "sender_full_name": "Chad Nester",
        "timestamp": 1749815584
    },
    {
        "content": "<p>One option is to make sure all these dodgy AI-generated papers have comments on PubPeer. See eg <a href=\"https://www.pubpeer.com/publications/D52D1CC22593701472A83CFB9C2FD8\">https://www.pubpeer.com/publications/D52D1CC22593701472A83CFB9C2FD8</a> If the obvious red flags are documented here, then a list of links can be curated in a place category theorists have control over, or sent to arXiv admins, or employers of people making this nonsense.</p>",
        "id": 523933583,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1749816804
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"690871\">Chad Nester</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523930411\">said</a>:</p>\n<blockquote>\n<p>I'm not sure this kind of \"negative curation\", in which we maintain lists of things that are <em>bad</em>, is the way to go. <br>\n</p>\n</blockquote>\n<p>History disagrees <a href=\"https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum\">https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum</a> lists of things that are bad can be used to repress heresy.</p>",
        "id": 523951140,
        "sender_full_name": "fosco",
        "timestamp": 1749822766
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"282822\">fosco</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523951140\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"690871\">Chad Nester</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523930411\">said</a>:</p>\n<blockquote>\n<p>I'm not sure this kind of \"negative curation\", in which we maintain lists of things that are <em>bad</em>, is the way to go. <br>\n</p>\n</blockquote>\n<p>History disagrees <a href=\"https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum\">https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum</a> lists of things that are bad can be used to repress heresy.</p>\n</blockquote>\n<p><em>Index Paperorum Crackpoti</em></p>",
        "id": 523951855,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1749823003
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"282822\">fosco</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523951140\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"690871\">Chad Nester</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523930411\">said</a>:</p>\n<blockquote>\n<p>I'm not sure this kind of \"negative curation\", in which we maintain lists of things that are <em>bad</em>, is the way to go. <br>\n</p>\n</blockquote>\n<p>History disagrees <a href=\"https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum\">https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum</a> lists of things that are bad can be used to repress heresy.</p>\n</blockquote>\n<p>An inquisition would, at least, be entertaining :)</p>",
        "id": 524110845,
        "sender_full_name": "Chad Nester",
        "timestamp": 1749971889
    },
    {
        "content": "<p>A <a href=\"https://arxiv.org/abs/2506.14537\">new paper</a> in <a href=\"http://quant.ph\">quant.ph</a>  supposedly connecting modular tensor categories to quantum contextuality smells like LLM to me.</p>",
        "id": 524653576,
        "sender_full_name": "Martti Karvonen",
        "timestamp": 1750235007
    },
    {
        "content": "<p>Pages 7 and 8 definitely look like over-optimistic generalities of dot points. And the 'proof' here is altogether lacking in convincing detail in the last two sentences....</p>\n<blockquote>\n<p><strong>Proposition 4.3.</strong> The braid group representation <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>ρ</mi><mi>F</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\rho_F</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">ρ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">F</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> derived from the Fibonacci category violates the KCBS inequality maximally, demonstrating strong contextuality intrinsic to its topological structure.  <br>\n<strong>Proof.</strong> Projectors onto fusion basis states corresponding to the object <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>τ</mi></mrow><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.1132em;\">τ</span></span></span></span> generate a measurement scenario isomorphic to the pentagon graph underlying the KCBS inequality [9, 11]. The noncommuting braid generators create measurement contexts whose statistical correlations surpass classical bounds. Numerical evaluation of expectation values using explicit ρF matrices confirms maximal violation</p>\n</blockquote>",
        "id": 524660144,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1750237336
    },
    {
        "content": "<p>It's just verbiage, with the convenient out that \"numerical evaluation\" will bear out the claim.    In other words, bullshit.</p>",
        "id": 524661319,
        "sender_full_name": "John Baez",
        "timestamp": 1750237714
    },
    {
        "content": "<p>Another AI-slop paper from Reizi:</p>\n<p><a href=\"https://arxiv.org/abs/2506.21653\">https://arxiv.org/abs/2506.21653</a></p>\n<p>Primary subject this time math.LO not math.CT. Also, name changed from Barreto Joaquim Reiz to Higuchi Joaquim Reizi. The formatting is also weirdly broken, with line numbers appearing inconsistently...</p>",
        "id": 526313843,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1751254005
    },
    {
        "content": "<p>Identical submission email, this person needs to be put on a special watch-list.</p>",
        "id": 526313964,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1751254144
    },
    {
        "content": "<p>Have you considered emailing folks at the arXiv, where these suggestions can have some effect?</p>",
        "id": 526318854,
        "sender_full_name": "John Baez",
        "timestamp": 1751258605
    },
    {
        "content": "<p>I'm working on that, too, through a more senior person in the logic community, behind the scenes. I'm just cataloguing them here for the benefit of people who might see it and waste the time looking at it (though this one is pretty blatant). I can stop if it's too much noise.</p>",
        "id": 526319139,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1751258877
    },
    {
        "content": "<p>It's not too much noise; people can always mute this thread if they want.  I'm just glad you're trying to actually do something about this.</p>",
        "id": 526319398,
        "sender_full_name": "John Baez",
        "timestamp": 1751259104
    },
    {
        "content": "<p>Someone pointed me at the arXiv moderation contact form, I put my case to them.</p>",
        "id": 526328702,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1751265486
    },
    {
        "content": "<p>Thanks, yes that's an easy way to contact the moderators.  Since you don't sound like a crackpot, they should take you seriously, though action may be slow, and almost surely near-silent.</p>",
        "id": 526331329,
        "sender_full_name": "John Baez",
        "timestamp": 1751266850
    },
    {
        "content": "<p>E.g. I asked them whether they had an international backup of the arXiv, and they never replied, but <a href=\"https://mathstodon.xyz/@johncarlosbaez/114752342143968527\">now they have one</a>.</p>\n<p>(I'm not claiming I caused this, but it was an obvious thing to want so I'm glad they have it now.)</p>",
        "id": 526331547,
        "sender_full_name": "John Baez",
        "timestamp": 1751266947
    },
    {
        "content": "<p>Number of days since an AI-generated slop article made it to <code>math.LO</code>: zero. Again.</p>",
        "id": 526532165,
        "sender_full_name": "Zoltan A. Kocsis (Z.A.K.)",
        "timestamp": 1751350947
    },
    {
        "content": "<p>Yes. Also a repeat offender. But also it's going to math.FA, and not even cross-listed to math.CT, despite the title and the topic. </p>\n<p>I encourage people to use the 'arXiv moderation user support' contact link here: <a href=\"https://arxiv-org.atlassian.net/servicedesk/customer/portal/2\">https://arxiv-org.atlassian.net/servicedesk/customer/portal/2</a> and let the moderators know about the suspect paper(s) that turn up. Be specific in your report as to what makes you think it is LLM-generated, with examples from the paper that no human would write, if possible.</p>",
        "id": 526534815,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1751352190
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"510824\">Zoltan A. Kocsis (Z.A.K.)</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/526532165\">said</a>:</p>\n<blockquote>\n<p>Number of days since an AI-generated slop article made it to <code>math.LO</code>: zero. Again.</p>\n</blockquote>\n<p>\"Submitted to Inventiones Mathematicae\"</p>\n<p>LOL</p>",
        "id": 526551621,
        "sender_full_name": "fosco",
        "timestamp": 1751358005
    },
    {
        "content": "<p>It seems like the author is picking a different primary classification for every paper</p>",
        "id": 526556277,
        "sender_full_name": "Amar Hadzihasanovic",
        "timestamp": 1751359422
    },
    {
        "content": "<p>First one was math.CT, second cs.LO, third math.RT, fourth math.FA</p>",
        "id": 526556355,
        "sender_full_name": "Amar Hadzihasanovic",
        "timestamp": 1751359447
    },
    {
        "content": "<p>Looks like it could be a conscious effort to avoid moderation</p>",
        "id": 526556420,
        "sender_full_name": "Amar Hadzihasanovic",
        "timestamp": 1751359468
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"510824\">Zoltan A. Kocsis (Z.A.K.)</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/526532165\">said</a>:</p>\n<blockquote>\n<p>Number of days since an AI-generated slop article made it to <code>math.LO</code>: zero. Again.</p>\n</blockquote>\n<p>And again zero :(</p>",
        "id": 526914105,
        "sender_full_name": "Zoltan A. Kocsis (Z.A.K.)",
        "timestamp": 1751515736
    },
    {
        "content": "<p>I wonder what could be a community Plan B if the arXiv moderators are unable to cope with this wave and the arXiv becomes viXraised. ArXiv overlays with additional community filters?</p>",
        "id": 526914438,
        "sender_full_name": "Zoltan A. Kocsis (Z.A.K.)",
        "timestamp": 1751516030
    },
    {
        "content": "<p>In particular, it doesn't look like arXiv's \"get endorsement or academic-email\" system can be tightened any further without causing undue difficulty to regular academics who want to post genuine preprints.</p>",
        "id": 526914722,
        "sender_full_name": "Zoltan A. Kocsis (Z.A.K.)",
        "timestamp": 1751516307
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"510824\">@Zoltan A. Kocsis (Z.A.K.)</span> Is this about \"systemic contraints\"? ;-) The one graphic in that paper looks like a typical not-that-good LLM trying to make a technical image.</p>",
        "id": 526918572,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1751519380
    },
    {
        "content": "<p>uhm would a webpage with a list of 'suspected slop' preprints be too strong of a reaction to this phenomenon? I'd be willing to setup that, and have people submit me entries</p>",
        "id": 527444264,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1751882770
    },
    {
        "content": "<p>\"The Wall of Shame\"</p>",
        "id": 527463343,
        "sender_full_name": "Chad Nester",
        "timestamp": 1751889696
    },
    {
        "content": "<p>Sounds good to me <span class=\"user-mention\" data-user-id=\"275932\">@Matteo Capucci (he/him)</span></p>",
        "id": 527513258,
        "sender_full_name": "Morgan Rogers (he/him)",
        "timestamp": 1751904742
    },
    {
        "content": "<p>By the way, it's wise to be quite polite and cautious in your public description of this web page, to reduce your chance of getting sued and/or harrassed.   Having gotten threats from people I criticized publicly, I can assure you it's not much fun.</p>",
        "id": 527514058,
        "sender_full_name": "John Baez",
        "timestamp": 1751904998
    },
    {
        "content": "<p>If it's not wholly AI-generated, it's at least got lots of LLM fingerprints all over the formatting and structure <a href=\"https://arxiv.org/abs/2507.04089\">https://arxiv.org/abs/2507.04089</a></p>",
        "id": 527604656,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1751950966
    },
    {
        "content": "<p><a href=\"https://arxiv.org/search/math?searchtype=author&amp;query=Hajebi,+P\">https://arxiv.org/search/math?searchtype=author&amp;query=Hajebi,+P</a> :-(</p>",
        "id": 529199942,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1752726174
    },
    {
        "content": "<p>Not an AI-generated paper, but I just stumble on a AI-generated <a href=\"https://www.numberanalytics.com/blog/ultimate-guide-fibration-category-theory\">blog article about fibrations</a>. (At the very least, the website is quite honest since it reveals that the author is Llama-4)</p>",
        "id": 531552419,
        "sender_full_name": "Peva Blanchard",
        "timestamp": 1753779689
    },
    {
        "content": "<p>It's amusing how the definition of fibration is wrong.   Also funny how the grammar is frequently wrong in the same way:</p>\n<blockquote>\n<p>Fibration is a fundamental concept</p>\n</blockquote>\n<blockquote>\n<p>Relationship between Fibration and Other Category Theory Concepts</p>\n</blockquote>\n<blockquote>\n<p>Fibration is closely related to</p>\n</blockquote>\n<p>etc.</p>\n<p>Of course I have to be amused, because otherwise I'd break down and cry about how the pool of human knowledge is getting contaminated by sludge like this.</p>\n<p><span aria-label=\"poop\" class=\"emoji emoji-1f4a9\" role=\"img\" title=\"poop\">:poop:</span></p>",
        "id": 531586258,
        "sender_full_name": "John Baez",
        "timestamp": 1753789371
    },
    {
        "content": "<p>I would guess the places where grammar is funny is where the human used search+replace to prepare the prompts to write the pages. This is all mass produced, it's quite harrowing.</p>",
        "id": 531589210,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1753790280
    },
    {
        "content": "<p>I'd like to think all this AI slop will drive the value of expertise upward; after all, now the world not only needs John to write blog posts, but to correct AI generated slop posts too.</p>",
        "id": 531672374,
        "sender_full_name": "Ryan Wisnesky",
        "timestamp": 1753812885
    },
    {
        "content": "<p>If the world needs me to do that, the world is in deep trouble.</p>",
        "id": 531692599,
        "sender_full_name": "John Baez",
        "timestamp": 1753820621
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531586258\">said</a>:</p>\n<blockquote>\n<p>Of course I have to be amused, because otherwise I'd break down and cry about how the pool of human knowledge is getting contaminated by sludge like this.</p>\n</blockquote>\n<p>This prompted me to wonder (somewhat fancifully) whether we could create a parallel humans-only Internet.  Then we could cede the current Internet to the AIs, who would eventually implode due to model collapse.</p>\n<p>At the very least, I'm thinking seriously about not posting new preprints on the arXiv any more, or any other public site from which they could be scraped to train AIs to generate mathematical-sounding slop.  Surely there'd still be some way to make them freely accessible to humans.</p>",
        "id": 531748666,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1753851742
    },
    {
        "content": "<p>And I suppose the same should apply to blog posts.</p>",
        "id": 531767420,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1753860194
    },
    {
        "content": "<p>You have to both figure out some this-is-a-real-human verification method, which probably means some kind of biometrics, and also some method of preventing anybody from just downloading the human-only Internet and feeding it to the bots offline...The first one is solvable but invasive but I'm really not sure how to do the second. DRM for every file on your Internet? Bleh</p>",
        "id": 531768321,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1753860501
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276777\">Mike Shulman</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531748666\">said</a>:</p>\n<blockquote>\n<p>At the very least, I'm thinking seriously about not posting new preprints on the arXiv any more, or any other public site from which they could be scraped to train AIs to generate mathematical-sounding slop.  Surely there'd still be some way to make them freely accessible to humans.</p>\n</blockquote>\n<p>I don't understand how this helps anyone? AI will still be used to output nonsense whether or not it is trained on your specific papers. It will also be trained on any papers  you publish in journals in any case. All you would be doing is making it harder for humans to access your articles.</p>",
        "id": 531769767,
        "sender_full_name": "Graham Manuell",
        "timestamp": 1753860956
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"609515\">Kevin Carlson</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531768321\">said</a>:</p>\n<blockquote>\n<p>You have to both figure out some this-is-a-real-human verification method, which probably means some kind of biometrics, and also some method of preventing anybody from just downloading the human-only Internet and feeding it to the bots offline...The first one is solvable but invasive but I'm really not sure how to do the second. DRM for every file on your Internet? Bleh</p>\n</blockquote>\n<p>I don't know for the second problem. The first problem (distinguishing humans from bots) is indeed already an issue. (e.g., if I remember correctly, Facebook removes billions fake accounts every year). Some people in the cryptography/privacy world work on that, something along the lines of proving that you hold a state-issued ID card without revealing the details (using zero-knowledge proofs).</p>\n<p>I guess this will probably trigger an arms race between \"human-detectors\" and \"human-provers\".</p>",
        "id": 531773951,
        "sender_full_name": "Peva Blanchard",
        "timestamp": 1753862259
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"609515\">Kevin Carlson</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531768321\">said</a>:</p>\n<blockquote>\n<p>also some method of preventing anybody from just downloading the human-only Internet and feeding it to the bots offline</p>\n</blockquote>\n<p>I was imagining that human users would be constantly verified (however that would work) whenever they access an individual document, so they couldn't just log in once and then click \"download the Internet\" and get it all.</p>\n<p>I did say it was  fanciful.  But if the alternative is ceding the Internet to the AIs and having nothing to replace it with, maybe we should be working harder on it.</p>",
        "id": 531774387,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1753862379
    },
    {
        "content": "<p>Yes, I'm pretty sympathetic.</p>",
        "id": 531774647,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1753862433
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"386922\">Graham Manuell</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531769767\">said</a>:</p>\n<blockquote>\n<p>AI will still be used to output nonsense whether or not it is trained on your specific papers.</p>\n</blockquote>\n<p>It's like voting, or reducing your carbon footprint.  Anything any individual person does has a miniscule effect on the world, but the world is made up of individuals, so we should all follow the categorical imperative.</p>\n<p>But I suppose you're right that currently we have no technical solution for disseminating information to humans only, and if we want to stay in this job we have to disseminate our research in some way.  I guess we can hope that the pending copyright lawsuits against AI trainers bear some fruit...</p>",
        "id": 531777576,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1753863179
    },
    {
        "content": "<p>There is another line of research focusing on \"voluntarily poisoning\" your data, so that an AI trained on a dataset including your data could be flagged. See e.g. <a href=\"https://arxiv.org/abs/2410.09101\">this paper</a>.</p>",
        "id": 531778548,
        "sender_full_name": "Peva Blanchard",
        "timestamp": 1753863444
    },
    {
        "content": "<p>That's a nice idea.  I don't suppose it's possible for those of us who don't work with data?</p>",
        "id": 531779722,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1753863792
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276777\">Mike Shulman</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531777576\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"386922\">Graham Manuell</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531769767\">said</a>:</p>\n<blockquote>\n<p>AI will still be used to output nonsense whether or not it is trained on your specific papers.</p>\n</blockquote>\n<p>It's like voting, or reducing your carbon footprint.  Anything any individual person does has a miniscule effect on the world, but the world is made up of individuals, so we should all follow the categorical imperative.</p>\n</blockquote>\n<p>That's why we do category theory!   <span aria-label=\"upside down\" class=\"emoji emoji-1f643\" role=\"img\" title=\"upside down\">:upside_down:</span> </p>\n<p>I do a lot of things that seem a bit quixotic in that they have a miniscule effect.  But I wouldn't stop posting my papers to the arXiv because the positive effect of spreading my ideas to more human mathematicians seems to grossly outweigh the negative effect due to AIs reading them.   </p>\n<p>One slightly quixotic act I've been enjoying is <em>not using Google</em> and instead paying to use a search engine called <a href=\"https://kagi.com/\">Kagi</a> which doesn't show me advertisements, doesn't rely on ad revenue, isn't as susceptible to search engine optimization tricks, doesn't push AI on me, and has a greater variety of search filters, like an \"academic\" filter.   I heard about this from <a href=\"https://pluralistic.net/2025/07/28/twiddlehazard/#outboard-brains-considered-harmful\">Cory Doctorow</a>, who has very interesting things to say about enshittification (a term he invented).</p>",
        "id": 531790513,
        "sender_full_name": "John Baez",
        "timestamp": 1753866861
    },
    {
        "content": "<p>I have little hope that the deluge of AI-generated content will abate. However, it seems more realistic to me that a human verification process could be used to whitelist authors who are known not to be bad actors. arXiv's current verification process is currently entirely insufficient, but I feel it can be addressed if arXiv actually take action. However, a severe disadvantage of this is that it makes academia even less accessible to those outside of it than it already is (because most likely the only entrypoint to verification would be via a verified user/institution).</p>",
        "id": 531790919,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1753866976
    },
    {
        "content": "<p>In some sense, I feel it doesn't matter if huge amounts of AI slop is generated (distasteful as it is), so long as it's possible to filter out. In this case, I think that only permitting trusted users to post research is more important than only permitting trusted users to view research.</p>",
        "id": 531791352,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1753867102
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531790513\">said</a>:</p>\n<blockquote>\n<p>One slightly quixotic act I've been enjoying is <em>not using Google</em> and instead paying to use a search engine called <a href=\"https://kagi.com/\">Kagi</a> which doesn't show me advertisements, doesn't rely on ad revenue, isn't as susceptible to search engine optimization tricks, doesn't push AI on me, and has a greater variety of search filters, like an \"academic\" filter.</p>\n</blockquote>\n<p>That's very interesting!  I see that they do also supply an AI, which seems to contradict their goal of \"humanizing the web\", but I gather from your remarks that you can turn it off.  How does Kagi compare to Google with <a href=\"https://udm14.com/\">udm14</a>, which disables ads and AI?</p>",
        "id": 531793942,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1753867888
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276777\">Mike Shulman</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531779722\">said</a>:</p>\n<blockquote>\n<p>That's a nice idea.  I don't suppose it's possible for those of us who don't work with data?</p>\n</blockquote>\n<p>In principle, the idea works on a vectorial representation of the data, thus should be applicable to text. However, text is more complicated in practice because the mapping \"text <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>→</mo></mrow><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.3669em;\"></span><span class=\"mrel\">→</span></span></span></span> vector\" is less flexible than, e.g., \"image <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>→</mo></mrow><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.3669em;\"></span><span class=\"mrel\">→</span></span></span></span> vector\", so the poison is harder to craft.</p>\n<p>Also, since an individual author only provides a \"few\" samples, I don't know how relevant the technique can be for individual usage. I expect journals or any document archive to be more likely to be \"clients\" of this approach. Anyway, this is still research, so there is no off-the-shelf software/service available for now, as far as I know.</p>",
        "id": 531826666,
        "sender_full_name": "Peva Blanchard",
        "timestamp": 1753877489
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276777\">Mike Shulman</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531793942\">said</a>:</p>\n<blockquote>\n<p>That's very interesting!  I see that they do also supply an AI, which seems to contradict their goal of \"humanizing the web\", but I gather from your remarks that you can turn it off. </p>\n</blockquote>\n<p>I must have turned it off immediately, because I never see it.</p>\n<blockquote>\n<p>How does Kagi compare to Google with <a href=\"https://udm14.com/\">udm14</a>, which disables ads and AI?</p>\n</blockquote>\n<p>I'll have to compare them for a while.  So far they look comparable except udm14 doesn't have those various filter settings.   Kagi claims to have boolean search but it seems to be working erratically - maybe I'm not using it right.   It also claims you can search by \"least relevant first\", which is hilarious.</p>",
        "id": 531837180,
        "sender_full_name": "John Baez",
        "timestamp": 1753880443
    },
    {
        "content": "<p>Here's Cory Doctorow on Google and related things.    (It's long, but folks can skip it if they don't care.)</p>\n<blockquote>\n<p>That's where Ardoline and Lenzo's work comes in. They both document the ways in which we turn these online services into cognitive prostheses, and then investigate how the enshittification of these services ends up making us stupider, by taking away the stuff that helps us think. They're drawing a line between platform decay and cognitive decay.</p>\n<p>The authors look at examples like the enshittification of Google Search, a product that Google has deliberately and irretrievably enshittified:</p>\n<p><a href=\"https://pluralistic.net/2024/04/24/naming-names/#prabhakar-raghavan\">https://pluralistic.net/2024/04/24/naming-names/#prabhakar-raghavan</a></p>\n<p>The web is a giant cognitive prosthesis, and early web tools put a lot of emphasis on things like bookmark management and local caching, so that the knowledge and cognition you externalized to the web were under your control. But Google Search was so goddamned magic – before they cynically destroyed it – that a lot of us switched from \"not remembering things because you have a bookmark that takes you to a website that remembers it for you\" to \"not remembering things and not remembering where to find them, and just typing queries into Google.\" The collapse of Google into a giant pile of shit is like giving every web user a traumatic brain injury.</p>\n<p>It's a good paper, but I think the situation is actually more dire than the paper makes it out to be, thanks to the AI bubble –</p>\n<p>Wait! I'm not actually going to talk about what AI can do (which is a combination of a small set of boring useful things, a bunch of novelties, and a long list of things that AI can't do but is being used to do anyway). I'm talking about the financial fraud that AI serves.</p>\n<p>Tech companies <em>must</em> be perceived as growing, because when a company is growing, it is valued <em>far</em> more highly than a company is once it has \"matured.\" This is called the \"price to earnings ratio\" – the number of dollars investors are willing to pay for the company compared to the number of dollars a company is bringing in. So long as a company is growing, the PE ratio is very high, and this helps the company to <em>actually</em> grow. That's because the shares in growing companies are highly liquid, and can be traded for equity in other companies and/or the labor of key employees, meaning that growth companies can almost always outbid their mature counterparts when it comes to expanding through acquisition and hiring. That means that while a company is growing, its PE ratio can help it <em>keep</em> growing.</p>\n<p>But here's the corollary: when a growth company <em>stops</em> growing, its shares are suddenly and violently revalued as though they were shares in a mature company, which tanks the personal net worth of the company's top managers and key employees (whose portfolios are stuffed with their employer's now-plummeting stock). Worse: in order to retain those employees and hire more (or to acquire key companies), the no-longer-growing company has to pay with cash, which is <em>much</em> harder to get than its own shares. Even worse: they have to bid against <em>growing</em> companies.</p>\n<p>A growth company is like an airplane that has two modes: climbing and nose-diving, and while it's easy to go from climbing to crashing, it's <em>much</em> harder to go the other way. Ironically, the moment at which a company's growth is most likely to stall is right after its greatest triumph: after a company conquers its market, it has nowhere else to go. Google's got a 90% Search market-share – how can it possibly grow Search?</p>\n<p>It can't (just like Meta can't really grow social, and Microsoft can't grow office suites, etc), so it has to convince Wall Street that it has a shot at conquering some <em>other</em> market that the street perceives as unimaginably vast and thus capable of keeping the growth engine going. Tech has pulled a lot of sweaty tricks to create this impression, inflating bubbles like \"pivot to video\" and \"metaverse\" and \"cryptocurrency,\" and now it's AI.</p>\n<p>The problem is that AI just isn't very popular. People go out of their way to avoid AI products:</p>\n<p><a href=\"https://www.tandfonline.com/doi/full/10.1080/19368623.2024.2368040\">https://www.tandfonline.com/doi/full/10.1080/19368623.2024.2368040</a></p>\n<p>For an AI-driven growth story to work, tech companies have to produce a stream of charts depicting lines that go up and to the right, reflecting some carefully chosen set of metrics demonstrating AI's increasing popularity. One way to produce these increasing trend-lines on demand is to replace all the most commonly used parts of a service that you love and rely on with buttons that summon an AI. This is the \"fatfinger AI economy,\" a set of trendlines produced by bombarding people who graze their screens with a stray fingertip with a bunch of AI bullshit, so you can claim that your users are \"engaging\" with AI:</p>\n<p><a href=\"https://pluralistic.net/2025/05/02/kpis-off/#principal-agentic-ai-problem\">https://pluralistic.net/2025/05/02/kpis-off/#principal-agentic-ai-problem</a></p>\n<p>It's a form of \"twiddling\" – changing how a service works on a per-user, per-interaction basis in order to shift value from the user to the company:</p>\n<p><a href=\"https://pluralistic.net/2023/02/19/twiddler/\">https://pluralistic.net/2023/02/19/twiddler/</a></p>\n<p>Twiddling represents <em>the</em> big cognitive hazard from enshittification during the AI bubble: the parts of your UI that matter most to you are the parts that you use as vital cognitive prostheses. A product team whose KPI is \"get users to tap on an AI button\" is going to use the fine-grained data they have on your technological activities to preferentially target these UI elements that you rely on with AI boobytraps. You are too happy, so they are leaving money on the table, and they're coming for it.</p>\n<p>This is a form of \"attention rent\": the companies are taxing your muscle-memory, forcing you to produce deceptive usage statistics at the price of either diverting your cognition from completing a task to hunt around for the button that banishes the AI and lets you get back to what you were doing; or to simply abandon that cognitive prosthesis:</p>\n<p><a href=\"https://pluralistic.net/2023/11/03/subprime-attention-rent-crisis/#euthanize-rentiers\">https://pluralistic.net/2023/11/03/subprime-attention-rent-crisis/#euthanize-rentiers</a></p>\n<p>It's <em>true</em> \"engagement-hacking\": not performing acts of dopamine manipulation; but rather, spying on your habitual usage of a digital tool in order to swap buttons around in order to get you to make a number go up. It's exploiting the fact that you engage with something useful and good to make it less useful and worse, because if you're too happy, some enshittifier is leaving money on the table.</p>\n</blockquote>",
        "id": 531837805,
        "sender_full_name": "John Baez",
        "timestamp": 1753880606
    },
    {
        "content": "<p>I think there is a new danger in a kind of \"crank singularity\" happening.</p>\n<p>I'll admit I am not an experienced crankologist with decades under my belt like Dr. Baez, but I've noticed there is a stark difference in two different classes of cranks. I often stop by reddit and view the local \"alt-physics\" subreddits. I refer to these as \"crank aquariums\".</p>\n<p>The \"lower cranks\" are mostly mystical, don't know much math and talk about the typical \"woo\" topics. Consciousness collapses the wavefunction, sacred geometry, you know the drill.</p>\n<p>The \"higher cranks\" often use very sophisticated math to prove \"new theorems\". But their math is PDE heavy and fundamentally brittle. They discover \"new terms\" that Maxwell and Schrodinger \"forgot\" in their equations. They never use things like category theory, homology, moduli spaces, Lie groups, etc. They only do very heavy analysis.</p>\n<p>The problem is that with new LLMs, these two separate classes of cranks could merge into a new form of \"hybridized super-crank\" generating endless reams of \"self-conscious quantum operator algebras\" and \"quasi-cosmic graviton quantum field theories\" with actual PDEs that are potentially sophisticated enough to overwhelm hapless journal editors.</p>\n<p>Maybe there could be an additional filter (Category Theory CAPCHA?) of some kind where people hoping to publish could go into front of an AI interviewer and answer randomly generated questions about group theory, topology, cohomology classes, functors and sheaves. If you pass you receive a badge of some kind (this used to be referred to as a \"degree\" I believe?) These kinds of topics are usually too abstract for cranks to actually understand, so it could be a \"mental block\".</p>\n<p>Anyway, I'm partly spitballing here, but the true lesson to remember is that mathematics chases deep structure, whereas cranks may only imitate it shallowly. (I am serious about that, but I felt <em>some</em> comic relief is also in order here. I hope the mods won't exile me to the crank aquarium!)</p>",
        "id": 532396447,
        "sender_full_name": "Ben Kaminsky",
        "timestamp": 1754093957
    },
    {
        "content": "<p>I'll add my piece of comic relief. There's a certain resistance in flagging some content as AI generated slop, for a similar reason that people are very wary of flagging a text as plagiarism... No problem, I can do it <span aria-label=\"smiling devil\" class=\"emoji emoji-1f608\" role=\"img\" title=\"smiling devil\">:smiling_devil:</span> I have a certain experience and it's very pleasurable for me to tell someone who deserves it \"shut up and com me back when you know what a determinant is\"</p>",
        "id": 532417410,
        "sender_full_name": "fosco",
        "timestamp": 1754111274
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"930833\">Ben Kaminsky</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532396447\">said</a>:</p>\n<blockquote>\n<p>I think there is a new danger in a kind of \"crank singularity\" happening.</p>\n</blockquote>\n<p>It's happening.  I'm getting <em>many</em> more emails from cranks, who are mostly working with LLMs to develop their 'theories'.   A quote from one of these emails:</p>\n<p>\"developed rigorously with the help of large language models\"</p>\n<p><span aria-label=\"rolling eyes\" class=\"emoji emoji-1f644\" role=\"img\" title=\"rolling eyes\">:rolling_eyes:</span> </p>\n<blockquote>\n<p>The \"higher cranks\" often use very sophisticated math to prove \"new theorems\". But their math is PDE heavy and fundamentally brittle. </p>\n</blockquote>\n<p>I haven't seen any cranks using very sophisticated math.  Some <em>pretend</em> to do so.   And I agree with you that the number is dramatically increasing now that fake math is easy to get from a LLM.  Luckily anyone who really knows math can see this stuff is fake.</p>",
        "id": 532431169,
        "sender_full_name": "John Baez",
        "timestamp": 1754122960
    },
    {
        "content": "<p>maybe this has been brought up before, but what about adding \"community notes\" to the arxiv, like how it works on twitter?  That's how twitter was able to pass moderation responsibility onto the public at scale.</p>",
        "id": 532480521,
        "sender_full_name": "Ryan Wisnesky",
        "timestamp": 1754161995
    },
    {
        "content": "<p>This is a dangerous plan, for several reasons that people noticed about 15 minutes after first thinking of this idea a couple of decades ago.  Some still favor it, but the arXiv moderators aren't going to take those chances.</p>",
        "id": 532548885,
        "sender_full_name": "John Baez",
        "timestamp": 1754222010
    },
    {
        "content": "<p>Note however that anyone can start their own \"arXiv reviews\".</p>",
        "id": 532548959,
        "sender_full_name": "John Baez",
        "timestamp": 1754222064
    },
    {
        "content": "<p>just out of curiosity, why would it not work for the arxiv (what are those reasons?) if it does work for twitter (or maybe it doesn't work for twitter?)?  the only thing I can think of off the top of my head is that there is too small of a \"public of experts\" for the community notes to be accurate</p>",
        "id": 532588555,
        "sender_full_name": "Ryan Wisnesky",
        "timestamp": 1754247616
    },
    {
        "content": "<p>My impression is that it's at least questionable whether it works for twitter.</p>",
        "id": 532593382,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1754251004
    },
    {
        "content": "<p>Although I don't use twitter myself so I can't say of personal experience.</p>",
        "id": 532593399,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1754251018
    },
    {
        "content": "<p>I'm also curious. Though I'm no longer on X/Twitter, my impression is that community notes continued to function quite well even as the platform deteriorated in other respects, pretty reliably flagging false or misleading posts.</p>",
        "id": 532593403,
        "sender_full_name": "Evan Patterson",
        "timestamp": 1754251021
    },
    {
        "content": "<p>Imagine if everyone got to say shit about each other's papers on the arXiv.  It would be a bloodbath.  It would quickly degenerate into obscenities and lawsuits unless the comments were moderated.  Some of those lawsuits would even target the arXiv itself.  <em>All</em> of this is the last thing the arXiv moderators want.  They don't have time for this.</p>",
        "id": 532594434,
        "sender_full_name": "John Baez",
        "timestamp": 1754251761
    },
    {
        "content": "<p>To be successful, such an approach would need to carefully circumscribe the allowed claims for a community note. The allowed claims would <em>not</em> include random opinions about or reviews of papers. Rather, the point would be to use the community to reach consensus on matters of fact, such as:</p>\n<ul>\n<li>was this paper largely/completely generated by an LLM?</li>\n<li>does this paper plagiarize from existing sources?</li>\n<li>is this paper a \"crank\" paper, in the specific sense that it fails to conform to widely shared norms about mathematical/scientific communication?</li>\n</ul>",
        "id": 532595032,
        "sender_full_name": "Evan Patterson",
        "timestamp": 1754252198
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275965\">Evan Patterson</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532595032\">said</a>:</p>\n<blockquote>\n<p>such an approach would need to carefully circumscribe the allowed claims for a community note</p>\n</blockquote>\n<p>To enforce that, all the comments would have to be moderated, right?</p>",
        "id": 532599213,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1754254956
    },
    {
        "content": "<p>The arXiv staff might prefer to spend their limited time/energy/money on moderating papers rather than moderating comments on papers.</p>\n<p>I suppose one approach to avoid moderating comments would be a form which allowed no freely written text, just yes/no answers to questions like \"this paper is AI-generated\".   This would have its own problems.</p>",
        "id": 532600252,
        "sender_full_name": "John Baez",
        "timestamp": 1754255607
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532600252\">said</a>:</p>\n<blockquote>\n<p>The arXiv staff might prefer to spend their limited time/energy/money on moderating papers</p>\n</blockquote>\n<p>And apparently they don't even have enough time/energy/money to do a good enough job of that, which is what led to this whole conversation.  (Not intended as a criticism of them, just an observation about lack of resources.)</p>",
        "id": 532601705,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1754256587
    },
    {
        "content": "<p>I don't suppose any agency would be likely to award a grant to support work to keep AI-generated slop off of the arXiv.</p>",
        "id": 532601852,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1754256670
    },
    {
        "content": "<p>here's how it works on twitter:  The Community Notes algorithm publishes notes based on agreement from contributors who have a history of disagreeing.[21] Rather than based on majority rule,[34] the program's algorithm prioritizes notes that receive ratings from a \"diverse range of perspectives\".[28][35] For a note to be published, a contributor must first propose a note under a tweet.[21] The program assigns different values to contributors' ratings, categorising users with similar rating histories as a form of \"opinion classification\", determined by a vague alignment with the left and right-wing political spectrum. The bridging-based machine-learning algorithm requires ratings from both sides of the spectrum in order to publish notes, that can have the intended effect of decreasing interaction with such content.[35][36][37]</p>\n<p>Contributors are volunteers with access to an interface from which they have the ability to monitor tweets and replies that may be misleading.[21][9][38] Notes in need of ratings by contributors are located under a \"Needs your help\" section of the interface. Other contributors then give their opinion on the usefulness of the note, identifying notes as \"Helpful\" or \"Not Helpful\".[21][39] The contributor gets points if their note is validated,[40][21] known as \"Rating Impact\", that reflects how helpful a contributors' ratings have been.[39][41][42] X users are able to vote on whether they find notes helpful or not,[18] but must apply to become contributors in order to write notes, the latter being restricted by \"Rating Impact\" as well as the Community Notes guidelines.[39][41]</p>",
        "id": 532603451,
        "sender_full_name": "Ryan Wisnesky",
        "timestamp": 1754257834
    },
    {
        "content": "<p>OK, so how many people in the world do you think a) will have a legitmate informed opinion on a somewhat niche research (sub)field and b) will engage in the commenting process on arXiv papers? Twitter notes worked (or \"worked\") because of scale. If you have a hundred thousand people, a million people, engaging on a topic that doesn't require PhD-level education to understand even the words, then this type of approach might achieve some level of community consensus.</p>",
        "id": 532610646,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1754262854
    },
    {
        "content": "<p>This reminds me of the “wisdom of the crowd” phenomenon!</p>",
        "id": 532613451,
        "sender_full_name": "Ruby Khondaker (she/her)",
        "timestamp": 1754264798
    },
    {
        "content": "<p>maybe it would be enough for the experts to be more numerous than the crackpots, rather than needing huge numbers of experts? (hopefully there are more experts than crackpots, but I have no idea tbh). But I suppose the discussion is moot without the arxiv actually doing it.</p>",
        "id": 532631112,
        "sender_full_name": "Ryan Wisnesky",
        "timestamp": 1754275714
    },
    {
        "content": "<p><a href=\"https://www.daniellitt.com/blog/2025/7/17/arxiv-in-trouble\">https://www.daniellitt.com/blog/2025/7/17/arxiv-in-trouble</a></p>",
        "id": 532648279,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1754285114
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"281326\">Ryan Wisnesky</span> <a href=\"#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/532631112\">said</a>:</p>\n<blockquote>\n<p>maybe it would be enough for the experts to be more numerous than the crackpots, rather than needing huge numbers of experts? (hopefully there are more experts than crackpots, but I have no idea tbh). But I suppose the discussion is moot without the arxiv actually doing it.</p>\n</blockquote>\n<p>I feel like almost definitionally there would have to be more crackpots than experts, owing to the relative difficulty in becoming either?</p>",
        "id": 532650306,
        "sender_full_name": "Ruby Khondaker (she/her)",
        "timestamp": 1754286631
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532594434\">said</a>:</p>\n<blockquote>\n<p>Imagine if everyone got to say shit about each other's papers on the arXiv.  It would be a bloodbath.  It would quickly degenerate into obscenities and lawsuits unless the comments were moderated.</p>\n</blockquote>\n<p>I think you're too pessimistic John, people tend to be decent 99% of the time, especially if their full name is on display. I agree there would be the need to moderate the remaining 1% though.</p>",
        "id": 532667301,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1754293827
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"934428\">Ruby Khondaker (she/her)</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532650306\">said</a>:</p>\n<blockquote>\n<p>I feel like almost definitionally there would have to be more crackpots than experts, owing to the relative difficulty in becoming either?</p>\n</blockquote>\n<p>I assumed the only people allowed to play this game would be people who have been endorsed to write papers on the arXiv.   In this population there are more experts than crackpots.... though not everyone is an expert on every topic, indeed quite the opposite. </p>\n<p>If you let random passers-by evaluate arXiv papers, there will definitely be lots of crackpots and people with grudges and other unproductive motivations.</p>",
        "id": 532667322,
        "sender_full_name": "John Baez",
        "timestamp": 1754293838
    },
    {
        "content": "<p>Yeah of course! And it'd be spamland very quickly...</p>",
        "id": 532667606,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1754293939
    },
    {
        "content": "<p>I think the whole problem is addressed by fixing the endorsement system. If arXiv is unable to moderate by themselves (which appears to be the case), then they need to either hire more people, or ask for trusted volunteers.</p>",
        "id": 532669299,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1754294500
    },
    {
        "content": "<p>So much of the academic publishing system already depends upon volunteers, it doesn't feel like a stretch to have people contributing to moderating arXiv. It'd be great to have it become a place of recorded scientific discussions re a piece of work, including errata, reviews, and comments, so that it can also be a starting point for journal reviews.</p>",
        "id": 532675539,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1754296434
    },
    {
        "content": "<p>The arXiv does a lot of moderating, and it's sometimes too strict: the case of Phillip Helbig comes to mind:</p>\n<ul>\n<li>Phillip Helbig, <a href=\"https://johncarlosbaez.wordpress.com/2022/02/04/submission-to-arxiv/\">Submission to arXiv</a>, <em>Azimuth</em>.</li>\n</ul>\n<p>I've had a paper shifted from the group theory section to combinatorics against my will.   I think their problem with AI-generated papers is that they're not used to filtering out papers of this sort.  Most crackpots write in a way that sends off a particular vibe, which is easy for experienced moderators to detect.  But LLMs are different.  I think one can learn to detect them, <em>at least so far</em>.</p>",
        "id": 532677607,
        "sender_full_name": "John Baez",
        "timestamp": 1754297104
    },
    {
        "content": "<p>In comments awaiting moderation (heh) on Peter Woit's blog that pointed out Daniel Litt's blog post on the topic, I wrote</p>\n<blockquote>\n<p>I would like to see anyone whose papers were deemed to be AI-generated have all their endorsements stripped, and they should need to get fresh endorsements, probably more than one.</p>\n</blockquote>\n<blockquote>\n<p>Moreover, I would even go so far as to propose that anyone who endorses an AI-generated paper should have their endorser-status reset, so that they cannot immediately re-endorse the person who they originally endorsed, until they have submitted more papers as usual. It would be a bit of an incentive to actually look at the paper for fear of a relatively harmless removal of a privilege. Active researchers would get back to having endorser powers before too long...</p>\n</blockquote>",
        "id": 532681838,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1754298420
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532677607\">said</a>:</p>\n<blockquote>\n<p>The arXiv does a lot of moderating, and it's sometimes too strict</p>\n</blockquote>\n<p>Clearly not enough (though being sometimes too strict is another problem).</p>",
        "id": 532687391,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1754300093
    },
    {
        "content": "<p>I don't think a sheer increase in quantity is the best solution, given that the arXiv has an approximately zero budget for doing this.   Moderation needs to be focused on the key problems.  Now that AI is a big problem, the moderators need to be pointed to AI-generated papers, and they need to learn to spot them.  They do look at every paper, I believe.</p>",
        "id": 532690328,
        "sender_full_name": "John Baez",
        "timestamp": 1754301021
    },
    {
        "content": "<blockquote>\n<p>I don't think a sheer increase in quantity is the best solution, given that the arXiv has an approximately zero budget for doing this.</p>\n</blockquote>\n<p>That would be solved by having volunteers. It would not take many volunteers for each category, and I think this would be relatively easy to achieve, as I think many people would be willing to help.</p>",
        "id": 532703725,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1754305611
    },
    {
        "content": "<p>And now we're back full circle to the question we started with: how do we contact the arXiv moderators and get them to do something?</p>",
        "id": 532703874,
        "sender_full_name": "Mike Shulman",
        "timestamp": 1754305653
    },
    {
        "content": "<p>Possibly someone could email <a href=\"mailto:moderators@arxiv.org\">moderators@arxiv.org</a> (mentioned on <a href=\"https://info.arxiv.org/help/moderation/index.html\">https://info.arxiv.org/help/moderation/index.html</a>) and see whether they can give any information?</p>",
        "id": 532706774,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1754306748
    },
    {
        "content": "<p>One can just email them.  The moderators are listed <a href=\"https://arxiv.org/moderators/\">here</a> and the people in charge are listed starting <a href=\"https://info.arxiv.org/about/people/index.html\">here</a>.  </p>\n<p>They seem fairly quiet and secretive.   People with problems report getting little response.  When I emailed some of the leaders about the virtues of getting a backup hosted outside the US, they never replied.   They did develop such a backup system.  But I'm not claiming they did that in response to my email.</p>\n<p>It probably helps a lot to contact someone in charge whom you know personally.  I could contact Jacques Distler, for example.   I'm betting he'll say they are already familiar with the AI problem.</p>",
        "id": 532707106,
        "sender_full_name": "John Baez",
        "timestamp": 1754306880
    },
    {
        "content": "<p>I find the lack of any official statement from arXiv on the matter a little disappointing, if they are aware of it.</p>",
        "id": 532707944,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1754307226
    },
    {
        "content": "<p>They tend to talk as little as possible.</p>",
        "id": 532711954,
        "sender_full_name": "John Baez",
        "timestamp": 1754308643
    },
    {
        "content": "<p>I'm a smidge nervous that this is getting awfully close to the point of someone like this actually getting a grant from a place like Templeton.</p>",
        "id": 532781938,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1754334279
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/532711954\">said</a>:</p>\n<blockquote>\n<p>We contend that the result—autoequivalence with the Monster Group—is statistically improbable unless the theory holds validity. Consequently, AI consensus would not have been achieved erroneously.</p>\n</blockquote>\n<p>My eyes are bleeding <span aria-label=\"skull\" class=\"emoji emoji-1f480\" role=\"img\" title=\"skull\">:skull:</span><span aria-label=\"skull\" class=\"emoji emoji-1f480\" role=\"img\" title=\"skull\">:skull:</span></p>",
        "id": 532859335,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1754381447
    },
    {
        "content": "<p>'What are the chances I lost the lottery with this very specific ticket? So low it must be the lottery which is wrong'</p>",
        "id": 532859517,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1754381500
    },
    {
        "content": "<p>Yes, this would be hilarious if it were intended as a joke.    I also like the misuse of \"autoequivalence\" - which usually means an equivalence of something with <em>itself</em> - in the claim that U-category theory (whatever that is) is autoequivalent with the Monster Group.</p>",
        "id": 532860404,
        "sender_full_name": "John Baez",
        "timestamp": 1754381771
    },
    {
        "content": "<blockquote>\n<p>They tend to talk as little as possible.</p>\n</blockquote>\n<p><em>[...] With its customary discretion, the Company did not reply directly; instead, it scrawled its brief argument in the rubble of a mask factory. [...]</em></p>",
        "id": 532865604,
        "sender_full_name": "fosco",
        "timestamp": 1754383260
    },
    {
        "content": "<p>Blimey, it looks like some overcomplicated topics for 1st year students.<br>\n<a href=\"/user_uploads/21317/JnuRrz16uz1RSqNwxlLekz1Y/Screenshot-2025-08-05-at-14.39.43.png\">Screenshot 2025-08-05 at 14.39.43.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/21317/JnuRrz16uz1RSqNwxlLekz1Y/Screenshot-2025-08-05-at-14.39.43.png\" title=\"Screenshot 2025-08-05 at 14.39.43.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"1662x1368\" src=\"/user_uploads/thumbnail/21317/JnuRrz16uz1RSqNwxlLekz1Y/Screenshot-2025-08-05-at-14.39.43.png/840x560.webp\"></a></div>",
        "id": 532919217,
        "sender_full_name": "Daniel Rogozin",
        "timestamp": 1754401252
    },
    {
        "content": "<p>Here's a counter-point from twitter: \"I don’t get why Arxiv containing slop is a bad thing. I mean sure it’s frustrating and annoying, but merely having been put on Arxiv should give a paper draft exactly zero additional credibility\".  I suppose I feel the same way; I always thought of the arxiv as simply a substitute for putting pdfs on a personal website, with endorsement meant to keep the arxiv from turning into a public API for storing PDFs as opposed to technical vetting.</p>",
        "id": 532942336,
        "sender_full_name": "Ryan Wisnesky",
        "timestamp": 1754408396
    },
    {
        "content": "<p>arXiv is a tool for researchers. It becomes useless if there is absolutely no moderation. If anyone can upload whatever they like, you might as well refer to the <a href=\"https://libraryofbabel.info/\">Library of Babel</a> instead.</p>",
        "id": 532948882,
        "sender_full_name": "Nathanael Arkor",
        "timestamp": 1754410680
    },
    {
        "content": "<p>\"I don't see why this thing that two bad adjectives apply to is bad\"</p>",
        "id": 532950769,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1754411366
    },
    {
        "content": "<p>Besides being frustrating and annoying: Almost everything on arXiv is real research, which is a huge benefit for discoverability, and does actually mean that something being on arXiv increases its credibility far over \"a random thing on the Internet.\" Both those values could be mostly destroyed by too high a slop ratio.</p>",
        "id": 532951144,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1754411508
    },
    {
        "content": "<p>Yes, I too don't want all the pseudoscientific vomit in the world to be on the arXiv.   That's called the <em>internet</em>.</p>\n<p>When I search for papers with a given keyword on the arXiv, I want a majority of them to actually make sense!  And they do, so far.  </p>\n<p>The arXiv is a tremendously useful tool, which I use several times a day, largely because of what's <em>not</em> on it.</p>",
        "id": 532952547,
        "sender_full_name": "John Baez",
        "timestamp": 1754412058
    },
    {
        "content": "<p>For now, I'd expect custom tools devoted to detecting AI to still work.  If the detect/evade war gets to the point where AI is providing LEAN proofs of its work, I think that's a win.</p>\n<ul>\n<li><a href=\"https://github.com/mbzuai-nlp/DetectLLM\">https://github.com/mbzuai-nlp/DetectLLM</a></li>\n<li><a href=\"https://arxiv.org/abs/2301.11305\">https://arxiv.org/abs/2301.11305</a></li>\n<li><a href=\"https://www.scribbr.com/ai-tools/best-ai-detector/\">https://www.scribbr.com/ai-tools/best-ai-detector/</a></li>\n</ul>",
        "id": 535171697,
        "sender_full_name": "Mike Stay",
        "timestamp": 1755623274
    },
    {
        "content": "<p>See Figure 3 in this paper <a href=\"https://arxiv.org/abs/2410.02457\">https://arxiv.org/abs/2410.02457</a> It's a midjourney dream of what a mathematical image should look like.</p>",
        "id": 535228109,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1755656513
    },
    {
        "content": "<p>you're talking about figure 3, but I would like to draw your attention to page 2....</p>",
        "id": 535265213,
        "sender_full_name": "fosco",
        "timestamp": 1755679052
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276422\">David Michael Roberts</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/535228109\">said</a>:</p>\n<blockquote>\n<p>See Figure 3 in this paper <a href=\"https://arxiv.org/abs/2410.02457\">https://arxiv.org/abs/2410.02457</a> It's a midjourney dream of what a mathematical image should look like.</p>\n</blockquote>\n<p>The escalation from Figs 1--2 to Fig 3 gave me a good laugh.</p>",
        "id": 535295077,
        "sender_full_name": "Josh Chen",
        "timestamp": 1755690157
    },
    {
        "content": "<p><a href=\"https://arxiv.org/abs/2510.17829\">This work</a> claims a full lean verification of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo mathvariant=\"normal\">≠</mo><mi>N</mi><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P\\neq NP</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\"><span class=\"mrel\"><span class=\"mord vbox\"><span class=\"thinbox\"><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"inner\"><span class=\"mord\"><span class=\"mrel\"></span></span></span><span class=\"fix\"></span></span></span></span></span><span class=\"mrel\">=</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">NP</span></span></span></span>, but only provides snippets of the code in their paper that reads like the output of an LLM.</p>",
        "id": 546399334,
        "sender_full_name": "Martti Karvonen",
        "timestamp": 1761123055
    },
    {
        "content": "<p>I saw the title and comments in the daily arXiv mailing, and <em>knew</em> it was LLM-generated.</p>",
        "id": 546409368,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1761126024
    },
    {
        "content": "<p>Opening it to find many, many short subsections consisting just of dot points was almost not needed.</p>",
        "id": 546409473,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1761126051
    },
    {
        "content": "<p>Out of curiosity, I went straight to the technical part (Sect. 3) and randomly checked one of  the results (Theorem 3.14), finding that it is obviously false.  (The category <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"bold\">C</mi><mi mathvariant=\"bold\">o</mi><mi mathvariant=\"bold\">m</mi><mi mathvariant=\"bold\">p</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Comp}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8805em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">Comp</span></span></span></span></span> mentioned in the theorem is formally introduced and makes sense per se, but it is certainly <em>not</em> additive with the proposed definition).</p>",
        "id": 546413701,
        "sender_full_name": "Damiano Mazza",
        "timestamp": 1761127222
    },
    {
        "content": "<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>",
        "id": 546438780,
        "sender_full_name": "Elisha Goldman",
        "timestamp": 1761134325
    },
    {
        "content": "<p>The Lean code is not publically available as far as I can tell, so we only have the word of the author/LLM that such a verification exists.</p>",
        "id": 546440181,
        "sender_full_name": "Martti Karvonen",
        "timestamp": 1761134750
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"971326\">Elisha Goldman</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546438780\">said</a>:</p>\n<blockquote>\n<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>\n</blockquote>\n<p>Just to note, while it's highly likely the Lean code is proof of _something_ (assuming it's not just full of <code>sorry</code>s), (un)subtly incorrect definitions and theorem statements mean the code almost surely does not prove P /= NP.</p>\n<p>[Back in the day, someone won a full bitcoin for proving false in Rocq. They did this by simply defining <code>False := True</code> and then proving \"false\" according to this new definition. The moral of this story is that theorem provers are not really designed to be adversary-proof and mechanized proofs are only as good as the formalized statement is]</p>",
        "id": 546466317,
        "sender_full_name": "daniel gratzer",
        "timestamp": 1761141722
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"277285\">Martti Karvonen</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546440181\">said</a>:</p>\n<blockquote>\n<p>The Lean code is not publically available as far as I can tell, so we only have the word of the author/LLM that such a verification exists.</p>\n</blockquote>\n<p>It's in the paper itself, it just takes a bit to get there (the main proof is on page 40)</p>",
        "id": 546466591,
        "sender_full_name": "Elisha Goldman",
        "timestamp": 1761141793
    },
    {
        "content": "<p>Lol, I wanna hear more about the Rocq story</p>",
        "id": 546467088,
        "sender_full_name": "Elisha Goldman",
        "timestamp": 1761141916
    },
    {
        "content": "<p>Here are <a href=\"https://yoichihirai.com/edcon-yoichi-hirai.pdf\">some slides</a> by the person who created the site (and lost the bitcoin). I can't find the original cite now searching quickly, but more of the story is here. Gosh... can't believe this was all already 10 years ago.</p>",
        "id": 546468666,
        "sender_full_name": "daniel gratzer",
        "timestamp": 1761142280
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"971326\">Elisha Goldman</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546466591\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"277285\">Martti Karvonen</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546440181\">said</a>:</p>\n<blockquote>\n<p>The Lean code is not publically available as far as I can tell, so we only have the word of the author/LLM that such a verification exists.</p>\n</blockquote>\n<p>It's in the paper itself, it just takes a bit to get there (the main proof is on page 40)</p>\n</blockquote>\n<p>Oh oops, I hadn't realized they seemingly give all of it in bits and pieces, rather than just showing an excerpt or two.</p>",
        "id": 546542529,
        "sender_full_name": "Martti Karvonen",
        "timestamp": 1761163823
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"971326\">Elisha Goldman</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546438780\">said</a>:</p>\n<blockquote>\n<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>\n</blockquote>\n<p>To be more specific, people usually misunderstand things in very vague ways, so being able to see exactly what they misunderstand here seems useful pedagogically since it shows where students might be confused.<br>\nAlso, even if it doesn't say what they think it does, the fundamental idea (computational complexity via homology) seems novel and is formally-verified non-gibberish, so maybe some form of it could be interesting (though I definitely don't know enough CS to say anything meaningful here)</p>",
        "id": 546564071,
        "sender_full_name": "Elisha Goldman",
        "timestamp": 1761174225
    },
    {
        "content": "<p>Imo the best thing to tell cranks is to provide a formal verification, it filters out the slop and helps them realize where they're wrong/too vague<br>\n(the other response is getting annoyed, which doesn't really help anyone)</p>",
        "id": 546564869,
        "sender_full_name": "Elisha Goldman",
        "timestamp": 1761174668
    },
    {
        "content": "<p>Interestingly, in the formalization section, the author does not claim to have formally verified the result on additivity of Comp that <span class=\"user-mention\" data-user-id=\"276839\">@Damiano Mazza</span> correctly points out is obviously false.</p>",
        "id": 546569105,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1761177616
    },
    {
        "content": "<p>Elisha, I'm afraid you are massively begging the question by assuming that \"the fundamental idea\" exists. It seems to me that this paper constitutes nothing less than an adversarial attack on the scientific community, or at least on its non-expert fringe who cannot check its claims for themselves.</p>",
        "id": 546571175,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1761179086
    },
    {
        "content": "<p>I spent almost an hour poking through here carefully to see where the more central claims begin to fall apart. Theorems 3.24 and 4.1 brazenly contradict each other, proving respectively that problems in P are homologically trivial and that all NP-complete problems are homologically isomorphic to all problems in NP. Even more to the point, the <em>proof</em> of 3.24 really shows the lie where it says \"The detailed argument uses the functoriality of the computational homology construction and the fact that homology isomorphisms preserve the 'computational topology' of problems.\" The last claim is, naturally, not mathematically defined. The computational chain complex also appears not to be genuinely defined, as far as I can tell. I haven't compared to see what the author chucked into the formalized definition.</p>",
        "id": 546571332,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1761179220
    },
    {
        "content": "<p>Hopefully nobody here is surprised that this is entirely nonsense, but I think it was still worth looking at a little more closely to continue tracking the progress of these adversarial attacks. I was just discussing the notion of a slop artist (whether or not they believe their own slop) trying to justify themselves via submitting formal verifications with a friend last week, and here we are now! This is a clear big step less obviously wrong than the papers we were discussing even a couple of months ago.</p>",
        "id": 546571425,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1761179299
    },
    {
        "content": "<p>OK, looking at the formalization, the author specifically attempts to bamboozle the reader by exhibiting a detailed-looking verification of the chain complex property of an alleged chain complex which would, if it existed, be the singular chain complex of the graph of \"computation paths\" in a computational problem; but a computation path <em>itself</em> depends on some concept of a \"Configuration\" of a verifier of a computational problem which is never actually defined.</p>",
        "id": 546571860,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1761179669
    },
    {
        "content": "<p>I think this work reduces upon inspection to ordinary crankery about \"imagine the simplicial complexes of all efficient computation paths, man, and then, like, the cycles in this complex would represent the irreducible computational steps, bro!\" But it's really buried quite a lot deeper than it otherwise would be with all the LLM help.</p>",
        "id": 546571861,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1761179670
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"609515\">Kevin Carlson</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546571175\">said</a>:</p>\n<blockquote>\n<p>Elisha, I'm afraid you are massively begging the question by assuming that \"the fundamental idea\" exists. It seems to me that this paper constitutes nothing less than an adversarial attack on the scientific community, or at least on its non-expert fringe who cannot check its claims for themselves.</p>\n</blockquote>\n<p>Ah, well, that's what I get for only skimming it, fair enough</p>",
        "id": 546580875,
        "sender_full_name": "Elisha Goldman",
        "timestamp": 1761187759
    },
    {
        "content": "<p>You're right though, my internal model of the crank was that they start as a passionate amateur, invest way too much ego in their theories, and only then become antagonistic due to perceived rejection; and that formal verification can replace the social rejection with a learning experience where they find their mistake and salvage what they can. I hadn't really considered any deliberate deception (aside from self-deception). The psychology must be different, I guess the antagonism comes before actually appreciating the subject, maybe for political reasons, like Sokal?</p>",
        "id": 546582792,
        "sender_full_name": "Elisha Goldman",
        "timestamp": 1761189518
    },
    {
        "content": "<p>There are very sad stories of people being completely sent off the deep end by AI-hallucinated theories of everything/conspiracies. Something like this is rather mild by comparison. I don't think the author doubts this paper at all.</p>",
        "id": 546583103,
        "sender_full_name": "David Michael Roberts",
        "timestamp": 1761189812
    },
    {
        "content": "<p>Fwiw I think that when people get to this stage, telling them that LLM output is meaningless (while correct) is not gonna be accepted, since it's already replaced half their brain</p>",
        "id": 546583558,
        "sender_full_name": "Elisha Goldman",
        "timestamp": 1761190164
    },
    {
        "content": "<p><a href=\"https://arxiv.org/pdf/2510.19444\">https://arxiv.org/pdf/2510.19444</a></p>",
        "id": 546621262,
        "sender_full_name": "Ivan Di Liberti",
        "timestamp": 1761208804
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"971326\">Elisha Goldman</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546438780\">said</a>:</p>\n<blockquote>\n<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>\n</blockquote>\n<p>It may be good, but it's not important.   It may not even be good, since it wasted the time of several smart people here, who could have been doing something more useful.</p>",
        "id": 546696273,
        "sender_full_name": "John Baez",
        "timestamp": 1761229856
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276422\">David Michael Roberts</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546583103\">said</a>:</p>\n<blockquote>\n<p>There are very sad stories of people being completely sent off the deep end by AI-hallucinated theories of everything/conspiracies. Something like this is rather mild by comparison. I don't think the author doubts this paper at all.</p>\n</blockquote>\n<p>Maybe. I was taking from the deeply buried gaps in the allegedly formalized exposition, plus the assertion that all code was shared on a GitHub repo which remains private (if it exists), that this might be an intentional scam. It's hard for me to see how you'd believe your Lean code was really proving something true if you hadn't actually defined all the types in it.</p>",
        "id": 546762726,
        "sender_full_name": "Kevin Carlson",
        "timestamp": 1761250727
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"358258\">daniel gratzer</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546466317\">said</a>:</p>\n<blockquote>\n<p>[Back in the day, someone won a full bitcoin for proving false in Rocq. They did this by simply defining <code>False := True</code> and then proving \"false\" according to this new definition.</p>\n</blockquote>\n<p>Wow that's an easy 100k <span aria-label=\"open mouth\" class=\"emoji emoji-1f62e\" role=\"img\" title=\"open mouth\">:open_mouth:</span></p>",
        "id": 547042102,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1761383178
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"277318\">Ivan Di Liberti</span> <a href=\"#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546621262\">said</a>:</p>\n<blockquote>\n<p><a href=\"https://arxiv.org/pdf/2510.19444\">https://arxiv.org/pdf/2510.19444</a></p>\n</blockquote>\n<p>What's wrong about this one?</p>",
        "id": 547042620,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1761383617
    }
]