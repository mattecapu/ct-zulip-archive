[
    {
        "content": "<p>Somebody posted a correspondence table between Logic, Type Theory, and Machine Learning. It seems wrong to me. What would the correct table look like?</p>\n<p>Here's the post which looks incorrect to me, but I don't recommend using it as a starting point, I would prefer to focus on the core concepts: Proposition, Proof, Conjunction, Disjunction, and Implication.</p>",
        "id": 417445402,
        "sender_full_name": "Samuel Gélineau",
        "timestamp": 1706016492
    },
    {
        "content": "<p><a href=\"https://twitter.com/attunewise/status/1736072333780201526?s=20\">https://twitter.com/attunewise/status/1736072333780201526?s=20</a></p>",
        "id": 417445430,
        "sender_full_name": "Samuel Gélineau",
        "timestamp": 1706016502
    },
    {
        "content": "<p>Why would there be a correct table? The curry Howard correspondence is actual math, not just vague analogies, and I don’t see why you’d expect any more than the latter with ML.</p>",
        "id": 417488591,
        "sender_full_name": "Kevin Arlin",
        "timestamp": 1706029031
    },
    {
        "content": "<p>The ML column in that table involves a lot of linear algebra.  There <em>is</em> a pretty solid relationship between logic and programming (at least insofar as they are about cartesian closed categories) and linear algebra (at least insofar as it's about a symmetric monoidal closed category, namely Vect).    I wrote about it in my \"<a href=\"https://arxiv.org/abs/0903.0340\">Rosetta Stone</a>\" paper with <span class=\"user-mention\" data-user-id=\"276048\">@Mike Stay</span>.  But I'm not claiming this sheds any light on machine learning.</p>",
        "id": 417496528,
        "sender_full_name": "John Baez",
        "timestamp": 1706031530
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"609515\">Kevin Arlin</span> <a href=\"#narrow/stream/229952-theory.3A-type-theory/topic/Curry-Howard.20and.20Machine.20Learning/near/417488591\">said</a>:</p>\n<blockquote>\n<p>Why would there be a correct table? The curry Howard correspondence is actual math, not just vague analogies, and I don’t see why you’d expect any more than the latter with ML.</p>\n</blockquote>\n<p>I have joined this Zulip via a lecture series called \"Categories for AI\", and I am only now realizing that this Zulip is about category theory in general, not about the link between category theory and machine learning.</p>\n<p>I did not understand everything in that series, but the impression I got was that the essence of neural networks was now understood in the language of category theory. Since I know (from the Rosetta Stone paper quoted above, even!) that the Curry-Howard correspondence extends to category theory, I expected the correspondence to extend to neural networks, via that category theory formalization of neural networks.</p>\n<p>I am guessing that in the same way that the Curry-Howard correspondence maps logic to type systems like STLC's, not Java's, I expect the extended correspondence to map logic and type systems to constructs in some essence of machine learning, not to some constructs in ChatGPT.</p>",
        "id": 417563880,
        "sender_full_name": "Samuel Gélineau",
        "timestamp": 1706062932
    },
    {
        "content": "<p>The ML column of that chart looks very ad-hoc to me, more like symbolism than functoriality.  What you ought to see in that column is the <a href=\"https://en.wikipedia.org/wiki/Linear_logic\">linear logic</a> version of the first column (specifically, multiplicative intuitionistic linear logic where tensor and par are identified).  All the linear algebra used in machine learning is done in a finite-dimensional Hilbert space, and the \"Physics\" column of our table is really about Hilbert spaces:</p>\n<p><a href=\"/user_uploads/21317/QawmrAvR5_nWrNVoBsQ0EgaD/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/21317/QawmrAvR5_nWrNVoBsQ0EgaD/image.png\" title=\"image.png\"><img src=\"/user_uploads/21317/QawmrAvR5_nWrNVoBsQ0EgaD/image.png\"></a></div><p>Since all morphisms in FDHilb have a conjugate transpose, contraction is fundamentally the same thing as cloning quantum states, which is forbidden.  So there isn't a linear algebra notion corresponding to contraction.  If you add an exponential comonad, you can include linear functions into all functions and then contract, but that's not what he put in the ML column, either.</p>\n<p>On the other hand, machine learning isn't entirely linear algebra: there are plenty of nonlinear activation functions involved.</p>\n<p><a href=\"/user_uploads/21317/Oyz2mV3DZwJbMm_nwBCw_vHv/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/21317/Oyz2mV3DZwJbMm_nwBCw_vHv/image.png\" title=\"image.png\"><img src=\"/user_uploads/21317/Oyz2mV3DZwJbMm_nwBCw_vHv/image.png\"></a></div><p>What's the class of functions you get when you include these?  There are theorems like the <a href=\"https://en.wikipedia.org/wiki/Universal_approximation_theorem\">Universal approximation theorem</a> that give some conditions under which a neural network can learn a function.  But I haven't seen anyone do the work to define the actual set of functions learnable in a particular framework and build a symmetric monoidal closed category out of it.</p>",
        "id": 417684323,
        "sender_full_name": "Mike Stay",
        "timestamp": 1706114197
    },
    {
        "content": "<p>And of course, none of this is to say that people haven't done good work modeling machine learning using category-theoretic concepts, like using lenses &amp; optics to model back propagation.</p>",
        "id": 417686715,
        "sender_full_name": "Mike Stay",
        "timestamp": 1706114923
    },
    {
        "content": "<p>Relevant paper: <a href=\"https://discovery.ucl.ac.uk/id/eprint/10068974/\">https://discovery.ucl.ac.uk/id/eprint/10068974/</a></p>",
        "id": 418018002,
        "sender_full_name": "Chris Barrett",
        "timestamp": 1706165311
    },
    {
        "content": "<p>You can look at cartesian reverse derivative categories introduced in this paper: <a href=\"https://arxiv.org/abs/1910.07065\">reverse derivative categories</a> (Cockett, Cruttwell, Gallagher, Pacaud Lemay, MacAdam, Plotkin, Pronk). It's a categorical axiomatization of the reverse derivative used in machine learning.</p>",
        "id": 418145735,
        "sender_full_name": "Jean-Baptiste Vienney",
        "timestamp": 1706209333
    },
    {
        "content": "<p>There is much more work than this on category theory and machine learning. I've heard that <a href=\"https://ncatlab.org/nlab/show/optic+%28in+computer+science%29\">optics</a> are usefull. This paper seems to be a good overview (it mentions in particular reverse derivative categories and optics): <a href=\"https://arxiv.org/abs/2106.07032\">Category Theory in Machine Learning</a>(Shiebler,  Gavranović, Wilson).</p>",
        "id": 418146271,
        "sender_full_name": "Jean-Baptiste Vienney",
        "timestamp": 1706209545
    },
    {
        "content": "<p>By the way, I don't completely agree with the second message posted here. Curry-Howard is a real correspondence, but it is rarely stated as \"actual math\" with a precise isomorphism. The most important is to understand the intuition I think.</p>",
        "id": 418146832,
        "sender_full_name": "Jean-Baptiste Vienney",
        "timestamp": 1706209687
    },
    {
        "content": "<p>It is often the case in computer science, that things can be encoded in different equivalent ways, like the notion of computable function, which can be defined using Turing machines, recursive functions... Compared to mathematics, we have a bit less of a canonical language.</p>",
        "id": 418148020,
        "sender_full_name": "Jean-Baptiste Vienney",
        "timestamp": 1706210146
    },
    {
        "content": "<p>One thing that's tangential, but which people in this thread might be interested in: You can embed terms from the lambda calculus (and thus turing machines) into linear logic, which you can then interpret in the category of vector spaces. This gives a way of doing \"gradient descent\" directly on turing machines! </p>\n<p>See the talk here: <a href=\"https://www.youtube.com/watch?v=IW4LjjAWrO4\">https://www.youtube.com/watch?v=IW4LjjAWrO4</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"IW4LjjAWrO4\" href=\"https://www.youtube.com/watch?v=IW4LjjAWrO4\"><img src=\"https://uploads.zulipusercontent.net/be6c3a5379af5386a29218eae246ebdefbc5ccb8/68747470733a2f2f692e7974696d672e636f6d2f76692f4957344c6a6a4157724f342f64656661756c742e6a7067\"></a></div>",
        "id": 418157253,
        "sender_full_name": "Chris Grossack (they/them)",
        "timestamp": 1706213612
    },
    {
        "content": "<p>:O wow this is such a cool idea!</p>",
        "id": 418316351,
        "sender_full_name": "Matteo Capucci (he/him)",
        "timestamp": 1706288775
    }
]