[
    {
        "content": "<p>Today at 17:00 UTC, as per usual <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span> <img alt=\":topos-institute:\" class=\"emoji\" src=\"https://zulip-avatars.s3.amazonaws.com/21317/emoji/images/22209.png\" title=\"topos-institute\"> </p>\n<p><em>Abstract:</em><br>\nA fundamental component of many machine learning algorithms is differentiation. Thus, if one wishes to abstract and generalize aspects of machine learning, it is useful to have an abstract perspective on differentiation. There has been much work on categorical differential structures in the past few years with the advent of differential categories, Cartesian differential categories, and tangent categories. In this talk I'll focus on Cartesian reverse differential categories, a recent variant of Cartesian differential categories, and touch on how they can be used in abstract machine learning.</p>\n<p>YouTube: <a href=\"https://www.youtube.com/watch?v=ljE9CWEUzJM\">https://www.youtube.com/watch?v=ljE9CWEUzJM</a><br>\nZoom: <a href=\"https://topos-institute.zoom.us/j/5344862882\">https://topos-institute.zoom.us/j/5344862882</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"ljE9CWEUzJM\" href=\"https://www.youtube.com/watch?v=ljE9CWEUzJM\"><img src=\"https://uploads.zulipusercontent.net/f6e097a5c6b7fbbc09dfe76632a15f97703b2b18/68747470733a2f2f692e7974696d672e636f6d2f76692f6c6a4539435745557a4a4d2f64656661756c742e6a7067\"></a></div>",
        "id": 245296029,
        "sender_full_name": "Tim Hosgood",
        "timestamp": 1625740699
    },
    {
        "content": "<p>This was a shockingly clear and enjoyable talk!</p>",
        "id": 245386502,
        "sender_full_name": "John Baez",
        "timestamp": 1625788880
    },
    {
        "content": "<p>I am reminded of this work by Matthijs Vákár: <a href=\"https://arxiv.org/abs/2103.15776\">https://arxiv.org/abs/2103.15776</a>, where forward- and reverse-mode differentiation are similarly linked by taking the fiberwise opposite of a fibration (Section 6). He talks about assumptions on the fibration that guarantee that the total category is cartesian closed, and the point of all this is to extend automatic differentiation to higher-order functions in a principled way. Cool stuff!</p>",
        "id": 245535754,
        "sender_full_name": "Mitchell Riley",
        "timestamp": 1625902484
    }
]