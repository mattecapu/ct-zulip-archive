[
    {
        "content": "<p><time datetime=\"2022-05-26T17:00:00Z\">2022-05-26T17:00:00+00:00</time></p>\n<p><strong>Tai-Danae Bradley</strong>: <em>Entropy as an Operad Derivation</em></p>\n<p>This talk features a small connection between information theory, algebra, and topology‚Äînamely, a correspondence between Shannon entropy and derivations of the operad of topological simplices. We will begin with a brief review of operads and their representations with topological simplices and the real line as the main example. We then give a general definition for a derivation of an operad in any category with values in an abelian bimodule over the operad. The main result is that Shannon entropy defines a derivation of the operad of topological simplices, and that for every derivation of this operad there exists a point at which it is given by a constant multiple of Shannon entropy. We show this is compatible with, and relies heavily on, a well-known characterization of entropy given by Faddeev in 1956 and a recent variation given by Leinster.</p>\n<p>Zoom: <a href=\"https://topos-institute.zoom.us/j/84392523736?pwd=bjdVS09wZXVscjQ0QUhTdGhvZ3pUdz09\">https://topos-institute.zoom.us/j/84392523736?pwd=bjdVS09wZXVscjQ0QUhTdGhvZ3pUdz09</a><br>\nYouTube: <a href=\"https://youtu.be/_cAEfQQcELA\">https://youtu.be/_cAEfQQcELA</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"_cAEfQQcELA\" href=\"https://youtu.be/_cAEfQQcELA\"><img src=\"https://uploads.zulipusercontent.net/c605c8950dfac2eb7a828b70930465db65ed47fd/68747470733a2f2f692e7974696d672e636f6d2f76692f5f63414566515163454c412f64656661756c742e6a7067\"></a></div>",
        "id": 283277058,
        "sender_full_name": "Tim Hosgood",
        "timestamp": 1653288063
    },
    {
        "content": "<p>I have found a bit of time off from work to try to follow a little the many interesting courses on entropy and CT that have appeared recently.  <br>\n<a href=\"https://twitter.com/bblfish/status/1528969976950005760\">https://twitter.com/bblfish/status/1528969976950005760</a></p>\n<div class=\"inline-preview-twitter\"><div class=\"twitter-tweet\"><a href=\"https://twitter.com/bblfish/status/1528969976950005760\"><img class=\"twitter-avatar\" src=\"https://uploads.zulipusercontent.net/77b199c7e82b7b9c52e9d03926bbf1970d09fe5d/68747470733a2f2f7062732e7477696d672e636f6d2f70726f66696c655f696d616765732f313030343332363736313630383431373238312f486a786a533942585f6e6f726d616c2e6a7067\"></a><p>\"Tutorial on Categorical Semantics of Entropy\" by <a href=\"https://twitter.com/johncarlosbaez\">@johncarlosbaez</a> and <a href=\"https://twitter.com/math3ma\">@math3ma</a>, starting with Shannon entropy. \nThis looks like it could be the course that was missing from Stiegler's philosophical work on negentropy \n<a href=\"https://twitter.com/ArsIndustrialis\">@ArsIndustrialis</a>\n<a href=\"https://t.co/DVjxPEDerg\">https://www.youtube.com/watch?v=5phJVSWdWg4</a></p><span>- The‚Äâüêü‚Äç‚Äç‚ÄâBabelFish (@bblfish)</span></div></div>",
        "id": 284100316,
        "sender_full_name": "Henry Story",
        "timestamp": 1653636833
    },
    {
        "content": "<p>My recent interest in Entropy came from listening to Bernard Stiegler a well known French philosopher (with quite an amazing history: he spend 5 years in prison after robbing a few banks in the 70s, sudied philosophy there, came out and did his Phd under Derrida on technics and time). He liked the long term views of humanity starting 2 million years ago with ability of pre-humans to control fire, which then changed them genetically over the ages leading to our ancestors. That story is captured by the greek myth of Prometheus stealing fire from the gods and giving it to humans, for which he was condemned to be chained to a rock and have an eagle eat his liver out every day which would then grow back (the liver in French is foi, which is homophonic in French to the word that we may translate as faith).<br>\n<a href=\"https://eduscol.education.fr/odysseum/le-mythe-de-promethee\">https://eduscol.education.fr/odysseum/le-mythe-de-promethee</a></p>",
        "id": 284100480,
        "sender_full_name": "Henry Story",
        "timestamp": 1653636964
    },
    {
        "content": "<p>Bernard Stiegler in the last 5 years or so developed the theme of entropy and emphasised the work of Shannon's negentropy as well as the relation of that to questions on what life is. Physical entropy has as endpoint the heat death of the universe and Life seems to be a local negentropic bubble that goes in the opposite direction to the physical system, creating more and more order where physical entropy is more about the creation of disorder (and hence why we have to keep organising ourselves, cleaning up, etc...). </p>\n<p>Is this notion of order vs disorder captured by the definitions of entropy that are being put forwards recently?</p>",
        "id": 284101555,
        "sender_full_name": "Henry Story",
        "timestamp": 1653637861
    },
    {
        "content": "<p>My guess is that <span class=\"user-mention\" data-user-id=\"275920\">@John Baez</span>'s definition on \"compositional thermostatics\" is taking this into account via the notion of Symmetric Monoidal Categories.</p>\n<blockquote>\n<p>Symmetric monoidal structures on a categories are another formalism that allows<br>\none to discuss morphisms with multiple inputs, and to permute these inputs [12], and in fact there is in fact a strong relationship between operads and symmetric monoidal categories. Every symmetric monoidal category has an underlying operad.</p>\n</blockquote>\n<p><a href=\"https://arxiv.org/pdf/2111.10315.pdf\">https://arxiv.org/pdf/2111.10315.pdf</a></p>\n<p><span class=\"user-mention\" data-user-id=\"276053\">@Brendan Fong</span> clearly relates Symmetric Monoidal Categories to biology in his thesis and to open and closed systems<br>\n<a href=\"https://twitter.com/bblfish/status/1398230682237911041\">https://twitter.com/bblfish/status/1398230682237911041</a></p>\n<div class=\"inline-preview-twitter\"><div class=\"twitter-tweet\"><a href=\"https://twitter.com/bblfish/status/1398230682237911041\"><img class=\"twitter-avatar\" src=\"https://uploads.zulipusercontent.net/77b199c7e82b7b9c52e9d03926bbf1970d09fe5d/68747470733a2f2f7062732e7477696d672e636f6d2f70726f66696c655f696d616765732f313030343332363736313630383431373238312f486a786a533942585f6e6f726d616c2e6a7067\"></a><p><a href=\"https://twitter.com/johncarlosbaez\">@johncarlosbaez</a> <a href=\"https://twitter.com/ejpatters\">@ejpatters</a> Ah yes, the preface to Brendan Fong's thesis \"The Algebra of Open and Interconnected Systems\" <a href=\"https://t.co/bz9wmfPWvI\">https://arxiv.org/abs/1609.05382</a> should be of great interest to Philosophers as well as #linkeddata folks interested in what is behind the Open World Assumption. cc <a href=\"https://twitter.com/DJRoss70\">@DJRoss70</a> <a href=\"https://t.co/OiBEhy2Ddy\">https://twitter.com/bblfish/status/1398230682237911041/photo/1</a></p><span>- The‚Äâüêü‚Äç‚Äç‚ÄâBabelFish (@bblfish)</span><div class=\"twitter-image\"><a href=\"https://t.co/OiBEhy2Ddy\"><img src=\"https://uploads.zulipusercontent.net/bbb72c9e0cfed4b9b3f66b54bbac2457684d1846/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f453265444d43415830414532566c4c2e706e673a7468756d62\"></a></div></div></div>",
        "id": 284102510,
        "sender_full_name": "Henry Story",
        "timestamp": 1653638519
    },
    {
        "content": "<p>Stiegler's belief is these negentropic discoveries required rethinking all of philosophy and many other sciences as well, such as Economics. A key thinker in Economics he mentioned a lot was Georgescu Roegen, who studied under the famous \"Creative Destruction\" Schumpeter and who then wrote \"The entropy Law and the Economic Process\".   Stiegler thought that modern economists or poltics had not yet correctly integrated this thinking. Perhaps because the definitions of entropy were so muddled in the 1980s. So Perhaps these new analysis on entropy being put forward, linked to economics explained as Game theory (<span class=\"user-mention\" data-user-id=\"275901\">@Jules Hedges</span> ) can fix the misunderstandings  that stalled thinking on the subect? </p>\n<p><a href=\"https://en.wikipedia.org/wiki/Nicholas_Georgescu-Roegen\">https://en.wikipedia.org/wiki/Nicholas_Georgescu-Roegen</a></p>",
        "id": 284103566,
        "sender_full_name": "Henry Story",
        "timestamp": 1653639288
    },
    {
        "content": "<p>I get the feeling that the book <a href=\"https://arxiv.org/pdf/2012.02113.pdf\">Entropy and Diversity: the Axiomatic Approach</a> on the axiv may be the best starting point.</p>",
        "id": 284162135,
        "sender_full_name": "Henry Story",
        "timestamp": 1653672770
    },
    {
        "content": "<blockquote>\n<p>Is this notion of order vs disorder captured by the definitions of entropy that are being put forwards recently?</p>\n</blockquote>\n<p>I don't know any new definition of entropy being put forward recently, just new ways of understanding the usual ones.</p>",
        "id": 284196008,
        "sender_full_name": "John Baez",
        "timestamp": 1653694506
    },
    {
        "content": "<p>I don't think \"order vs disorder\" is a very helpful way to understand entropy.     At the very least you have to be extremely careful.  For example, the entropy of a frozen dead cat is much, <em>much</em> less than that of a warm live cat, and the latter is just a <em>very tiny</em> bit less than that of a warm dead cat.</p>",
        "id": 284196182,
        "sender_full_name": "John Baez",
        "timestamp": 1653694690
    },
    {
        "content": "<p>For my thoughts on entropy, <a href=\"https://math.ucr.edu/home/baez/entropy/\">check out my talk video and slides</a>.   Tom Leinster's book is really good, but of course much longer.   He's not a physicist, so for a physics perspective you should go somewhere else (and not my talk, I didn't have time for any physics).</p>",
        "id": 284196428,
        "sender_full_name": "John Baez",
        "timestamp": 1653694931
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/stream/269484-seminar.3A-Topos-Colloquium/topic/Tai-Danae.20Bradley.3A.20Entropy.20as.20an.20Operad.20Derivation/near/284196182\">said</a>:</p>\n<blockquote>\n<p>I don't think \"order vs disorder\" is a very helpful way to understand entropy.     At the very least you have to be extremely careful.  For example, the entropy of a frozen dead cat is much, <em>much</em> less than that of a warm live cat, and the latter is just a <em>very tiny</em> bit less than that of a warm dead cat.</p>\n</blockquote>\n<p>Or take some oil and water and shake it up. It separates again over time because in the separated state it has a higher entropy than the mixed state (due to being very slightly warmer), even though the mixed state is intuitively more disordered.</p>",
        "id": 284202677,
        "sender_full_name": "Nathaniel Virgo",
        "timestamp": 1653703027
    },
    {
        "content": "<p>Where do you find  out what the entropy of a dead cat, a frozen one, mixed oil and vinegar, etc are?</p>",
        "id": 284209056,
        "sender_full_name": "Henry Story",
        "timestamp": 1653711515
    },
    {
        "content": "<p>I was wondering: what is the entropy of the web  with and without the index? (does that make a difference?) Say, before AltaVista crawled the web and released its index in 1994. I guess the operadic tools could should be useful here as we are combining entropies.</p>",
        "id": 284209088,
        "sender_full_name": "Henry Story",
        "timestamp": 1653711588
    },
    {
        "content": "<p>I guess, to answer my question, there are research departments and magazines on entropy where one goes to ask these questions. :-)  I just found this <a href=\"https://www.mdpi.com/1099-4300/23/2/222\">The Entropy Universe</a> which gives a whole history of entropy formulations (but only for time series data?)</p>",
        "id": 284210031,
        "sender_full_name": "Henry Story",
        "timestamp": 1653712978
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"281126\">Henry Story</span> <a href=\"#narrow/stream/269484-seminar.3A-Topos-Colloquium/topic/Tai-Danae.20Bradley.3A.20Entropy.20as.20an.20Operad.20Derivation/near/284209056\">said</a>:</p>\n<blockquote>\n<p>Where do you find  out what the entropy of a dead cat, a frozen one, mixed oil and vinegar, etc are?</p>\n</blockquote>\n<p>As for dead, frozen etc. cats you don't need to think about cats <em>per se</em> to understand which one has more entropy - the entropy of any sort of meat is about the same as a function of temperature; the difference in entropy due to being alive or dead is tiny.</p>",
        "id": 284210092,
        "sender_full_name": "John Baez",
        "timestamp": 1653713056
    },
    {
        "content": "<p>Just wondering. Have the presentations of the recent 2 day <a href=\"https://itsatcuny.org/calendar/220511/tutorial-scse\">Seminar on the Categorical Semantics of Entropy</a> (also <a href=\"https://itsatcuny.org/calendar/220513/scse\">scse day 2</a>) succeeded in unifying the concepts of entropy that evolved over 160 years as described in the 2021 article <a href=\"https://www.mdpi.com/1099-4300/23/2/222\">The Entropy Universe</a>?   <br>\n<a href=\"/user_uploads/21317/W7EqovvE5NRw95FQcgOVA2qr/Screen-Shot-2022-05-28-at-07.14.26.png\">Diagram of the history of entropy</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/21317/W7EqovvE5NRw95FQcgOVA2qr/Screen-Shot-2022-05-28-at-07.14.26.png\" title=\"Diagram of the history of entropy\"><img src=\"/user_uploads/21317/W7EqovvE5NRw95FQcgOVA2qr/Screen-Shot-2022-05-28-at-07.14.26.png\"></a></div>",
        "id": 284211512,
        "sender_full_name": "Henry Story",
        "timestamp": 1653715475
    },
    {
        "content": "<p>About oil and water, I was cheating a bit: we know the end result must have a higher entropy since otherwise the separation of oil and water wouldn't be a spontaneous process. (I'm assuming it would indeed be spontaneous if the experiment were performed in an adiabatic container - I think probably if you do it in the kitchen it's closer to adiabatic than isothermal.) It probably is the sort of thing one could look up though, if one knew the right terms to search for.</p>",
        "id": 284212838,
        "sender_full_name": "Nathaniel Virgo",
        "timestamp": 1653717746
    },
    {
        "content": "<p>When oil and water separate they get slightly warmer since the gravitational potential energy of the water at the top of the jar gets converted to kinetic energy as the water falls, and then by friction to thermal energy when the water settles at the bottom.</p>",
        "id": 284213814,
        "sender_full_name": "Oscar Cunningham",
        "timestamp": 1653719486
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"275920\">@John Baez</span> said:</p>\n<blockquote>\n<p>I don't know any new definition of entropy being put forward recently, just new ways of understanding the usual ones.</p>\n</blockquote>\n<p>See <a href=\"https://www.quantamagazine.org/physicists-trace-the-rise-in-entropy-to-quantum-information-20220526/\">Physicists Rewrite the Fundamental Law That Leads to Disorder</a> for research basing entropy on the flow of quantum information.</p>",
        "id": 284219749,
        "sender_full_name": "Daniel Geisler",
        "timestamp": 1653728946
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276867\">Daniel Geisler</span> <a href=\"#narrow/stream/269484-seminar.3A-Topos-Colloquium/topic/Tai-Danae.20Bradley.3A.20Entropy.20as.20an.20Operad.20Derivation/near/284219749\">said</a>:</p>\n<blockquote>\n<p>See <a href=\"https://www.quantamagazine.org/physicists-trace-the-rise-in-entropy-to-quantum-information-20220526/\">Physicists Rewrite the Fundamental Law That Leads to Disorder</a> for research basing entropy on the flow of quantum information.</p>\n</blockquote>\n<p>Now that is really hot off the press: Just two days old!  :-)<br>\nOn the <a href=\"https://itsatcuny.org/calendar/220513/scse\">second day of scse</a> <span class=\"user-mention\" data-user-id=\"500006\">@Tom Mainiero</span>  and <span class=\"user-mention\" data-user-id=\"296639\">@Arthur Parzygnat</span> spoke about quantum entropy. They may have some interesting views on that research and article. Perhaps even <span class=\"user-mention\" data-user-id=\"276049\">@David Spivak</span> will be interested as he had the feeling that his Poly take on entropy was loosing something and did not quite tie in with dynamical systems.</p>",
        "id": 284222713,
        "sender_full_name": "Henry Story",
        "timestamp": 1653733902
    }
]