[
    {
        "content": "<p>Our paper <a href=\"https://arxiv.org/pdf/2506.08616\">Generalizing while preserving monotonicity</a> has been accepted at NeurIPS 2025. It's been a long time since I last published a paper, so I'm quite happy. And this zulip is partly responsible for getting me back in the mood of doing research again.</p>\n<p>In short, we study models (variants of <a href=\"https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model\">Bradley-Terry models</a>) that learn preferences from comparisons between alternatives. We want these models to be <em>monotone</em>: if you say you prefer <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">a</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">a</span></span></span></span> over <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span>, the score of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">a</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">a</span></span></span></span> should increase and the score of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span> should decrease. Surprisingly, this property is not always satisfied. In our work, we assume that the alternatives are represented by vector embeddings, yielding an embedding matrix <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span>, and that the score is a linear function of embeddings. We found a sufficient condition (diffusion embedding) on <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span> for monotonicity to hold, namely that the inverse of the Gram matrix <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x^Tx</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathnormal\">x</span></span></span></span> is <a href=\"#narrow/channel/229156-theory.3A-applied-category-theory/topic/Laplacian.20of.20an.20open.20graph/near/555038932\">super-laplacian</a>. Intuitively, with a diffusion embedding, score propagates like heat: stating that you prefer <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">a</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">a</span></span></span></span> over <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span> is like installing a heat pump that heats <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">a</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">a</span></span></span></span> at the expense of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span>.</p>\n<p>So it is not directly related to CT. But in future work, I would like to understand a bit more these diffusion embeddings, probably using ideas from open graphs.</p>",
        "id": 555298135,
        "sender_full_name": "Peva Blanchard",
        "timestamp": 1763024281
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"619003\">Peva Blanchard</span> <a href=\"#narrow/channel/274877-community.3A-our-work/topic/Peva.20Blanchard/near/555298135\">said</a>:</p>\n<blockquote>\n<p>Our paper <a href=\"https://arxiv.org/pdf/2506.08616\">Generalizing while preserving monotonicity</a> has been accepted at NeurIPS 2025. It's been a long time since I last published a paper, so I'm quite happy. And this zulip is partly responsible for getting me back in the mood of doing research again.</p>\n</blockquote>\n<p>I haven't had the time to fully read the paper yet, but this looks very interesting. We wrote a paper on <a href=\"https://arxiv.org/abs/2310.00179\">preference dynamics</a> of networked agents. I interact more with the robotics and control community, but I think the framework we established could be useful for RLHF for networks of LLM agents.</p>",
        "id": 558630206,
        "sender_full_name": "Hans Riess",
        "timestamp": 1763722184
    },
    {
        "content": "<p>Interesting I'll have a look at your paper.</p>",
        "id": 560660260,
        "sender_full_name": "Peva Blanchard",
        "timestamp": 1764271128
    }
]