[
    {
        "content": "<p>Hello all! This is the official channel for discussion about Bob's talk.<br>\nTitle: Quantum Natural Language Processing</p>\n<p>Zoom Meeting:<br>\n<a href=\"https://mit.zoom.us/j/280120646\" title=\"https://mit.zoom.us/j/280120646\">https://mit.zoom.us/j/280120646</a><br>\nMeeting ID: 280 120 646</p>\n<p>Youtube live stream:<br>\n<a href=\"https://youtu.be/YV6zcQCiRjo\" title=\"https://youtu.be/YV6zcQCiRjo\">https://youtu.be/YV6zcQCiRjo</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"YV6zcQCiRjo\" href=\"https://youtu.be/YV6zcQCiRjo\" title=\"https://youtu.be/YV6zcQCiRjo\"><img src=\"https://i.ytimg.com/vi/YV6zcQCiRjo/default.jpg\"></a></div>",
        "id": 196337009,
        "sender_full_name": "Paolo Perrone",
        "timestamp": 1588698596
    },
    {
        "content": "<p>Hello. In 5 minutes we start!</p>",
        "id": 196791610,
        "sender_full_name": "Paolo Perrone",
        "timestamp": 1588866968
    },
    {
        "content": "<p>starting now!</p>",
        "id": 196792246,
        "sender_full_name": "Paolo Perrone",
        "timestamp": 1588867208
    },
    {
        "content": "<p>I feel like \"DisCo(Cat)\" arose from someone wanting \"disco cat\" to be a formal math term.</p>",
        "id": 196795528,
        "sender_full_name": "Brian Pinsky",
        "timestamp": 1588868607
    },
    {
        "content": "<p>I'm kind of curious about things like movement in a grammar like this</p>",
        "id": 196796531,
        "sender_full_name": "Brian Pinsky",
        "timestamp": 1588869124
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276424\">Rongmin Lu</span> so, basically just the 2 dimensional version of \"disco ball\"?</p>",
        "id": 196797218,
        "sender_full_name": "Brian Pinsky",
        "timestamp": 1588869496
    },
    {
        "content": "<p>Here is the repo, as shared by Alexis: <a href=\"https://github.com/oxford-quantum-group/discopy\" title=\"https://github.com/oxford-quantum-group/discopy\">https://github.com/oxford-quantum-group/discopy</a> <br>\nYou can play with it if you want!</p>",
        "id": 196800322,
        "sender_full_name": "Paolo Perrone",
        "timestamp": 1588870916
    },
    {
        "content": "<p>I'd like a nice machinery for translating traditional semantic denotations of words into this framework.  I can see it for easy functions like transitive verbs (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>e</mi><mo>→</mo><mo stretchy=\"false\">(</mo><mi>e</mi><mo>→</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">e\\to (e\\to t)</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">e</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">e</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mclose\">)</span></span></span></span>, but I don't see how you'd process something like \"the\" in this framework.</p>",
        "id": 196800403,
        "sender_full_name": "Brian Pinsky",
        "timestamp": 1588870935
    },
    {
        "content": "<p>Logically speaking pregroup grammars (that's what these things are called) have the same \"expressive power\" as context free grammars, there are translations back and forth. Lambek and Anne Preller and probably some other people I don't remember wrote some papers on the grammar of different real-world languages using pregroups</p>",
        "id": 196800782,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1588871096
    },
    {
        "content": "<p>Some of the grammatical types can get pretty complicated, but like often with formal grammar it gets easier with practice</p>",
        "id": 196801019,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1588871222
    },
    {
        "content": "<p>Doing it for \"fiddly\" words like <em>the</em> will depend a lot on the grammatical specifics of the language, so English in this case. My guess (and it's just a guess, maybe Lambek did something totally different) is that you'd have a primitive type for \"undetermined noun phrase\" and another for \"determined noun phrase\", and <em>the</em> takes the former on the right and outputs the latter</p>",
        "id": 196801393,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1588871391
    },
    {
        "content": "<p>So , if I give you a non-intersective adjective like \"favorite\", would that just be a state with 2 lines coming in, and the agent gets hooked up to the person it needs to be hooked up to somehow?</p>",
        "id": 196802292,
        "sender_full_name": "Brian Pinsky",
        "timestamp": 1588871756
    },
    {
        "content": "<p>Explaining neural nets is a big problem in theory and in applications. If NNs can be turned into a lower level approximation below the quantum circuits, it could be quite helpful in settings where it's important for humans to understand/interpret the model.</p>",
        "id": 196802686,
        "sender_full_name": "Tim Sears",
        "timestamp": 1588871942
    },
    {
        "content": "<p>The ultimate NLP problem is: \"one is a baby receiving example utterances paired with vision, and one wants to infer syntax and semantics\" --- which parts of this does QNLP help with, and which are left for future work?</p>",
        "id": 196803087,
        "sender_full_name": "Sam Tenka",
        "timestamp": 1588872137
    },
    {
        "content": "<p>Repeating my question from the other chat. Is there a way to translate these circuits to classical neural nets?<br>\n The language seems more expressive and coherent. It would be a nice \"higher level language\" if it sat atop NNs.</p>",
        "id": 196803331,
        "sender_full_name": "Tim Sears",
        "timestamp": 1588872249
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"282815\">@Tim Sears</span> I think the converse question is interesting, too: can we interpret NN's as approximating a lower level QNLP substrate?</p>",
        "id": 196803420,
        "sender_full_name": "Sam Tenka",
        "timestamp": 1588872308
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"282815\">@Tim Sears</span> via the formalism of \"Tensor Networks\" (just penrose string diagrams, where the connotation is that one thinks about them in a certain computational way), I think this can be modeled neurally in straightforward fashion <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mstyle mathcolor=\"red\"><msup><mrow></mrow><mo>⋆</mo></msup></mstyle></mrow><annotation encoding=\"application/x-tex\">{\\color{red}^\\star}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.688696em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"color:red;\"><span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\" style=\"color:red;\"><span class=\"mbin mtight\" style=\"color:red;\">⋆</span></span></span></span></span></span></span></span></span></span></span></span>.  If one is on a classical computer, though, the tensors can become unwieldy to represent.  In deep learning, one often approximates huge tensors by a low rank approximation.  Perhaps this is useful both as an optimization and also as an Occam prior on the nature of language?</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mstyle mathcolor=\"red\"><msup><mrow></mrow><mo>⋆</mo></msup></mstyle></mrow><annotation encoding=\"application/x-tex\">{\\color{red}^\\star}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.688696em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"color:red;\"><span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\" style=\"color:red;\"><span class=\"mbin mtight\" style=\"color:red;\">⋆</span></span></span></span></span></span></span></span></span></span></span></span> as long as the syntax tree of the sentence under consideration is given.  This is a limitation shared by the specific QNLP approach Bob shared, I think.  With LSTMs, though, as long as one commits to a certain dimensionality shared by all types, it is straightforward to emulate the QNLP networks as long as one's syntax trees are generated by a regular expression (so imagine CFG-style Pushdown Automata with bounded but potentially large stacks).</p>",
        "id": 196803658,
        "sender_full_name": "Sam Tenka",
        "timestamp": 1588872420
    },
    {
        "content": "<p>Question re. Meaning in Use : can you comment on meaning as dynamic significance (or influence) in some dynamic context of evolving, shared discourse + belief + action. &lt; —  was addressed nicely</p>",
        "id": 196803716,
        "sender_full_name": "Lee Mondshein",
        "timestamp": 1588872445
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"282815\">Tim Sears</span> <a href=\"#narrow/stream/229457-MIT-Categories.20Seminar/topic/May.207.20-.20Bob.20Coecke's.20talk/near/196803331\" title=\"#narrow/stream/229457-MIT-Categories.20Seminar/topic/May.207.20-.20Bob.20Coecke's.20talk/near/196803331\">said</a>:</p>\n<blockquote>\n<p>Repeating my question from the other chat. Is there a way to translate these circuits to classical neural nets?<br>\n The language seems more expressive and coherent. It would be a nice \"higher level language\" if it sat atop NNs.</p>\n</blockquote>\n<p>I think <span class=\"user-mention\" data-user-id=\"300045\">@Martha Lewis</span> worked on this</p>",
        "id": 196812626,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1588876505
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275989\">Paolo Perrone</span> <a href=\"#narrow/stream/229457-MIT-Categories.20Seminar/topic/May.207.20-.20Bob.20Coecke's.20talk/near/196800322\" title=\"#narrow/stream/229457-MIT-Categories.20Seminar/topic/May.207.20-.20Bob.20Coecke's.20talk/near/196800322\">said</a>:</p>\n<blockquote>\n<p>Here is the repo, as shared by Alexis: <a href=\"https://github.com/oxford-quantum-group/discopy\" title=\"https://github.com/oxford-quantum-group/discopy\">https://github.com/oxford-quantum-group/discopy</a> <br>\nYou can play with it if you want!</p>\n</blockquote>\n<p>Woo, I am very happy to see the @ symbol is used for tensor product. (Using it for matrix product is an abomination, imo.)</p>",
        "id": 196829463,
        "sender_full_name": "Simon Burton",
        "timestamp": 1588885247
    },
    {
        "content": "<p>I'm not so sure about the \"&gt;&gt;\" operator... as long as we never need to use \"+\" in these expressions it should be ok (&gt;&gt; has lower precedence than +).<br>\nThis is my own attempt at \"verifying\" the zig-zag equation works, <a href=\"https://github.com/punkdit/bruhat/blob/master/bruhat/vec.py\" title=\"https://github.com/punkdit/bruhat/blob/master/bruhat/vec.py\">https://github.com/punkdit/bruhat/blob/master/bruhat/vec.py</a> (see line 518.)<br>\nI just use \"*\" for vertical composition.</p>",
        "id": 196830177,
        "sender_full_name": "Simon Burton",
        "timestamp": 1588885625
    },
    {
        "content": "<p>Here's the video for all those that missed the talk!<br>\n<a href=\"https://youtu.be/mL-hWbwVphk\" title=\"https://youtu.be/mL-hWbwVphk\">https://youtu.be/mL-hWbwVphk</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"mL-hWbwVphk\" href=\"https://youtu.be/mL-hWbwVphk\" title=\"https://youtu.be/mL-hWbwVphk\"><img src=\"https://i.ytimg.com/vi/mL-hWbwVphk/default.jpg\"></a></div>",
        "id": 196831509,
        "sender_full_name": "Paolo Perrone",
        "timestamp": 1588886285
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"282815\">@Tim Sears</span> <span class=\"user-mention\" data-user-id=\"281272\">@Sam Tenka</span> FYI, there has been some work on finding a correspondence between certain NNs and tensor networks, e.g. see the work of Levine et. al. <a href=\"https://arxiv.org/pdf/1803.09780.pdf\" title=\"https://arxiv.org/pdf/1803.09780.pdf\">https://arxiv.org/pdf/1803.09780.pdf</a> and <a href=\"https://arxiv.org/pdf/1704.01552.pdf\" title=\"https://arxiv.org/pdf/1704.01552.pdf\">https://arxiv.org/pdf/1704.01552.pdf</a></p>",
        "id": 196836580,
        "sender_full_name": "Tai-Danae Bradley",
        "timestamp": 1588889300
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276424\">Rongmin Lu</span> <a href=\"#narrow/stream/229457-MIT-Categories.20Seminar/topic/May.207.20-.20Bob.20Coecke's.20talk/near/196867331\" title=\"#narrow/stream/229457-MIT-Categories.20Seminar/topic/May.207.20-.20Bob.20Coecke's.20talk/near/196867331\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"299060\">Tai-Danae Bradley</span> <a href=\"#narrow/stream/229457-MIT-Categories.20Seminar/topic/May.207.20-.20Bob.20Coecke's.20talk/near/196836580\" title=\"#narrow/stream/229457-MIT-Categories.20Seminar/topic/May.207.20-.20Bob.20Coecke's.20talk/near/196836580\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"282815\">Tim Sears</span> <span class=\"user-mention silent\" data-user-id=\"281272\">Sam Tenka</span> FYI, there has been some work on finding a correspondence between certain NNs and tensor networks, e.g. see the work of Levine et. al. <a href=\"https://arxiv.org/pdf/1803.09780.pdf\" title=\"https://arxiv.org/pdf/1803.09780.pdf\">https://arxiv.org/pdf/1803.09780.pdf</a> and <a href=\"https://arxiv.org/pdf/1704.01552.pdf\" title=\"https://arxiv.org/pdf/1704.01552.pdf\">https://arxiv.org/pdf/1704.01552.pdf</a></p>\n</blockquote>\n<p>Oh wow, thank you! These do appear to confirm the intuition that some of us had. </p>\n<p>In a way, Penrose might have been <em>morally</em> correct to say that \"consciousness is quantum\"; it seems we're now slowly figuring out the <em>technically</em> correct way to say the same thing.</p>\n<p>FWIW here are the published versions:</p>\n<ul>\n<li>1704.01552: <a href=\"https://openreview.net/forum?id=SywXXwJAb\" title=\"https://openreview.net/forum?id=SywXXwJAb\">https://openreview.net/forum?id=SywXXwJAb</a> (ICLR 2018 conference with open peer review)</li>\n<li>1803.09780: <a href=\"https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.065301\" title=\"https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.065301\">https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.065301</a></li>\n</ul>\n</blockquote>\n<p>This is interesting---I was not aware of journals that publicly display reviews like this!</p>",
        "id": 196874891,
        "sender_full_name": "Arthur Parzygnat",
        "timestamp": 1588930680
    }
]