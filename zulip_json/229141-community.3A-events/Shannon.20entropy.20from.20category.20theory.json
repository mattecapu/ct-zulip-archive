[
    {
        "content": "<p>If you're into machine learning there are lots of talks here:</p>\n<ul>\n<li><a href=\"https://mpml.tecnico.ulisboa.pt/\">Lisbon Webinar on Mathematics, Physics, and Machine Learning</a>.</li>\n</ul>",
        "id": 286163107,
        "sender_full_name": "John Baez",
        "timestamp": 1655267306
    },
    {
        "content": "<p>I'm giving a talk there this Thursday, June 16th, at 18:00 Lisbon time, which is 10:00 in California:</p>\n<ul>\n<li><a href=\"https://videoconf-colibri.zoom.us/j/91599759679#success\">Shannon entropy from category theory</a></li>\n</ul>\n<blockquote>\n<p>Shannon entropy is a powerful concept. But what properties single out Shannon entropy as special? Instead of focusing on the entropy of a probability measure on a finite set, it can help to focus on the \"information loss\", or change in entropy, associated with a measure-preserving function. Shannon entropy then gives the only concept of information loss that is functorial, convex-linear and continuous. This is joint work with Tom Leinster and Tobias Fritz.</p>\n</blockquote>",
        "id": 286163288,
        "sender_full_name": "John Baez",
        "timestamp": 1655267567
    },
    {
        "content": "<p>You can watch the talk on Zoom by clicking on the talk title.</p>",
        "id": 286163303,
        "sender_full_name": "John Baez",
        "timestamp": 1655267612
    },
    {
        "content": "<p>This will be a lot like the talk I gave at the tutorial at CUNY with <span class=\"user-mention\" data-user-id=\"299060\">@Tai-Danae Bradley</span> in May, and you can watch that one <a href=\"https://math.ucr.edu/home/baez/entropy/\">here</a>.</p>",
        "id": 286163391,
        "sender_full_name": "John Baez",
        "timestamp": 1655267752
    }
]