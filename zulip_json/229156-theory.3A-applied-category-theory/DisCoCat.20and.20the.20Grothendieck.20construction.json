[
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276438\">Fabrizio Genovese</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat/near/191878789\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276692\">Bob Coecke</span> claims to have solved this problem in the last paper. The idea of describing verbs as \"applying things to nouns\" is nice because gets rid of this sentence space altogether and allows you to live only in the spaces you already have but I don't know if it works for any verb. About the functor, the first DisCoCat paper was great because there wasn't any functor, and relationships between grammar/semantics were done using products. So yes, it was a unique category but things were still neatly separated, somehow. I still think that a functor Semantics -&gt; Grammar is useful, not from the NLP point of view, but from a linguistic perspective. It is also a nice point to tackle a lot of problems in Applied Category Theory that pop up pretty much everywhere, but are more pronounced for language, such as the fact that categories are more or less bad to deal with exceptions.</p>\n</blockquote>\n<p>Something I'd like to point out is the following: </p>\n<p>a DisCoCat is often thought of as a monoidal functor </p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>F</mi><mo>:</mo><mi>C</mi><mo>→</mo><mrow><mi mathvariant=\"sans-serif\">V</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">c</mi><mi mathvariant=\"sans-serif\">t</mi></mrow></mrow><annotation encoding=\"application/x-tex\"> F : C \\to \\mathsf{Vect}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathsf\" style=\"margin-right:0.01389em;\">V</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">c</span><span class=\"mord mathsf\">t</span></span></span></span></span></p>\n<p>Where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> is the free compact closed category on a set of grammatical types and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"sans-serif\">V</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">c</mi><mi mathvariant=\"sans-serif\">t</mi></mrow><annotation encoding=\"application/x-tex\">\\mathsf{Vect}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathsf\" style=\"margin-right:0.01389em;\">V</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">c</span><span class=\"mord mathsf\">t</span></span></span></span></span> is the category of real vector spaces made monoidal with tensor product. (<strong>fun exercise btw: prove that a monoidal functor also preserves compact closed structure when it is present</strong>) There is some debate about exactly what sort of category <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> should be. Instead you could choose <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> to be the free pregroup on a set...but as Anne Preller proved a monoidal functor from one of these can only be trivial so the usual next step is to wave your hands and claim that it all morally works.</p>\n<p>You can compose <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span></span></span></span> with the forgetful functor <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>U</mi><mo>:</mo><mrow><mi mathvariant=\"sans-serif\">V</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">c</mi><mi mathvariant=\"sans-serif\">t</mi></mrow><mo>→</mo><mrow><mi mathvariant=\"sans-serif\">S</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">t</mi></mrow></mrow><annotation encoding=\"application/x-tex\">U : \\mathsf{Vect} \\to \\mathsf{Set}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathsf\" style=\"margin-right:0.01389em;\">V</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">c</span><span class=\"mord mathsf\">t</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathsf\">S</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">t</span></span></span></span></span> to get a functor</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>U</mi><mo>∘</mo><mi>F</mi><mo>:</mo><mi>C</mi><mo>→</mo><mrow><mi mathvariant=\"sans-serif\">S</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">t</mi></mrow></mrow><annotation encoding=\"application/x-tex\"> U \\circ F : C \\to \\mathsf{Set}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∘</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathsf\">S</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">t</span></span></span></span></span></p>\n<p>Functors of this sort are equivalent to discrete opfibrations</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi><mo>→</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\"> X \\to C </annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span></p>\n<p>via the Grothendieck or \"category of elements\" construction</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>∫</mo><mo>:</mo><mo stretchy=\"false\">[</mo><mi>C</mi><mo separator=\"true\">,</mo><mrow><mi mathvariant=\"sans-serif\">S</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">t</mi></mrow><mo stretchy=\"false\">]</mo><mover><mo stretchy=\"true\">→</mo><mpadded lspace=\"0.3em\" width=\"+0.6em\"><mo lspace=\"0em\" rspace=\"0em\">≅</mo></mpadded></mover><mrow><mi mathvariant=\"sans-serif\">D</mi><mi mathvariant=\"sans-serif\">i</mi><mi mathvariant=\"sans-serif\">s</mi><mi mathvariant=\"sans-serif\">c</mi><mi mathvariant=\"sans-serif\">O</mi><mi mathvariant=\"sans-serif\">p</mi><mi mathvariant=\"sans-serif\">F</mi><mi mathvariant=\"sans-serif\">i</mi><mi mathvariant=\"sans-serif\">b</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"sans-serif\">C</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\"> \\int : [C, \\mathsf{Set}] \\xrightarrow{\\cong} \\mathsf{DiscOpFib(C)} </annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:1.11112em;vertical-align:-0.30612em;\"></span><span class=\"mop op-symbol small-op\" style=\"margin-right:0.19445em;position:relative;top:-0.0005599999999999772em;\">∫</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.2843em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathsf\">S</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">t</span></span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel x-arrow\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0343em;\"><span style=\"top:-3.322em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight x-arrow-pad\"><span class=\"mord mtight\"><span class=\"mrel mtight\">≅</span></span></span></span><span class=\"svg-align\" style=\"top:-2.689em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"hide-tail\" style=\"height:0.522em;min-width:1.469em;\"><svg height=\"0.522em\" preserveAspectRatio=\"xMaxYMin slice\" viewBox=\"0 0 400000 522\" width=\"400em\"><path d=\"M0 241v40h399891c-47.3 35.3-84 78-110 128 -16.7 32-27.7 63.7-33 95 0 1.3-.2 2.7-.5 4-.3 1.3-.5 2.3-.5 3 0 7.3 6.7 11 20  11 8 0 13.2-.8 15.5-2.5 2.3-1.7 4.2-5.5 5.5-11.5 2-13.3 5.7-27 11-41 14.7-44.7  39-84.5 73-119.5s73.7-60.2 119-75.5c6-2 9-5.7 9-11s-3-9-9-11c-45.3-15.3-85 -40.5-119-75.5s-58.3-74.8-73-119.5c-4.7-14-8.3-27.3-11-40-1.3-6.7-3.2-10.8-5.5 -12.5-2.3-1.7-7.5-2.5-15.5-2.5-14 0-21 3.7-21 11 0 2 2 10.3 6 25 20.7 83.3 67  151.7 139 205zm0 0v40h399900v-40z\"></path></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.010999999999999899em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathsf\">D</span><span class=\"mord mathsf\">i</span><span class=\"mord mathsf\">s</span><span class=\"mord mathsf\">c</span><span class=\"mord mathsf\">O</span><span class=\"mord mathsf\">p</span><span class=\"mord mathsf\">F</span><span class=\"mord mathsf\">i</span><span class=\"mord mathsf\">b</span><span class=\"mopen\">(</span><span class=\"mord mathsf\">C</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>I think that <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>∫</mo><mo stretchy=\"false\">(</mo><mi>U</mi><mo>∘</mo><mi>F</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\int (U \\circ F)</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:1.11112em;vertical-align:-0.30612em;\"></span><span class=\"mop op-symbol small-op\" style=\"margin-right:0.19445em;position:relative;top:-0.0005599999999999772em;\">∫</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∘</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mclose\">)</span></span></span></span> is morally what Prof Coecke is talking about when talks about one category with the grammar and semantics in the same place. The objects of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>∫</mo><mo stretchy=\"false\">(</mo><mi>U</mi><mo>∘</mo><mi>F</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\int (U \\circ F) </annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:1.11112em;vertical-align:-0.30612em;\"></span><span class=\"mop op-symbol small-op\" style=\"margin-right:0.19445em;position:relative;top:-0.0005599999999999772em;\">∫</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∘</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mclose\">)</span></span></span></span> are pairs <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,v)</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mclose\">)</span></span></span></span> where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span> is a product of grammatical types and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">v</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span> is a vector in the tensor product of their meaning spaces.</p>\n<p>So my point is that thinking of the grammar and semantics in one place doesn't really fix the issue about having the grammar category be a pregroup. Every property in the definition of \"monoidal functor\" corresponds to some desirable property of \"the category with grammar and semantics in the same place\".</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span></span></span></span> being a functor with domain <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> ensures that <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>∫</mo><mi>U</mi><mo>∘</mo><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">\\int U \\circ F</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:1.11112em;vertical-align:-0.30612em;\"></span><span class=\"mop op-symbol small-op\" style=\"margin-right:0.19445em;position:relative;top:-0.0005599999999999772em;\">∫</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∘</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span></span></span></span> is closed under composition</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span></span></span></span> being monoidal ensures that the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">v</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span>  (in the object <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,v)</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mclose\">)</span></span></span></span>) described above is an element of the tensor product of the meaning spaces in the types making up <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span>.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span></span></span></span> preserving identities means <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>∫</mo><mi>U</mi><mo>∘</mo><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">\\int U \\circ F</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:1.11112em;vertical-align:-0.30612em;\"></span><span class=\"mop op-symbol small-op\" style=\"margin-right:0.19445em;position:relative;top:-0.0005599999999999772em;\">∫</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∘</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span></span></span></span> contains trivial reductions from words to themselves.</li>\n</ul>\n<p>So if you give up the approach of seeing <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span></span></span></span> as a monoidal functor, then you must give up the above things as well!</p>",
        "id": 212465167,
        "sender_full_name": "Jade Master",
        "timestamp": 1602008284
    },
    {
        "content": "<p>The problem is that there is no notion of functor <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span></span></span></span>, to my knowledge, for which <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> is a satisfying notion of grammar</p>",
        "id": 212477076,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602014301
    },
    {
        "content": "<p>This is exactly because of what Preller showed, as you pointed out. On the contrary, building <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> as freely generated over a dicrionary (as Preller and Lambek themselves suggest as a fix) is simply as far as you can get from what a linguist means when they say \"grammar\"</p>",
        "id": 212477228,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602014365
    },
    {
        "content": "<p>I am very sure that grammar should emerge from semantics, and not the other way around. That is, if one wants to chase a functor, it should go from <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"bold\">V</mi><mi mathvariant=\"bold\">e</mi><mi mathvariant=\"bold\">c</mi><mi mathvariant=\"bold\">t</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Vect}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68611em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">V</span><span class=\"mord mathbf\">e</span><span class=\"mord mathbf\">c</span><span class=\"mord mathbf\">t</span></span></span></span></span> to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>, for some (pregroup?) <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>.</p>",
        "id": 212477478,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602014432
    },
    {
        "content": "<p>I have some intuition about how to do it. Someone I know is planning to write her master thesis on this tho, so I'm not making any progress until then. <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 212477644,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602014512
    },
    {
        "content": "<p>(Everything is obviously made more complicated by the fact that I'm not in academia, so supervision is more complicated, but that's something for another time)</p>",
        "id": 212477787,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602014558
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276438\">Fabrizio Genovese</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212477228\">said</a>:</p>\n<blockquote>\n<p>simply as far as you can get from what a linguist means when they say \"grammar\"</p>\n</blockquote>\n<p>Why? Morphisms of the free autonomous category are grammatical derivations, right? Sounds like grammar to me</p>",
        "id": 212478367,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1602014824
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276438\">Fabrizio Genovese</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212477228\">said</a>:</p>\n<blockquote>\n<p>This is exactly because of what Preller showed, as you pointed out. On the contrary, building <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> as freely generated over a dicrionary (as Preller and Lambek themselves suggest as a fix) is simply as far as you can get from what a linguist means when they say \"grammar\"</p>\n</blockquote>\n<p>I  don't think that's true. If you lose the compact closed structure you can encode reduction rules for a grammar by encoding them into a pre-net.( i.e. if a*b reduces to c then include a transition ab -&gt; c). The free monoidal category on this pre-net gives a reasonable (but bare-bones) model of a grammar. A monoidal functor from this grammar to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"sans-serif\">V</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">c</mi><mi mathvariant=\"sans-serif\">t</mi></mrow><annotation encoding=\"application/x-tex\">\\mathsf{Vect}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathsf\" style=\"margin-right:0.01389em;\">V</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">c</span><span class=\"mord mathsf\">t</span></span></span></span></span> gives a semantic interpretation of this grammar.</p>",
        "id": 212479079,
        "sender_full_name": "Jade Master",
        "timestamp": 1602015121
    },
    {
        "content": "<p>Maybe the free monoidal category isn't fancy enough to be satisfying and I understand that. Regardless I think it's a useful conceptual framework.</p>",
        "id": 212479845,
        "sender_full_name": "Jade Master",
        "timestamp": 1602015433
    },
    {
        "content": "<p>To interject with a question that is only tangentially related and possibly unwelcome... </p>\n<p>Why is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"sans-serif\">V</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">c</mi><mi mathvariant=\"sans-serif\">t</mi></mrow><annotation encoding=\"application/x-tex\">\\mathsf{Vect}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathsf\" style=\"margin-right:0.01389em;\">V</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">c</span><span class=\"mord mathsf\">t</span></span></span></span></span> a reasonable semantic domain for natural language? </p>\n<p>I’ve seen a lot of talks on DisCoCat and this has never made sense to me, but it’s never felt appropriate to ask.</p>",
        "id": 212480236,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602015584
    },
    {
        "content": "<p>aka \"the category of things that go fast on your graphics card\"</p>",
        "id": 212480469,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1602015703
    },
    {
        "content": "<p>That's a reasonable question which is welcome to me at least.</p>",
        "id": 212480481,
        "sender_full_name": "Jade Master",
        "timestamp": 1602015712
    },
    {
        "content": "<p>Because the dimension is pushed as high as you possibly can, anything nonlinear is probably impossible to actually run</p>",
        "id": 212480769,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1602015849
    },
    {
        "content": "<p>Lot's of people like <span class=\"user-mention\" data-user-id=\"300045\">@Martha Lewis</span> think it's not the best choice (Is that right Martha?) If I remember correctly, the idea is that a better category for semantics might be something like the category of convex relations. You can find more info about that in this paper: <a href=\"https://arxiv.org/abs/1703.08314\">https://arxiv.org/abs/1703.08314</a></p>",
        "id": 212480847,
        "sender_full_name": "Jade Master",
        "timestamp": 1602015892
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275901\">Jules Hedges</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212480769\">said</a>:</p>\n<blockquote>\n<p>Because the dimension is pushed as high as you possibly can, anything nonlinear is probably impossible to actually run</p>\n</blockquote>\n<p>This complaint I'm not sure I get. DisCoCat isn't supposed to be something you directly turn into code right? My understanding is that it's a way to frame the mathematical problem which might inspire code within that framework.</p>",
        "id": 212481133,
        "sender_full_name": "Jade Master",
        "timestamp": 1602016021
    },
    {
        "content": "<p>I think prematurely stuffing something into the framework of linear algebra, before you've figured out what the math <em>should</em> be, is dangerous.  But it's also extremely common.</p>",
        "id": 212481591,
        "sender_full_name": "John Baez",
        "timestamp": 1602016245
    },
    {
        "content": "<p>I think it's better to slow down a bit, figure out the right math for a subject, and then figure out how to do approximations that make it efficient to compute.</p>",
        "id": 212481699,
        "sender_full_name": "John Baez",
        "timestamp": 1602016303
    },
    {
        "content": "<p>For example gauge field theory was formulated long before people started finding efficient algorithms in \"lattice gauge theory\" to do the computations - like, compute the mass of the proton from first principles!</p>",
        "id": 212481859,
        "sender_full_name": "John Baez",
        "timestamp": 1602016357
    },
    {
        "content": "<p>But I think in a subject like linguistics people are so unsure that there <em>is</em> a \"right\" way to do things that they're more likely to seek a computationally convenient framework just to get anything to work at all.</p>",
        "id": 212481954,
        "sender_full_name": "John Baez",
        "timestamp": 1602016420
    },
    {
        "content": "<p>I think the issue I’m having with <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"sans-serif\">V</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">c</mi><mi mathvariant=\"sans-serif\">t</mi></mrow><annotation encoding=\"application/x-tex\">\\mathsf{Vect}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathsf\" style=\"margin-right:0.01389em;\">V</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">c</span><span class=\"mord mathsf\">t</span></span></span></span></span>, which seems to persist if we move to convex relations, is that this picture of language is completely divorced from reality (in a literal sense).</p>",
        "id": 212482168,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602016515
    },
    {
        "content": "<p>Do you know a framework that's <em>not</em> \"completely divorced from reality\"?</p>",
        "id": 212482227,
        "sender_full_name": "John Baez",
        "timestamp": 1602016555
    },
    {
        "content": "<p>I’ve encountered ideas of what language is that are not, at least in the sense I mean, and frankly i don’t see how it could be any other way: language <em>describes</em>.</p>",
        "id": 212482641,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602016751
    },
    {
        "content": "<p>I can’t give you a mathematical framework for this sort of thing, although I do think this is a very important question.</p>",
        "id": 212482718,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602016799
    },
    {
        "content": "<p>I don’t think “It’s all we have!” Is a terribly good justification.</p>",
        "id": 212482781,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602016817
    },
    {
        "content": "<p>Neither do I, but if you're saying one approach to linguistics \"completely divorced from reality\" I'm curious if you like the other approaches better: there are lots.</p>",
        "id": 212483340,
        "sender_full_name": "John Baez",
        "timestamp": 1602017117
    },
    {
        "content": "<p>I don't think the main purpose of language is to \"describe\", by the way.</p>",
        "id": 212483398,
        "sender_full_name": "John Baez",
        "timestamp": 1602017146
    },
    {
        "content": "<p>I believe people talk mainly to get other people to do things.</p>",
        "id": 212483417,
        "sender_full_name": "John Baez",
        "timestamp": 1602017156
    },
    {
        "content": "<p>But anyway, \"the main purpose(s) of language\" is something linguists have been discussing for a long time.</p>",
        "id": 212483636,
        "sender_full_name": "John Baez",
        "timestamp": 1602017256
    },
    {
        "content": "<p>I certainly don’t know what the “main purpose” (does that even make sense?!) of language is, but it must in any case “describe”. How else would you communicate what you want someone to do, for example?</p>",
        "id": 212483815,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602017361
    },
    {
        "content": "<p>I always thought of DisCoCat as natural language processing (aka NLP) rather than linguistics. Although I think different people have different ideas on that distinction</p>",
        "id": 212483909,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1602017411
    },
    {
        "content": "<p>Well, I'm saying that any approach to language that can't handle utterances like</p>\n<p>\"Wow!\"</p>\n<p>or</p>\n<p>\"Get out of my face!\"</p>\n<p>is not really handling language as spoken by ordinary people.   </p>\n<p>I don't think these utterances are mainly descriptive.</p>",
        "id": 212483962,
        "sender_full_name": "John Baez",
        "timestamp": 1602017452
    },
    {
        "content": "<p>That’s a good point.</p>",
        "id": 212483998,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602017473
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"275901\">@Jules Hedges</span> fair enough</p>",
        "id": 212484218,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602017591
    },
    {
        "content": "<p>Think of it as a physicist. A model that's useful only in some restricted domain can still be useful</p>",
        "id": 212484344,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1602017648
    },
    {
        "content": "<p>I see that from that angle my objections are pretty unfair haha.</p>",
        "id": 212484471,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602017732
    },
    {
        "content": "<p>In linguistics, the technical area studying \"what (a given instance of language) is <em>for</em>\" falls under <a href=\"https://en.wikipedia.org/wiki/Pragmatics\">pragmatics</a>. This is an echelon of language comprehension above semantics; if semantics describes the information content of a sentence on its own, then it's bound to take some extra work to incorporate the extra information contained in the sentence's surroundings.</p>",
        "id": 212484618,
        "sender_full_name": "Morgan Rogers (he/him)",
        "timestamp": 1602017809
    },
    {
        "content": "<p>I guess NLP benchmark datasets just don't contain interjections like that or other \"\"weird\"\" stuff. (Or only some of them do so you get to ignore those ones). I don't know whether that's because the benchmark datasets only contain the stuff that you need for the applications they have in mind (I think typically stuff like automatic sentiment analysis), or rather that nobody bothered to include stuff in the datasets that nobody had any idea how to handle</p>",
        "id": 212484696,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1602017852
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"277473\">[Mod] Morgan Rogers</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212484618\">said</a>:</p>\n<blockquote>\n<p>In linguistics, the technical area studying \"what (a given instance of language) is <em>for</em>\" falls under <a href=\"https://en.wikipedia.org/wiki/Pragmatics\">pragmatics</a>.</p>\n</blockquote>\n<p>That page actually contains a lot of aspects of language that look hard to tackle mathematically! NLP has its work cut out...</p>",
        "id": 212485062,
        "sender_full_name": "Morgan Rogers (he/him)",
        "timestamp": 1602018027
    },
    {
        "content": "<p>Forgetting for a moment our logical training, isn’t it sort of bizarre to consider an utterance “out of context” at all?</p>",
        "id": 212485520,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602018270
    },
    {
        "content": "<p>It seems like a natural starting point, no?</p>",
        "id": 212485720,
        "sender_full_name": "Morgan Rogers (he/him)",
        "timestamp": 1602018377
    },
    {
        "content": "<p>Personally I suspect that if we start there then we’ve already lost so much of what language is “about” that we’re taking about something else. Syntax with no semantics.</p>",
        "id": 212485872,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602018475
    },
    {
        "content": "<p>(I do realise that in this direction lies quite a lot of what we all do!)</p>",
        "id": 212486005,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602018538
    },
    {
        "content": "<p>Mathematics is an attempt to create an idealized language that's mainly descriptive - sentences are supposed to be <em>true</em> or <em>false</em> - and where the sentences make sense out of context, except for the context of the overall system they inhabit, like \"the language of Peano arithmetic\".</p>",
        "id": 212486286,
        "sender_full_name": "John Baez",
        "timestamp": 1602018643
    },
    {
        "content": "<p>These simplifications make it easier to understand mathematical language mathematically than ordinary language.</p>",
        "id": 212486369,
        "sender_full_name": "John Baez",
        "timestamp": 1602018675
    },
    {
        "content": "<p>And indeed one of the reasons for \"regimenting\" mathematical language this way was to make it to make it easier to study mathematically!   (Meta-mathematics.)</p>",
        "id": 212486501,
        "sender_full_name": "John Baez",
        "timestamp": 1602018726
    },
    {
        "content": "<p>While this is great, programming languages bust out of this framework (e.g. imperative languages can say \"do this\", and have a richer system of contexts) and human language busts out of it even more.</p>",
        "id": 212486576,
        "sender_full_name": "John Baez",
        "timestamp": 1602018776
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276114\">Chad Nester</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212485872\">said</a>:</p>\n<blockquote>\n<p>Personally I suspect that if we start there then we’ve already lost so much of what language is “about” that we’re taking about something else. Syntax with no semantics.</p>\n</blockquote>\n<p>We don't remove context completely; rather, we break things down to the basic building blocks (usually individual words) and analyse those pieces. They acquire relative meaning from their relationships to one another; a Vect-model can learn from a relatively small dataset that amongst nouns, \"cat\" and \"dog\" are semantically similar, because they turn up around similar words and phrases, even without having a model of what \"real\" things those words might be attached to.<br>\nWhat's missing is the interactions with non-language, and by their very nature such interactions are difficult to fit into a formalism describing language.</p>",
        "id": 212486711,
        "sender_full_name": "Morgan Rogers (he/him)",
        "timestamp": 1602018850
    },
    {
        "content": "<p>It’s just so.... <em>flat</em> :o</p>",
        "id": 212487186,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602019087
    },
    {
        "content": "<p>(I have to sign off for now but will check back! — thanks everyone for the discussion so far!)</p>",
        "id": 212487246,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602019133
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212486576\">said</a>:</p>\n<blockquote>\n<p>While this is great, programming languages bust out of this framework (e.g. imperative languages can say \"do this\", and have a richer system of contexts)...</p>\n</blockquote>\n<p>This is a great intermediate example that could be helpful: how does one handle the context in computer science? One has the advantage that the interaction between a computer and the outside world is very controlled: there are inputs and outputs, and we know in what form the information will enter and leave through these; in particular there are manageable limits on these. Natural language has the whole world under its purview, so it's an overwhelming task to try to handle the context as if they were just inputs and outputs, but structurally it's not too much of a stretch. We just need to decide which ingredients to start with and not be too ambitious.</p>",
        "id": 212488059,
        "sender_full_name": "Morgan Rogers (he/him)",
        "timestamp": 1602019611
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275901\">Jules Hedges</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212478367\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276438\">Fabrizio Genovese</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212477228\">said</a>:</p>\n<blockquote>\n<p>simply as far as you can get from what a linguist means when they say \"grammar\"</p>\n</blockquote>\n<p>Why? Morphisms of the free autonomous category are grammatical derivations, right? Sounds like grammar to me</p>\n</blockquote>\n<p>A grammar generated over a dictionary is not a grammar. It's literally taking all the language and calling it a grammar.</p>",
        "id": 212537855,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602065539
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276114\">Chad Nester</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212480236\">said</a>:</p>\n<blockquote>\n<p>To interject with a question that is only tangentially related and possibly unwelcome... </p>\n<p>Why is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"sans-serif\">V</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">c</mi><mi mathvariant=\"sans-serif\">t</mi></mrow><annotation encoding=\"application/x-tex\">\\mathsf{Vect}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathsf\" style=\"margin-right:0.01389em;\">V</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">c</span><span class=\"mord mathsf\">t</span></span></span></span></span> a reasonable semantic domain for natural language? </p>\n<p>I’ve seen a lot of talks on DisCoCat and this has never made sense to me, but it’s never felt appropriate to ask.</p>\n</blockquote>\n<p>I don't think it is. But it allows you to build meaning out of statistic correlations between words, which is something you can extract automatically from data. All in all, vector spaces are one of the few categories that allow you to build a semantics that is not a toy.</p>",
        "id": 212537960,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602065630
    },
    {
        "content": "<p>In any case, when it comes to fields like linguistics, I think we should save content over categories. Yes, taking \"free categories\" solves things categorically. But people have been thinking formally about what language is since Ferdinand De Saussurre in the beginning of the last century. Basically giving up all that just because you want a compact closed category around is short-sighted (if not insulting for a ton of people having worked in that field before).</p>",
        "id": 212538437,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602065907
    },
    {
        "content": "<p>With this I mean that either we do NLP, as Bob did in his  last paper, and give up on this \"grammar -&gt; semantics\" functor altogether, or we don't, but in this case we should maybe give up at calling \"grammar\" something that is not a grammar. The whole point of using pre-groups is that they have been shown, if I recall correctly, to be equivalent to context-free grammars. The great advantage of DisCoCat was exactly that it took something linguists liked very much (context free grammars) and related it to something computer scientists doing NLP liked very much (vector spaces). If you have to replace pre-groups with something else, frankly all the niceness of the framework goes away in my opinion.</p>",
        "id": 212538729,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602066078
    },
    {
        "content": "<p>I would already be putting \"syntax\" as the domain of the functor rather than grammar. Would that be an acceptable compromise to you <span class=\"user-mention\" data-user-id=\"276438\">@Fabrizio Genovese</span>, since syntax can take many forms?</p>",
        "id": 212547646,
        "sender_full_name": "Morgan Rogers (he/him)",
        "timestamp": 1602072237
    },
    {
        "content": "<p>It would be, but I think it is fundamental to ask \"what is syntax in the context of language\"?</p>",
        "id": 212547749,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602072302
    },
    {
        "content": "<p>And well, the first to give an answer was Panini 2500 years ago, roughly</p>",
        "id": 212547772,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602072324
    },
    {
        "content": "<p>He even invented pushdown automata in the process, so we could say that his answer was already quite formal.</p>",
        "id": 212547795,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602072345
    },
    {
        "content": "<p>What I am trying to say is that any compromise, to be acceptable, should entail studying all (or some) of these efforts, and trying to recast them with category theory. It is very easy to throw away the baby with the bath water otherwise, which is exactly what I think happens when one ditches pre-groups for free compact closed categories.</p>",
        "id": 212548035,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602072499
    },
    {
        "content": "<p>So, when you say \"syntax\", the natural question should be \"does you definition of syntax actually say or capture something interesting that happens in language?\"</p>",
        "id": 212548104,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602072557
    },
    {
        "content": "<p>John already pointed out some limitations of the context-free setting, namely, the difficulty of modelling things such as utterances, interjections and the like.</p>",
        "id": 212548181,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602072604
    },
    {
        "content": "<p>One of the problems there, imho, is that such elements are more context sensitive than other common elements of language. \"Wow\" can be used to mean sincere amazement or that you are seriously pissed off about something. This is very context-dependent, and it is not surprising that you'll have an hard time describing this with context-free grammars. So yes, there's definitely margin for improvement</p>",
        "id": 212548327,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602072699
    },
    {
        "content": "<p>I can think about a ton of things that happen in language that are really difficult to capture formally, even if endeavors have been made: Sentence tone and rythm, pitch accent, the synthetic Vs analythic characteristics of a languages, all the way up to corner cases such as Piraha which does not have recursion, and hence cannot really be modelled by a context-free language or anything more powerful than that</p>",
        "id": 212548649,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602072862
    },
    {
        "content": "<p>This entails another big problem: It is difficult to say what is the \"syntax of language\" without saying what is language. And, at least in my opinion, all the answers given to this question are quite anglo-centric, or at best western-centric. So we should also keep in mind that, in defining what \"grammar\" would even be, even categorically, we are starting from a very biased perspective. The reason why stuff like tones and accents are difficult to capture by \"traditional\" formal methods is that these are not features that are heavily present in English, and so may appear to be of secondary importance to a indo-european speaker</p>",
        "id": 212548968,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602073061
    },
    {
        "content": "<p>All in all, I think that focusing on \"how can I make this into a functor\" is not the right kind of question. There is quite a lot  of stuff to study before that, and quite a lot of philosophical, conceptual reasoning to make before even trying to set up the problem, depending mainly on the degree of generality you want to operate. <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 212549405,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602073232
    },
    {
        "content": "<p>Anyway, if we consider grammar in a traditional sense, I am confident that the functor should go the other way around. There are a lot of reasons for this, but in general the idea of using grammar to \"orchestrate meaning of language\" as in DisCoCat is made complicated by the fact that you can bend grammar pretty much in any way you like and still convey meaning. \"I hungry please food now\" is totally understandable yet not grammatical.</p>",
        "id": 212550093,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602073579
    },
    {
        "content": "<p>Grammar, as a \"generally useful description of how parts of language interact with each other\" should emerge from syntax and meaning \"up to epsilon\", not the other way around. This is why I'm critical of the functorial approach. I think it is too stiff.</p>",
        "id": 212550330,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602073659
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276438\">Fabrizio Genovese</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212550093\">said</a>:</p>\n<blockquote>\n<p>Anyway, if we consider grammar in a traditional sense, I am confident that the functor should go the other way around. There are a lot of reasons for this, but in general the idea of using grammar to \"orchestrate meaning of language\" as in DisCoCat is made complicated by the fact that you can bend grammar pretty much in any way you like and still convey meaning. \"I hungry please food now\" is totally understandable yet not grammatical.</p>\n</blockquote>\n<p>I don't get how this is an argument for the functor going the other way. The \"syntax -&gt; semantics\" or \"grammar -&gt; semantics\" functor is one that translates [instance of language] into \"meaning of [instance of language]\". Determining precisely where (in what category, say) \"meaning\" should live is a big hard question, but even in day to day experience we find plenty of experiences, i.e. \"meaning\", that language is not equipped to express, so a mapping from meaning to language doesn't seem like a feasible direction for this relationship to go. What did you have in mind?</p>",
        "id": 212554027,
        "sender_full_name": "Morgan Rogers (he/him)",
        "timestamp": 1602075557
    },
    {
        "content": "<p>The key here is \"up to epsilon\", I'm DMing you if that's ok</p>",
        "id": 212554783,
        "sender_full_name": "Fabrizio Genovese",
        "timestamp": 1602075886
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276114\">Chad Nester</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212480236\">said</a>:</p>\n<blockquote>\n<p>Why is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"sans-serif\">V</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">c</mi><mi mathvariant=\"sans-serif\">t</mi></mrow><annotation encoding=\"application/x-tex\">\\mathsf{Vect}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathsf\" style=\"margin-right:0.01389em;\">V</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">c</span><span class=\"mord mathsf\">t</span></span></span></span></span> a reasonable semantic domain for natural language? </p>\n<p>I’ve seen a lot of talks on DisCoCat and this has never made sense to me, but it’s never felt appropriate to ask.</p>\n</blockquote>\n<p>As far as I understand it grew out of a partially-verified empirical claim about the way that words are distributed in text. Others here know this better than I do and should correct me if I'm wrong. </p>\n<p>The idea is that, from large corpora of texts, you can compress essential language statistics down to vector representations for single words. One easy way is to compute a big array whose <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><mo separator=\"true\">,</mo><msup><mi>w</mi><mo lspace=\"0em\" mathvariant=\"normal\" rspace=\"0em\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">w,w'</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.946332em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span>-entry is the number of times that word <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>w</mi><mo lspace=\"0em\" mathvariant=\"normal\" rspace=\"0em\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">w'</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span> appeared together for some predefined notion of context (usually, a window of surrounding words or the same sentence, etc.). After some post-processing step like PCA or SVD  you obtain a decomposition of this matrix into more manageable factors from which you can get a vector for each word in your corpus. These days, from what I understand, language models (GPT-3 for example) are generative models: they model the probability distribution of sequences of words. Usually this is obtained by training a (deep) neural net to predict the occurrence of words in context. Then, the vector representation of a word is extracted from parameters of the neural network used to approximate the distribution.</p>\n<p>You might object that these are just arrays of numbers and not really vectors, since we make no use of the linear structure. And Discocat appeals to linearity in a crucial way, in the sense that the meaning of a sentence is computed by contracting a bunch of tensors, representing the meaning of individual words or phrases. But where do these tensors come from? And why does it make any sense to compose them following the shape of a grammatical derivation to compute the meaning of sentences? Here, Discocat relies on the assumption that, somehow, the vector representations obtained from the statistical data espouses the shape of the grammar. But this is a non-trivial empirical claim. It turns out that it is partially justified, for very simple cases where the vector representations capture some semantic relationships between words as linear dependencies, e.g., <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>v</mi><mrow><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub><mo>−</mo><msub><mi>v</mi><mrow><mi>m</mi><mi>a</mi><mi>n</mi></mrow></msub><mo>+</mo><msub><mi>v</mi><mrow><mi>w</mi><mi>o</mi><mi>m</mi><mi>a</mi><mi>n</mi></mrow></msub><mo>≈</mo><msub><mi>v</mi><mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">v_{king} - v_{man} + v_{woman} \\approx v_{queen}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8694379999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mord mathnormal mtight\">n</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">g</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.73333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">m</span><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.63312em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathnormal mtight\">o</span><span class=\"mord mathnormal mtight\">m</span><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">≈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span><span class=\"mord mathnormal mtight\">u</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>. I think this suffices to justify the way that certain simple adjectives are handled in Discocat and some of the papers have shown how to handle several toy examples. But to justify the way that the meaning of more complex sentences is derived, one would need to make sure that the distributional model contains much more complex higher-rank linear dependencies in order to guarantee that, say, a transitive verbs acts like a bilinear map, as it is supposed to. I know that people have worked on building (or discovering) these higher-rank tensors from statistical data but imho much more is still needed to justify the underlying assumptions that it makes sense to compute the meaning of sentences using this form of composition (a functor from grammar to linear semantics). </p>\n<p>Of course, another route is to change the model completely, going beyond distributional data to more structured representations.</p>",
        "id": 212557841,
        "sender_full_name": "Robin Piedeleu",
        "timestamp": 1602077335
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"277342\">@Robin Piedeleu</span>  Thanks for this answer!</p>",
        "id": 212652100,
        "sender_full_name": "Chad Nester",
        "timestamp": 1602139531
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275901\">Jules Hedges</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212484696\">said</a>:</p>\n<blockquote>\n<p>I guess NLP benchmark datasets just don't contain interjections like that or other \"\"weird\"\" stuff. (Or only some of them do so you get to ignore those ones). I don't know whether that's because the benchmark datasets only contain the stuff that you need for the applications they have in mind (I think typically stuff like automatic sentiment analysis), or rather that nobody bothered to include stuff in the datasets that nobody had any idea how to handle</p>\n</blockquote>\n<p>This is NOT true! The Universal Dependencies framework (worked by Stanford and Google and more than 300 individual contributors--UD is an open community effort with over 300 contributors producing more than 150 treebanks in 90 languages. <a href=\"https://universaldependencies.org/\">https://universaldependencies.org/</a> ) certainly has things like Wow and Hey and everythign else that it's difficult to model.</p>",
        "id": 212718362,
        "sender_full_name": "Valeria de Paiva",
        "timestamp": 1602175733
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"277342\">Robin Piedeleu</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212557841\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276114\">Chad Nester</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212480236\">said</a>:</p>\n<blockquote>\n<p>Why is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"sans-serif\">V</mi><mi mathvariant=\"sans-serif\">e</mi><mi mathvariant=\"sans-serif\">c</mi><mi mathvariant=\"sans-serif\">t</mi></mrow><annotation encoding=\"application/x-tex\">\\mathsf{Vect}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathsf\" style=\"margin-right:0.01389em;\">V</span><span class=\"mord mathsf\">e</span><span class=\"mord mathsf\">c</span><span class=\"mord mathsf\">t</span></span></span></span></span> a reasonable semantic domain for natural language? </p>\n<p>I’ve seen a lot of talks on DisCoCat and this has never made sense to me, but it’s never felt appropriate to ask.</p>\n</blockquote>\n<p>As far as I understand it grew out of a partially-verified empirical claim about the way that words are distributed in text. Others here know this better than I do and should correct me if I'm wrong. </p>\n</blockquote>\n<p>Great job summarizing the suggested explanation <span class=\"user-mention\" data-user-id=\"277342\">@Robin Piedeleu</span> !<br>\n but personally, I say that it isn't a reasonable semantic domain for natural language at all. </p>\n<p>the proposed explanation in terms of Curry-Howard correspondence doesn't make sense to me:  compact closed categories have no sense of negation  (because conjunctions, disjunctions and implications are all the same using the Curry-Howard correspondence in compact closed setups like Vect) and natural language is all about entailments and contradictions (which require negations and differences between the basic connectives!).  <br>\nNLP in real life needs to pay attention to coverage (any theory whatsoever can do ten simple sentences, the problem is to do the NYT!) and as Robin says the vectors work very well for measuring similarities, but that's about all, IFAIC.</p>",
        "id": 212721194,
        "sender_full_name": "Valeria de Paiva",
        "timestamp": 1602176968
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"276656\">@Valeria de Paiva</span>! </p>\n<p>I agree with your remark about logic in compact-closed categories. However, as I understand it, Discocat places logic at a different level: logical connectives should just be words and therefore, their interpretation is not at the type level but at the semantic level.  Concretely, negation should be a linear endomap on the sentence space, conjunction a bilinear maps etc.  Of course it is really unclear exactly which maps they should be, which is why your objection remains completely valid: there is no easy way to accommodate the usual logical connectives in the standard distributional semantics.  In fact, I think this was one of <span class=\"user-mention\" data-user-id=\"276692\">@Bob Coecke</span>'s main reasons to change the semantics to something more structured, the first candidate being density matrices, inspired by quantum physics. </p>\n<p>Your comment also reminded me of an important point I failed to problematise earlier: how do we evaluate what a reasonable semantics for natural language is? As someone who is interested in NLP and linguistics mostly from the outside, I have no idea. The traditional NLP approach seems to be all about improving scores at performing certain tasks, like reference resolution, question answering, relationship extraction, fooling people into thinking they are reading content produced by another human, what have you. The modern answer is that generative models  can perform really well at all of these tasks and more (provided that you have enough computing power to train humongous neural nets) so that getting a good model of the distribution of word sequences is a good semantics (if we stay within the realm of text processing at least---I know nothing about speech processing). Discocat seems to be focused on something else: in Discocat, semantics captures similarity of text fragments, e.g., two sentences have similar meaning if they represent close vectors in sentence space, whatever that is. This seems sensible but it is not at all obvious to me how we can use it to perform traditional NLP tasks better. But maybe that is not the aim. Maybe we're  just trying to understand better how language works, which aspects are genuinely compositional vs. which ones are not (but then, can that ever be divorced from predictive power?). I'm rambling now so I'll stop, but I would be happy to hear the opinion of people who work directly on this (<span class=\"user-mention\" data-user-id=\"294964\">@Alexis Toumi</span> ?).</p>",
        "id": 212841069,
        "sender_full_name": "Robin Piedeleu",
        "timestamp": 1602257538
    },
    {
        "content": "<p>DisCoCat has nothing directly to do with Vect cf the first papers where we also used Real as an example model, and later papers with conceptual spaces, density matrices etc.  See e.g. also this more recent paper <a href=\"https://arxiv.org/abs/1904.03478\">https://arxiv.org/abs/1904.03478</a> where framework and model are clearly distinct.  That said, FVect is where most current practical NLP takes place, so ignoring that model would be stupid.  For quantum computing the arguments is even stronger.</p>",
        "id": 212843450,
        "sender_full_name": "Bob Coecke",
        "timestamp": 1602258716
    },
    {
        "content": "<p>This is the real fun you get out of working with vector spaces.  This is a general comment about ACT, just don't talk about applied stuff, but do it!  <br>\n<a href=\"https://medium.com/cambridge-quantum-computing/quantum-natural-language-processing-748d6f27b31d\">https://medium.com/cambridge-quantum-computing/quantum-natural-language-processing-748d6f27b31d</a></p>",
        "id": 212844272,
        "sender_full_name": "Bob Coecke",
        "timestamp": 1602259006
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"277342\">Robin Piedeleu</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212841069\">said</a>:</p>\n<blockquote>\n<p>Your comment also reminded me of an important point I failed to problematise earlier: how do we evaluate what a reasonable semantics for natural language is? As someone who is interested in NLP and linguistics mostly from the outside, I have no idea. The traditional NLP approach seems to be all about improving scores at performing certain tasks, like reference resolution, question answering, relationship extraction, fooling people into thinking they are reading content produced by another human, what have you. The modern answer is that generative models  can perform really well at all of these tasks and more (provided that you have enough computing power to train humongous neural nets) </p>\n</blockquote>\n<p>Well, I beg to differ again. the generative models do NOT perform really well at all of these tasks, without a huge hedge. They perform really well within their training: there's a collection of literature showing that they cannot generalize, that their impressive results go down the drain with tiny adversarial modifications that show that they did not understand the contents at all--some of this new literature is collected in this blog post (<a href=\"https://logic-forall.blogspot.com/2020/03/artifacts-in-nlp.html\">https://logic-forall.blogspot.com/2020/03/artifacts-in-nlp.html</a>) from March 2020. but my checking of the literature was done in Nov 2019 and since then the number of papers explaining the failings of the so-called \"super-human\" performance has increased.</p>",
        "id": 212870064,
        "sender_full_name": "Valeria de Paiva",
        "timestamp": 1602274081
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276692\">Bob Coecke</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212843450\">said</a>:</p>\n<blockquote>\n<p>DisCoCat has nothing directly to do with Vect cf the first papers where we also used Real as an example model, and later papers with conceptual spaces, density matrices etc.  See e.g. also this more recent paper <a href=\"https://arxiv.org/abs/1904.03478\">https://arxiv.org/abs/1904.03478</a> where framework and model are clearly distinct.  That said, FVect is where most current practical NLP takes place, so ignoring that model would be stupid.  For quantum computing the arguments is even stronger.</p>\n</blockquote>\n<p>Hi Dr Coecke, nice to see you. Do you have any comment regarding what I said at the beginning of this topic?</p>",
        "id": 212937794,
        "sender_full_name": "Jade Master",
        "timestamp": 1602369170
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276037\">Jade Master</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212937794\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276692\">Bob Coecke</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212843450\">said</a>:</p>\n<blockquote>\n<p>DisCoCat has nothing directly to do with Vect cf the first papers where we also used Real as an example model, and later papers with conceptual spaces, density matrices etc.  See e.g. also this more recent paper <a href=\"https://arxiv.org/abs/1904.03478\">https://arxiv.org/abs/1904.03478</a> where framework and model are clearly distinct.  That said, FVect is where most current practical NLP takes place, so ignoring that model would be stupid.  For quantum computing the arguments is even stronger.</p>\n</blockquote>\n<p>Hi Dr Coecke, nice to see you. Do you have any comment regarding what I said at the beginning of this topic?</p>\n</blockquote>\n<p>Hi Jade, this is exactly what we did in the 1st paper:<br>\n<a href=\"http://www.cs.ox.ac.uk/people/stephen.clark/papers/qai08.pdf\">http://www.cs.ox.ac.uk/people/stephen.clark/papers/qai08.pdf</a><br>\nand I agree (for many reasons) that this may be the better view.  The only reason we changed is that people seemed to prefer the functor representation.</p>",
        "id": 212960891,
        "sender_full_name": "Bob Coecke",
        "timestamp": 1602414223
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276692\">Bob Coecke</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212960891\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276037\">Jade Master</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212937794\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276692\">Bob Coecke</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212843450\">said</a>:</p>\n<blockquote>\n<p>DisCoCat has nothing directly to do with Vect cf the first papers where we also used Real as an example model, and later papers with conceptual spaces, density matrices etc.  See e.g. also this more recent paper <a href=\"https://arxiv.org/abs/1904.03478\">https://arxiv.org/abs/1904.03478</a> where framework and model are clearly distinct.  That said, FVect is where most current practical NLP takes place, so ignoring that model would be stupid.  For quantum computing the arguments is even stronger.</p>\n</blockquote>\n<p>Hi Dr Coecke, nice to see you. Do you have any comment regarding what I said at the beginning of this topic?</p>\n</blockquote>\n<p>Hi Jade, this is exactly what we did in the 1st paper:<br>\n<a href=\"http://www.cs.ox.ac.uk/people/stephen.clark/papers/qai08.pdf\">http://www.cs.ox.ac.uk/people/stephen.clark/papers/qai08.pdf</a><br>\nand I agree (for many reasons) that this may be the better view.  The only reason we changed is that people seemed to prefer the functor representation.</p>\n</blockquote>\n<p>My point is that neither the functor approach or the product space approach is better because they are equivalent. The issues with the functor approach will also show up in the product space approach.</p>",
        "id": 212969905,
        "sender_full_name": "Jade Master",
        "timestamp": 1602430708
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276037\">Jade Master</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212969905\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276692\">Bob Coecke</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212960891\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276037\">Jade Master</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212937794\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276692\">Bob Coecke</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212843450\">said</a>:</p>\n<blockquote>\n<p>DisCoCat has nothing directly to do with Vect cf the first papers where we also used Real as an example model, and later papers with conceptual spaces, density matrices etc.  See e.g. also this more recent paper <a href=\"https://arxiv.org/abs/1904.03478\">https://arxiv.org/abs/1904.03478</a> where framework and model are clearly distinct.  That said, FVect is where most current practical NLP takes place, so ignoring that model would be stupid.  For quantum computing the arguments is even stronger.</p>\n</blockquote>\n<p>Hi Dr Coecke, nice to see you. Do you have any comment regarding what I said at the beginning of this topic?</p>\n</blockquote>\n<p>Hi Jade, this is exactly what we did in the 1st paper:<br>\n<a href=\"http://www.cs.ox.ac.uk/people/stephen.clark/papers/qai08.pdf\">http://www.cs.ox.ac.uk/people/stephen.clark/papers/qai08.pdf</a><br>\nand I agree (for many reasons) that this may be the better view.  The only reason we changed is that people seemed to prefer the functor representation.</p>\n</blockquote>\n<p>My point is that neither the functor approach or the product space approach is better because they are equivalent. The issues with the functor approach will also show up in the product space approach.</p>\n</blockquote>\n<p>I see.  Well, then again, :) at the moment I typically first present the diagrams, and then fill them in with models, which avoids the problem, right?  In some forthcoming paper with Vincent Wang we are moving entirely away from pregroups.  In practice, you only used 1/2 of the pregroup for the grammatical structure anyway (cf only cups, no caps).  But you are very right to use the world \"morally\", cause really, that's what the functor/pairing provided, some moral.</p>",
        "id": 212979048,
        "sender_full_name": "Bob Coecke",
        "timestamp": 1602445951
    },
    {
        "content": "<p>Oh great. I'm excited to see what y'all do.</p>",
        "id": 212980518,
        "sender_full_name": "Jade Master",
        "timestamp": 1602448346
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276656\">Valeria de Paiva</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/212870064\">said</a>:</p>\n<blockquote>\n<p>Well, I beg to differ again. the generative models do NOT perform really well at all of these tasks, without a huge hedge. They perform really well within their training: there's a collection of literature showing that they cannot generalize, that their impressive results go down the drain with tiny adversarial modifications that show that they did not understand the contents at all--some of this new literature is collected in this blog post (<a href=\"https://logic-forall.blogspot.com/2020/03/artifacts-in-nlp.html\">https://logic-forall.blogspot.com/2020/03/artifacts-in-nlp.html</a>) from March 2020. but my checking of the literature was done in Nov 2019 and since then the number of papers explaining the failings of the so-called \"super-human\" performance has increased.</p>\n</blockquote>\n<p>Thanks for your reply and for the very interesting blog post! It is reassuring to see concrete evidence that distributional features obtained from generative models are insufficient to capture essential structural properties. It seems that logic still has an important role to play in NLP.  Your post also points towards an answer to the question I (and implicitly <span class=\"user-mention\" data-user-id=\"276114\">@Chad Nester</span>)  asked before: how do we decide what is a reasonable semantics for natural language? You seem to suggest that possible answers are semantics that provide a basis for effective inference. I like the idea of inference as the mother of all NLP tasks! I wonder how far Discocat can be pushed in this direction. I know there is already some work on using density matrix representations for this purpose but, as <span class=\"user-mention\" data-user-id=\"276692\">@Bob Coecke</span> pointed out, the framework is very flexible and could theoretically accommodate much more structured semantics.</p>",
        "id": 213024777,
        "sender_full_name": "Robin Piedeleu",
        "timestamp": 1602500910
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"277342\">Robin Piedeleu</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/DisCoCat.20and.20the.20Grothendieck.20construction/near/213024777\">said</a>:</p>\n<p>I like the idea of inference as the mother of all NLP tasks! </p>\n<div class=\"codehilite\"><pre><span></span><code>So do I! Thanks!\nBut the other important criteria is really *coverage*. only ten sentences any theory can be made to do.\n</code></pre></div>",
        "id": 213199678,
        "sender_full_name": "Valeria de Paiva",
        "timestamp": 1602614779
    }
]