[
    {
        "content": "<p>For reference in case you missed Bob's online QNLP seminar, here's a copy of the chat with some useful question-answers and references:</p>\n<p>From Emily Riehl to Everyone: (06:02 pm)<br>\n I have to say I’m very impressed with the theatrics. <br>\nFrom Tim Sears to Everyone: (06:10 pm)<br>\n This is the coolest seminar ever. <br>\nFrom Me to Everyone: (06:11 pm)<br>\n the shady lighting adds so much to the scenery <br>\nFrom Samuel Tenka to Everyone: (06:12 pm)<br>\n question: are these states/effects normalized? <br>\nFrom Me to Everyone: (06:12 pm)<br>\n They’re not normalised ! <br>\nFrom Samuel Tenka to Everyone: (06:13 pm)<br>\n thanks!  So we keep track of a bit of extra information in our diagrams.  good to know :-) <br>\nFrom Me to Everyone: (06:13 pm)<br>\n Scalars would be represented as closed boxes with no input or output wires <br>\nFrom Samuel Tenka to Everyone: (06:13 pm)<br>\n gotcha.  makes sense.  so right now these are equivalent to penrose string diagrams? <br>\nFrom Me to Everyone: (06:14 pm)<br>\n Yes, they are the same Penrose string diagrams! (N.B. You need to assume finite dimensional Hilbert spaces if you want a compact-closed category.) <br>\nFrom Samuel Tenka to Everyone: (06:17 pm)<br>\n otherwise the cup (special state) won't be well defined? <br>\nFrom Priyaa to Everyone: (06:18 pm)<br>\n yes. one would get infinite sum otherwise <br>\nFrom Me to Everyone: (06:18 pm)<br>\n Exactly! But in quantum computing you only consider a finite number of quits anyway. <br>\nFrom David I Spivak to Everyone: (06:21 pm)<br>\n So is the cat alive or dead right now? <br>\nFrom Samuel Tenka to Everyone: (06:21 pm)<br>\n XD <br>\nFrom Tim Sears to Everyone: (06:25 pm)<br>\n Is this the best paper for further reading? <a href=\"https://arxiv.org/pdf/1904.03478.pdf\" title=\"https://arxiv.org/pdf/1904.03478.pdf\">https://arxiv.org/pdf/1904.03478.pdf</a> <br>\nFrom Brendan to Everyone: (06:27 pm)<br>\n Bob listed these as references for this talk in the abstract: <br>\nFrom Brendan to Everyone: (06:27 pm)<br>\n [1] B. Coecke, G. De Felice, K. Meichanetzidis &amp; A. Toumi (2020) Quantum Natural Language Processing: We did it! <a href=\"https://medium.com/cambridge-quantum-computing/quantum-natural-language-processing-748d6f27b31d\" title=\"https://medium.com/cambridge-quantum-computing/quantum-natural-language-processing-748d6f27b31d\">https://medium.com/cambridge-quantum-computing/quantum-natural-language-processing-748d6f27b31d</a>.  [2] B. Coecke (2016) From quantum foundations via natural language meaning to a theory of everything. arXiv:1602.07618. <br>\nFrom Me to Everyone: (06:28 pm)<br>\n Yes, that paper gives some strong intuition for the QNLP experiments we did. For the more technical / categorical details we just submitted this tool paper : <a href=\"https://arxiv.org/abs/2005.02975\" title=\"https://arxiv.org/abs/2005.02975\">https://arxiv.org/abs/2005.02975</a> <br>\nFrom Brendan to Everyone: (06:28 pm)<br>\n He also mentioned at the beginning of the talk there’s a new paper on the arXiv today, I think, but I can’t find it <br>\nFrom theprotoncat to Everyone: (06:28 pm)<br>\n The arXiv is the tools paper Alexis mentioned I assume <br>\nFrom Brendan to Everyone: (06:29 pm)<br>\n Yes it is; thanks Alexis <br>\nFrom joscha to Everyone: (06:29 pm)<br>\n Are the outputs/inputs ‘labelled’ to know which ones are allowed to match? <br>\nFrom joscha to Everyone: (06:30 pm)<br>\n (noun going out, noun accepted, ..) <br>\nFrom Me to Everyone: (06:30 pm)<br>\n Yes! The blog post [1] describes the experiments for a general audience, the paper [2] gives a nice story compositionally from physics to NLP @joscha the basic types are the labels <br>\nFrom joscha to Everyone: (06:32 pm)<br>\n thanks <br>\nFrom Konstantinos Meichanetzidis to Everyone: (06:33 pm)<br>\n In general, each type-wire carries a vector space of a type-dependent dimension <br>\nFrom theprotoncat to Everyone: (06:34 pm)<br>\n I assume in the quantum case they’re all Hilbert spaces of different dimensions? <br>\nFrom Konstantinos Meichanetzidis to Everyone: (06:34 pm)<br>\n so after you join the wires according to how the types compose, then you forget the labels and have just some vector spaces yes that's right, in the quantum case, if you have qubits, they are hilbert spaces of some power of two, say <br>\nFrom quaang to Everyone: (06:36 pm)<br>\n To deal with different dimensions in diagrams, you could use a framework which I call qufinite ZX-calculus. <br>\nFrom Konstantinos Meichanetzidis to Everyone: (06:37 pm)<br>\n yes! <br>\nFrom Me to Everyone: (06:53 pm)<br>\n Here’s the repo if you want to play with it : <a href=\"https://github.com/oxford-quantum-group/discopy\" title=\"https://github.com/oxford-quantum-group/discopy\">https://github.com/oxford-quantum-group/discopy</a> <br>\nFrom Samuel Tenka to Everyone: (06:54 pm)<br>\n You should share that on the Zulip, too! <br>\nFrom Me to Everyone: (06:54 pm)<br>\n True! <br>\nFrom Samuel Tenka to Everyone: (06:57 pm)<br>\n Question: what computation did the IBM computer actually do?  (e.g. did it predict the next word in a sentence?  or, toward learning grammar, did it figure out the arity of various words?) <br>\nFrom Lee J. O'Riordan to Everyone: (06:59 pm)<br>\n What kind of circuit depths do you hit for these on both machines? <br>\nFrom Me to Everyone: (07:00 pm)<br>\n It solved some basic question answering task, i.e. given “Does Alice love Bob ?” It said “Yes!\" <br>\nFrom Giovanni de Felice to Everyone: (07:00 pm)<br>\n Check out the circuits here: <br>\nFrom Giovanni de Felice to Everyone: (07:00 pm)<br>\n<a href=\"https://github.com/oxford-quantum-group/discopy/blob/master/notebooks/qnlp-experiment.ipynb\" title=\"https://github.com/oxford-quantum-group/discopy/blob/master/notebooks/qnlp-experiment.ipynb\">https://github.com/oxford-quantum-group/discopy/blob/master/notebooks/qnlp-experiment.ipynb</a> <br>\nFrom Me to Everyone: (07:01 pm)<br>\n The circuits had CNOT depth two <br>\nFrom Yutaka Shikano to Everyone: (07:01 pm)<br>\n Question: How many qubits can we maximally use? More than 5Q machines, such as 16 qubits, 20 qubits, and 53 qubits, can we complile? <br>\nFrom Lee J. O'Riordan to Everyone: (07:01 pm)<br>\n Cheers Alexis. <br>\nFrom Giovanni de Felice to Everyone: (07:01 pm)<br>\n Depth is around 8/10 before the ket compiler <br>\nFrom Samuel Tenka to Everyone: (07:01 pm)<br>\n Neat!  Near the beginning, Bob mentioned that this framework was able to \"understand grammar\".  Does this mean that you didn't have to hardcode the circuit topology?  E.g., how did you figure out that Alice is a noun etc (classical NLP approaches can do this by scanning over a large corpus) <br>\nFrom Me to Everyone: (07:01 pm)<br>\n Depth 2 for “Alice loves Bob”, depth 3 for “Alice loves Bob who is rich” <br>\nFrom Stephen Mell to Everyone: (07:01 pm)<br>\n Do these need to be done on a real quantum computer, or can they be done in simulation (and if so, can they be done at larger scale)? <br>\nFrom Tim Sears to Everyone: (07:02 pm)<br>\n Same question as Stephen <br>\nFrom Me to Everyone: (07:02 pm)<br>\n They were done by simulation beforehand. For now it was only a proof-of-concept but it can / it will be scaled indeed. Basically we used 1 qubit for representing a noun, in general we can use n-qubit noun space <br>\nFrom Tim Sears to Everyone: (07:03 pm)<br>\n Would be nice to compare to word2vec or some latter day NN’s like BERT. <br>\nFrom Me to Everyone: (07:04 pm)<br>\n @Samuel the idea is we took the grammatical structure of the sentence (which can be computed efficiently) and mapped it automatically (with a monodical functor actually) to the architecture of the circuits <br>\nFrom Konstantinos Meichanetzidis to Everyone: (07:05 pm)<br>\n Indeed. We also are aware of word2ket! But priority is scaling up as Alexis said. Also yes we should compare with other models, but we should compare fairly: i.e. compare models that have the same order of parameters <br>\nFrom Yutaka Shikano to Everyone: (07:05 pm)<br>\n Similar question on qubit size or real-device usage:<br>\nHow many qubits do we need to go beyond the proof-of-concept stage? <br>\nFrom Tim Sears to Everyone: (07:06 pm)<br>\n You have a richer, but more compact and interpretable model in the ML sense. <br>\nFrom Giovanni de Felice to Everyone: (07:06 pm)<br>\n @Ytaka Using around 50 qubits would match the google supremacy experiment <br>\nFrom Tim Sears to Everyone: (07:07 pm)<br>\n Can you do a large scale version of this on a classical computer? <br>\nFrom theprotoncat to Everyone: (07:07 pm)<br>\n I have a slight side-quest question: could ZX-calculus be used to translate efficiently between digital and analog QC? Particularly when we are stuck with some specific native gates on some device, say an Ising Hamiltonian is the only entangling gate. It would involve trotterization I guess, but is there any intuition for looking into it? <br>\nFrom Giovanni de Felice to Everyone: (07:07 pm)<br>\n The dimension of the space grows exponentially in the number of qubits, so it would be very expensive after 30 qubits or so <br>\nFrom Tim Sears to Everyone: (07:08 pm)<br>\n thx <br>\nFrom Samuel Tenka to Everyone: (07:08 pm)<br>\n @Alexis can one compute the parse tree of a sentence without knowing the parts-of-speech of words?  Most modern approaches to NLP can figure out grammatical rules just from a corpus of un-annotated strings.  In other words, can one learn the types of words without manually feeding annotations to the quantum computer? <br>\nFrom Konstantinos Meichanetzidis to Everyone: (07:09 pm)<br>\n Also note: expensive classical sim is not only about qubit number: it's about how entangling the circuit is. <br>\nFrom Samuel Tenka to Everyone: (07:09 pm)<br>\n (if not, the work is still really cool!  I'm just confused by Bob's advertisement that the quantum computer was able to understand grammar because it seems the grammar was manually fed into it) <br>\nFrom Tim Sears to Everyone: (07:09 pm)<br>\n Might be worth it since the modeling approach is interesting on its own <br>\nFrom Me to Everyone: (07:10 pm)<br>\n @Samuel, that is indeed one of the big open questions we need to solve. There is this paper by Smolensky et al. where they propose a quantum algorithm for learning the grammatical structure : <a href=\"https://arxiv.org/abs/1902.05162\" title=\"https://arxiv.org/abs/1902.05162\">https://arxiv.org/abs/1902.05162</a> <br>\nFrom Samuel Tenka to Everyone: (07:10 pm)<br>\n Awesome!  Thanks! <br>\nFrom Giovanni de Felice to Everyone: (07:10 pm)<br>\n @Samuel we didn’t implement a grammar induction algorithm for regroups yet but it should be possible and would be interesting if a quantum computer could do it</p>",
        "id": 196803262,
        "sender_full_name": "Alexis Toumi",
        "timestamp": 1588872232
    },
    {
        "content": "<p>More copy and paste:</p>\n<p>From Konstantinos Meichanetzidis to Everyone: (07:11 pm)<br>\n Also: a classical parser would do: it would annotate the text with types (noun, verb, etc...) and then the circuit is created by reducing the types to the sentence type. <br>\nFrom Tim Sears to Everyone: (07:11 pm)<br>\n Maybe there is a translation to the nearest neural network. Then you can have better explanation for a NN that “works well in practice” <br>\nFrom Matteo Capucci to Everyone: (07:12 pm)<br>\n How universal is this approach wrt languages? Is it mainly targeted at English and similar languages or does it work in more generality? <br>\nFrom Konstantinos Meichanetzidis to Everyone: (07:12 pm)<br>\n For now, the assignment of types, and the state preparation (or computing the meaning) as Bob calls it, can be seen as independent stages <br>\nFrom Konstantinos Meichanetzidis to Everyone: (07:12 pm)<br>\n @Matteo pregroup grammar is supposed to capture most natural language structure <br>\nFrom Me to Everyone: (07:13 pm)<br>\n @Matteo pregroups are equivalent to Chomsky’s context-free grammars so you can do all of those, you can do mildly context-sensitive with a small variation on pregroups <br>\nFrom Konstantinos Meichanetzidis to Everyone: (07:13 pm)<br>\n They are as powerful as context free grammars, and they are pretty much universally applicable <br>\nFrom Lee Mondshein to Everyone: (07:13 pm)<br>\n Re “meaning in use”: can you comment on meaning as dynamic significance ( or influence) in some dynamic context of shared discourse + belief + action ? <br>\nFrom Brian Pinsky to Everyone: (07:13 pm)<br>\n You're eliding things like tense in your verb denotations.  How do you want to deal with that? <br>\nFrom Konstantinos Meichanetzidis to Everyone: (07:14 pm)<br>\n @ Tim Sears: good point about nearest NN @Brian Pinsky: the point is that more pregroup types and rules on them capture morphology like that <br>\nFrom Matteo Capucci to Everyone: (07:16 pm)<br>\n (thanks!) <br>\nFrom Martha Lewis to Everyone: (07:16 pm)<br>\n This paper by Sadrzadeh et al is <br>\nFrom Martha Lewis to Everyone: (07:17 pm)<br>\n looks at dynamic aspects of language: <a href=\"http://www.eecs.qmul.ac.uk/~mpurver/papers/sadrzadeh-et-al18semdial.pdf\" title=\"http://www.eecs.qmul.ac.uk/~mpurver/papers/sadrzadeh-et-al18semdial.pdf\">http://www.eecs.qmul.ac.uk/~mpurver/papers/sadrzadeh-et-al18semdial.pdf</a> <br>\nFrom Me to Everyone: (07:18 pm)<br>\n @Brian on the syntactic side, you can deal with tense by adding more specific types instead of just sentence and noun. On the semantic side, it’s not completely clear how to deal with tense yet. <br>\nFrom Martha Lewis to Everyone: (07:19 pm)<br>\n re: tense, Lambek does include aspects like person, sentence type in his original work on pregroups Lambek 1999: <a href=\"https://link.springer.com/chapter/10.1007/3-540-48975-4_1\" title=\"https://link.springer.com/chapter/10.1007/3-540-48975-4_1\">https://link.springer.com/chapter/10.1007/3-540-48975-4_1</a> <br>\nFrom Konstantinos Meichanetzidis to Everyone: (07:22 pm)<br>\n @theprotoncat In principle, yes, as ZX is complete. In practice, it's a different topic and it reduces to how one maps from digital circs to cv qc <br>\nFrom Brian Pinsky to Everyone: (07:22 pm)<br>\n @alexis tense seems pretty well solved in classical linguistic frameworks.  I'm not sure why translating it into a pregroup grammar should be hard, but we should have this conversation on zulip <br>\nFrom Me to Everyone: (07:23 pm)<br>\n yes, let’s all go to Zulip ! <br>\nFrom theprotoncat to Everyone: (07:23 pm)<br>\n @konstantinos Thanks yeah I figured this is a far more complicated question than to be discussed right now. The intuition is ‘it’s possible but probably not efficient’.</p>",
        "id": 196803332,
        "sender_full_name": "Alexis Toumi",
        "timestamp": 1588872249
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"294964\">Alexis Toumi</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/QNLP/near/196803262\" title=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/QNLP/near/196803262\">said</a>:</p>\n<blockquote>\n<p>From Emily Riehl to Everyone: (06:02 pm)<br>\n I have to say I’m very impressed with the theatrics. <br>\nFrom Tim Sears to Everyone: (06:10 pm)<br>\n This is the coolest seminar ever. </p>\n</blockquote>\n<p>You succeeded in making me feel bad for having missed it <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span></p>",
        "id": 196804480,
        "sender_full_name": "Morgan Rogers (he/him)",
        "timestamp": 1588872736
    },
    {
        "content": "<p>The talk was recorded, it should appear on youtube soon :)</p>",
        "id": 196804777,
        "sender_full_name": "Alexis Toumi",
        "timestamp": 1588872892
    },
    {
        "content": "<p>Oh wow, actually it's already on youtube: <a href=\"https://www.youtube.com/watch?v=YV6zcQCiRjo\" title=\"https://www.youtube.com/watch?v=YV6zcQCiRjo\">https://www.youtube.com/watch?v=YV6zcQCiRjo</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"YV6zcQCiRjo\" href=\"https://www.youtube.com/watch?v=YV6zcQCiRjo\" title=\"https://www.youtube.com/watch?v=YV6zcQCiRjo\"><img src=\"https://i.ytimg.com/vi/YV6zcQCiRjo/default.jpg\"></a></div>",
        "id": 196805271,
        "sender_full_name": "Alexis Toumi",
        "timestamp": 1588873127
    },
    {
        "content": "<p>The correct link seems to be <a href=\"https://www.youtube.com/watch?v=mL-hWbwVphk\">https://www.youtube.com/watch?v=mL-hWbwVphk</a> now :)</p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"mL-hWbwVphk\" href=\"https://www.youtube.com/watch?v=mL-hWbwVphk\"><img src=\"https://i.ytimg.com/vi/mL-hWbwVphk/default.jpg\"></a></div>",
        "id": 197136519,
        "sender_full_name": "Antonin Delpeuch",
        "timestamp": 1589199529
    },
    {
        "content": "<p>This may be of interest to the QNLP folks. There's going to be an interactive podcast on May 18, 3pm Pacific Time, by Sam Charrington, of This Week in ML and AI, who's going to be exploring the question of whether linguistics has been missing from NLP research with <a href=\"https://faculty.washington.edu/ebender/\">Emily Bender</a> of the University of Washington. Seems like something that QNLP can weigh in on. Link with registration details below:</p>\n<p><a href=\"https://twimlai.com/live-viewing-party-linguistics-missing-nlp-research/\">https://twimlai.com/live-viewing-party-linguistics-missing-nlp-research/</a></p>",
        "id": 197648218,
        "sender_full_name": "(=_=)",
        "timestamp": 1589518275
    }
]