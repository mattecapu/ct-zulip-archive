[
    {
        "content": "<p>I find myself taking a dive into the world of <a href=\"https://ncatlab.org/nlab/show/category-theoretic+approaches+to+probability+theory\">[[category-theoretic approaches to probability theory]]</a> and it's raising some questions for me. That nLab page, predominently written by <span class=\"user-mention\" data-user-id=\"275989\">@Paolo Perrone</span>, talks of three main structures of interest: Markov categories, Probability monads, and Dagger categories.</p>\n<p>Taking the second of these, I see that there's a desire to have some nicer category than <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi><mi>e</mi><mi>a</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">Meas</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">s</span></span></span></span>, so that one might look at, say, the quasi-topos of <a href=\"https://ncatlab.org/nlab/show/quasi-Borel+space\">[[quasi-Borel spaces]]</a>. This would be for probabilistic programming purposes, cartesian closure obviously being a good thing. </p>\n<p>But presumably we shouldn't see this line of research as competing with the Markov category approach, since at some point one will want to use the Kleisli category of some probability monad, and, as we hear at <a href=\"https://ncatlab.org/nlab/show/monads+of+probability%2C+measures%2C+and+valuations\">[[monads of probability, measures, and valuations]]</a></p>\n<blockquote>\n<p>Kleisli categories of probability monads are often instances of Markov categories.</p>\n</blockquote>\n<p>This is the case for the probability monad on <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi><mi>b</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">Qbs</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">Q</span><span class=\"mord mathnormal\">b</span><span class=\"mord mathnormal\">s</span></span></span></span>. </p>\n<p>Might we say then that the best of both worlds comes in situations where a topos-like category of measurable spaces with a probability monad generates a Markov category through the Kleisli construction?</p>\n<p>Sometimes Kleisli categories inherit some structure, such as <a href=\"https://www.cl.cam.ac.uk/~mpf23/papers/PreSheaves/CT2004.pdf\">here</a>, where there are even examples of some being themselves toposes. Do the Kleisli categories of probability monads typically have some useful extra structure beyond being Markov derived from the structure of the original category?</p>",
        "id": 521061117,
        "sender_full_name": "David Corfield",
        "timestamp": 1748512817
    },
    {
        "content": "<p>Yes, that's more or less what we want. <br>\nKleisli categories of probability monads tend moreover to be <em>representable</em> Markov categories, which we particularly nice. (And they can be even nicer, such as <em>observationallly</em> representable.)</p>",
        "id": 521063570,
        "sender_full_name": "Paolo Perrone",
        "timestamp": 1748513747
    },
    {
        "content": "<p>Thanks, Paolo! And I see the first of these is on the nLab at <a href=\"https://ncatlab.org/nlab/show/Markov+category#representable_markov_categories\">[[Representable Markov categories]]</a>. </p>\n<p>Could you say briefly what <em>observational representability</em> adds?</p>",
        "id": 521069576,
        "sender_full_name": "David Corfield",
        "timestamp": 1748515956
    },
    {
        "content": "<p>I think it's worth adding that having a \"nice\" category equipped with a probability monad (whatever this means precisely) is <em>not</em> sufficient for the purposes of probability theory, and it can in fact fall far short of it. Let me illustrate this with two examples, where the first one is rather obvious and the second may be quite surprising:</p>\n<ol>\n<li>\n<p>Set is a category that is as nice as it gets, and presumably the <a href=\"https://ncatlab.org/nlab/show/distribution+monad\">distribution monad</a> counts as a probability monad. The resulting Markov category is good for the purposes of <em>discrete</em> probability theory, but obviously doesn't capture any measure-theoretic probability. Categorically, this manifests itself in the lack of <a href=\"https://ncatlab.org/nlab/show/Kolmogorov%20products\">[[Kolmogorov products]]</a> for the Markov category. Since Kolmogorov products are necessary to talk about joint distributions of infinitely many random variables -- a core theme in probability -- we're lacking a core feature, and this happens although the category we started with was extremely nice.</p>\n</li>\n<li>\n<p>Quasi-Borel spaces with their probability monad do not conform with the desiderata of probability theory either.  As we've shown in Prop 3.3 of <a href=\"https://arxiv.org/abs/2211.02507\">Dilations and information flow axioms in categorical probability</a>, the Kleisli category fails to be a <em>positive</em> Markov category. In a positive Markov category, every distribution on a product space <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi><mo>×</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X \\times Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span> which has a deterministic marginal necessarily makes the two variables independent. This is a basic feature enjoyed by ordinary probability in both the discrete and measure-theoretic incarnations, but it fails for distributions on quasi-Borel spaces. I believe that this rules them out as a convenient category for probability.</p>\n</li>\n</ol>",
        "id": 521069903,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1748516091
    },
    {
        "content": "<p>In other words, there are many additional conditions that one can impose on a Markov category in order for it to support some of the basic theorems of probability, as they have been proven in entirely categorical terms based on these conditions. And these conditions are quite unrelated to whether the subcategory of deterministic morphisms is cartesian closed or a topos.</p>\n<p>It may also be interesting to note that these conditions are really just properties, such as representability; no extra structure is needed for any of the existing results (that I can think of right now).</p>",
        "id": 521075891,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1748518382
    },
    {
        "content": "<p>I don't want to deny that cartesian closure (of the category of deterministic morphisms) is important for probabilistic programming. But I do want to argue that other properties are more important than that still, because without these you don't even get anything that resembles probability!</p>",
        "id": 521076456,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1748518600
    },
    {
        "content": "<p>Thanks! And nLab has <a href=\"https://ncatlab.org/nlab/show/Markov+category#positivity_and_causality\">[[positivity]]</a> too (no doubt thanks to Paolo). </p>\n<p>If <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi><mi>b</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">Qbs</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">Q</span><span class=\"mord mathnormal\">b</span><span class=\"mord mathnormal\">s</span></span></span></span> is lacking, as you explained, is there a leading candidate for a good subcategory of deterministic morphisms? One which would allow the good features of its probabilistic Markov category mentioned.</p>",
        "id": 521076779,
        "sender_full_name": "David Corfield",
        "timestamp": 1748518709
    },
    {
        "content": "<p>I should read further! There's a nice table at <a href=\"https://ncatlab.org/nlab/show/Markov+category#DetailedList\">[[Markov category -- Detailed list]]</a> where it seems that <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>l</mi><mi>S</mi><mi>t</mi><mi>o</mi><mi>c</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">BorelStoch</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\">ore</span><span class=\"mord mathnormal\">lSt</span><span class=\"mord mathnormal\">oc</span><span class=\"mord mathnormal\">h</span></span></span></span> is doing well. But then presumably <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>l</mi><mi>M</mi><mi>e</mi><mi>a</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">BorelMeas</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\">ore</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">lM</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">s</span></span></span></span> isn't so nice.</p>",
        "id": 521078226,
        "sender_full_name": "David Corfield",
        "timestamp": 1748519235
    },
    {
        "content": "<p>There aren't any leading candidates that I'm aware of, but this doesn't mean much -- Paolo, <span class=\"user-mention\" data-user-id=\"308397\">@Sam Staton</span> and <span class=\"user-mention\" data-user-id=\"386742\">@Sean Moss</span> will know the start of the art on this problem better.</p>",
        "id": 521079060,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1748519537
    },
    {
        "content": "<p>Right, BorelStoch is our \"default\" Markov category for measure-theoretic probability, for essentially the same reasons as why probability theorists typically work with standard Borel spaces. That seems to work pretty well in the sense that essentially all measurable spaces that tend to occur in probability research (on both the pure and applied side) are standard Borel. But the deterministic subcategory BorelMeas is indeed not that nice: it only has countable products but not uncountable ones, and it's not cartesian closed. Relatedly, BorelStoch only has countable Kolmogorov products but not uncountable ones.</p>",
        "id": 521079069,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1748519542
    },
    {
        "content": "<p>About nice categories of deterministic morphisms: as proven <a href=\"https://arxiv.org/abs/2007.08638\">in proposition 5.8 here</a>, under reasonable assumptions, cartesian closed categories which have \"something like the real line\" tend not to be positive. So I feel there's a bit of a compromise necessary.</p>",
        "id": 521095928,
        "sender_full_name": "Paolo Perrone",
        "timestamp": 1748525684
    },
    {
        "content": "<p><strong>About observational representability:</strong> it's true, there is not much on the nLab, and it's becoming such a standard notion that we should probably write something about it. (A stub is <a href=\"https://ncatlab.org/nlab/show/observational+monad\">here</a>, but more needs to be added.)<br>\nRoughly, it says that <em>we can distinguish two probability distributions by sampling them independently many times</em>, which is what people do (ideally) in statistics, science, etc.<br>\nFormally, this says that given the <a href=\"https://ncatlab.org/nlab/show/sampling+map\">sampling map</a> <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi></mrow><mo>:</mo><mi>P</mi><mi>X</mi><mo>→</mo><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">\\mathit{samp}:PX\\to X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathit\">samp</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">PX</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span> (the counit of the Kleisli adjunction), the composite maps </p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>P</mi><mi>X</mi><mover><mo stretchy=\"true\" minsize=\"3.0em\">→</mo><mpadded width=\"+0.6em\" lspace=\"0.3em\"><mrow><mi>c</mi><mi>o</mi><mi>p</mi><mi>y</mi></mrow></mpadded></mover><mo stretchy=\"false\">(</mo><mi>P</mi><mi>X</mi><msup><mo stretchy=\"false\">)</mo><mi>n</mi></msup><mover><mo stretchy=\"true\" minsize=\"3.0em\">→</mo><mpadded width=\"+0.6em\" lspace=\"0.3em\"><mrow><mi>s</mi><mi>a</mi><mi>m</mi><msup><mi>p</mi><mi>n</mi></msup></mrow></mpadded></mover><msup><mi>X</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">PX \\xrightarrow{copy} (PX)^n \\xrightarrow{samp^n} X^n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9344em;vertical-align:-0.011em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">PX</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel x-arrow\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9234em;\"><span style=\"top:-3.322em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight x-arrow-pad\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">co</span><span class=\"mord mathnormal mtight\">p</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span></span></span></span><span class=\"svg-align\" style=\"top:-2.689em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"hide-tail\" style=\"height:0.522em;min-width:1.469em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"400em\" height=\"0.522em\" viewBox=\"0 0 400000 522\" preserveAspectRatio=\"xMaxYMin slice\"><path d=\"M0 241v40h399891c-47.3 35.3-84 78-110 128\n-16.7 32-27.7 63.7-33 95 0 1.3-.2 2.7-.5 4-.3 1.3-.5 2.3-.5 3 0 7.3 6.7 11 20\n 11 8 0 13.2-.8 15.5-2.5 2.3-1.7 4.2-5.5 5.5-11.5 2-13.3 5.7-27 11-41 14.7-44.7\n 39-84.5 73-119.5s73.7-60.2 119-75.5c6-2 9-5.7 9-11s-3-9-9-11c-45.3-15.3-85\n-40.5-119-75.5s-58.3-74.8-73-119.5c-4.7-14-8.3-27.3-11-40-1.3-6.7-3.2-10.8-5.5\n-12.5-2.3-1.7-7.5-2.5-15.5-2.5-14 0-21 3.7-21 11 0 2 2 10.3 6 25 20.7 83.3 67\n 151.7 139 205zm0 0v40h399900v-40z\"/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.011em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.389em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">PX</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7144em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel x-arrow\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.139em;\"><span style=\"top:-3.322em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight x-arrow-pad\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">s</span><span class=\"mord mathnormal mtight\">am</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">p</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7385em;\"><span style=\"top:-2.931em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span></span></span></span></span></span></span><span class=\"svg-align\" style=\"top:-2.689em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"hide-tail\" style=\"height:0.522em;min-width:1.469em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"400em\" height=\"0.522em\" viewBox=\"0 0 400000 522\" preserveAspectRatio=\"xMaxYMin slice\"><path d=\"M0 241v40h399891c-47.3 35.3-84 78-110 128\n-16.7 32-27.7 63.7-33 95 0 1.3-.2 2.7-.5 4-.3 1.3-.5 2.3-.5 3 0 7.3 6.7 11 20\n 11 8 0 13.2-.8 15.5-2.5 2.3-1.7 4.2-5.5 5.5-11.5 2-13.3 5.7-27 11-41 14.7-44.7\n 39-84.5 73-119.5s73.7-60.2 119-75.5c6-2 9-5.7 9-11s-3-9-9-11c-45.3-15.3-85\n-40.5-119-75.5s-58.3-74.8-73-119.5c-4.7-14-8.3-27.3-11-40-1.3-6.7-3.2-10.8-5.5\n-12.5-2.3-1.7-7.5-2.5-15.5-2.5-14 0-21 3.7-21 11 0 2 2 10.3 6 25 20.7 83.3 67\n 151.7 139 205zm0 0v40h399900v-40z\"/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.011em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7144em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7144em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p>for all <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> form a jointly monic family. <br>\nFor now it's explained <a href=\"https://arxiv.org/abs/2204.07003\">in section 6 here</a> and <a href=\"https://arxiv.org/abs/2308.00651\">in appendix A.4 here</a>.</p>",
        "id": 521096744,
        "sender_full_name": "Paolo Perrone",
        "timestamp": 1748525982
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275989\">Paolo Perrone</span> <a href=\"#narrow/channel/229199-learning.3A-questions/topic/Category-theoretic.20probability/near/521095928\">said</a>:</p>\n<blockquote>\n<p>So I feel there's a bit of a compromise necessary.</p>\n</blockquote>\n<p>I was getting that sense.</p>",
        "id": 521101879,
        "sender_full_name": "David Corfield",
        "timestamp": 1748527621
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276702\">Tobias Fritz</span> <a href=\"#narrow/stream/229199-learning.3A-questions/topic/Category-theoretic.20probability/near/521079069\">said</a>:</p>\n<blockquote>\n<p>But the deterministic subcategory BorelMeas is indeed not that nice: it only has countable products but not uncountable ones, and it's not cartesian closed. Relatedly, BorelStoch only has countable Kolmogorov products but not uncountable ones.</p>\n</blockquote>\n<p>Just to point it out, this should be solved by taking Baire measurable spaces (i.e. measurable spaces whose sigma-algebra is the Baire sigma-algebra induced by some compact Hausdorff topology). At the moment, we need to dive more deeply in this setting if we want to ensure this is a well-behaved extension of BorelMeas out of the separability condition.</p>",
        "id": 521114683,
        "sender_full_name": "Antonio Lorenzin",
        "timestamp": 1748531585
    },
    {
        "content": "<p>(btw this should solve the problem of products, but not that of cartesian closure!)</p>",
        "id": 521115997,
        "sender_full_name": "Antonio Lorenzin",
        "timestamp": 1748531810
    },
    {
        "content": "<p>Alex Simpson has three toposes in which to do probability theory in:</p>\n<p><a href=\"https://www.youtube.com/watch?v=Y1RkPhwJ0Mo\">https://www.youtube.com/watch?v=Y1RkPhwJ0Mo</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"Y1RkPhwJ0Mo\" href=\"https://www.youtube.com/watch?v=Y1RkPhwJ0Mo\"><img src=\"https://uploads.zulipusercontent.net/200ab7a1c55869aa85379d306cf5fe88ab11d463/68747470733a2f2f692e7974696d672e636f6d2f76692f5931526b5068774a304d6f2f6d7164656661756c742e6a7067\"></a></div>",
        "id": 521132550,
        "sender_full_name": "Madeleine Birchfield",
        "timestamp": 1748537411
    },
    {
        "content": "<p>Yes! But to be precise, Simpson is <em>not exactly</em> forming a topos of (say) measurable spaces and measurable maps, or the \"deterministic morphisms\" of a Markov category. He is defining a topos of <em>sets or spaces of random variables</em>. These are beautifully modeled as particular sheaves on a category of probability spaces (or on a similar site).<br>\nSo it's a different topos altogether. <br>\n(There are tight links between Simpson's approach, the dagger-category approach, and Markov categories. See for example <a href=\"https://arxiv.org/abs/2503.02477\">this recent work by Stein</a>.)</p>",
        "id": 521133483,
        "sender_full_name": "Paolo Perrone",
        "timestamp": 1748537787
    },
    {
        "content": "<p>Ah, this could be useful for me:</p>\n<p><a href=\"/user_uploads/21317/8gxNxFfGYUPWVP4lS0Mkeb9T/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/21317/8gxNxFfGYUPWVP4lS0Mkeb9T/image.png\" title=\"image.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"820x460\" src=\"/user_uploads/thumbnail/21317/8gxNxFfGYUPWVP4lS0Mkeb9T/image.png/840x560.webp\"></a></div><p>Tobias's <a href=\"https://arxiv.org/abs/1908.07021\">A synthetic approach to Markov kernels, conditional independence and theorems on sufficient statistics</a></p>",
        "id": 521146594,
        "sender_full_name": "David Corfield",
        "timestamp": 1748542784
    },
    {
        "content": "<p>I don't remember if I was aware of that at the time of writing the paper, but ideas like that (with stochastic processes in mind) already go back to Lawvere's <a href=\"\">Category of Probabilistic Mappings</a> manuscript! (See the final few pages)</p>",
        "id": 521147202,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1748543002
    },
    {
        "content": "<p>Would you possibly have use for this in your work on the Safeguarded AI programme?</p>",
        "id": 521147537,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1748543118
    },
    {
        "content": "<p>I've been working a bit with Vineet Rajani and Dominic Orchard, and it seems that the former's approach to cost analysis of programs is modelled well by presheaves over an ordered monoid, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>M</mi><mo separator=\"true\">,</mo><mi>S</mi><mi>e</mi><mi>t</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[M, Set]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">t</span><span class=\"mclose\">]</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span> thin monoidal. We were looking for a model for the version for probabilistic programming, so I guess we could think about your construction which would allow a Markov category structure on these presheaves.</p>",
        "id": 521148732,
        "sender_full_name": "David Corfield",
        "timestamp": 1748543540
    },
    {
        "content": "<p>Yep, that should work well as long as your presheaves take values in <em>deterministic</em> morphisms (while the morphisms of presheaves can have non-deterministic components). This determinism is necessary in order to get a Markov category again. This is related to why probability theorists tend to model stochastic processes in terms of filtrations of σ-algebras: the identity map is a measurable determinsitic morphism from a measurable space with a large σ-algebra into the same space with a smaller σ-algebra.</p>",
        "id": 521151188,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1748544505
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276702\">Tobias Fritz</span> <a href=\"#narrow/channel/229199-learning.3A-questions/topic/Category-theoretic.20probability/near/521151188\">said</a>:</p>\n<blockquote>\n<p>...as long as your presheaves take values in <em>deterministic</em> morphisms...</p>\n</blockquote>\n<p>Yes, that's it.</p>",
        "id": 521151523,
        "sender_full_name": "David Corfield",
        "timestamp": 1748544641
    },
    {
        "content": "<p>A couple of questions this morning:</p>\n<p>(1) Do the diagrammatic Markov categories just mentioned always inherit the good qualities of the codomain Markov category (having conditionals, being representable, Kolmogorov products, etc.)?</p>\n<p>(2) I see from the <a href=\"https://ncatlab.org/nlab/show/monads+of+probability%2C+measures%2C+and+valuations#detailed_list\">table</a> of probability monads that in many cases the full Eilenberg–Moore category isn't known . Is there any sense that non-free algebras could have some importance for category-theoretic probability theory?</p>",
        "id": 521250159,
        "sender_full_name": "David Corfield",
        "timestamp": 1748595125
    },
    {
        "content": "<p>For question 2: yes, and it has to do with expectation values. We're working on a paper about it, and it should be out soon. [I know I've been saying that for months, sorry! <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span>]</p>",
        "id": 521250387,
        "sender_full_name": "Paolo Perrone",
        "timestamp": 1748595223
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"671813\">Madeleine Birchfield</span> <a href=\"#narrow/channel/229199-learning.3A-questions/topic/Category-theoretic.20probability/near/521132550\">said</a>:</p>\n<blockquote>\n<p>Alex Simpson has three toposes in which to do probability theory in:</p>\n<p><a href=\"https://www.youtube.com/watch?v=Y1RkPhwJ0Mo\">https://www.youtube.com/watch?v=Y1RkPhwJ0Mo</a></p>\n</blockquote>\n<p>I missed this talk when it happened, I'm glad to have been reintroduced to it. I often complain that \"applications\" of topos theory rarely get further than constructing a single topos (or quasi-topos) to work inside and then leveraging the properties of that topos. There is a bigger picture to be found, and a closer examination of ways of comparing different categories used in categorical probability theory could connect things together much more firmly than the typical 'such and such used a similar but subtly different approach' that one sees in computer science papers all the time.</p>",
        "id": 521262130,
        "sender_full_name": "Morgan Rogers (he/him)",
        "timestamp": 1748599510
    },
    {
        "content": "<p>As for (1), I think that the inheritance of representability and Kolmogorov products is fairly easy to see by doing it objectwise, athough AFAIK it hasn't been written up. Conditionals are much harder though, and I believe that it's still open whether conditionals are inherited e.g. by an arrow category. Since there is no obvious construction of conditionals in an arrow category from conditionals in the original category, it seems likely that it they won't be inherited in general. But it might well be the case in specific categories like the arrow category of BorelStoch.</p>",
        "id": 521262732,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1748599703
    },
    {
        "content": "<p>Concerning non-free Eilenberg-Moore algebras, they're <em>surprisingly</em> irrelevant in the sense that many algebras that many more algebras than one would naively expect turn out to be free. For example, it's quite obvious that idempotents split in the EM category of a (reasonably nice) probability monad. What's nontrivial, and what fails for many other kinds of monads, is that they even split in the Kleisli category! In other words, the splitting in the EM category is through a free algebra. We've shown this for BorelStoch <a href=\"https://arxiv.org/abs/2308.00651\">here</a>.</p>\n<p>Another instance is limits of certain diagrams, like equalizers of group actions. Also in this case the limit quite obviously exists in the EM category, but the fact that it's a free algebra is much less obvious. This is one way to state the ergodic decomposition theorem! (See <a href=\"https://arxiv.org/abs/2207.07353\">A category-theoretic proof of the ergodic decomposition theorem</a> for a slightly weaker version modulo almost sure equality.)</p>",
        "id": 521263650,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1748600036
    },
    {
        "content": "<p>Returning to the diagram category idea from above <a href=\"#narrow/channel/229199-learning.3A-questions/topic/Category-theoretic.20probability/near/521146594\">#learning: questions &gt; Category-theoretic probability @ 💬</a>, I guess one easy way to construct these is the situation where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> arises as the Kleisli category for some monad, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span> on <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>C</mi><mrow><mi>d</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">C_{det}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, and then the diagram category is the Kleisli category for <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span> extended to a monad on <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>D</mi><mo separator=\"true\">,</mo><msub><mi>C</mi><mrow><mi>d</mi><mi>e</mi><mi>t</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[D, C_{det}]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">]</span></span></span></span>.</p>\n<p>Since in the case I mentioned we are dealing with <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo separator=\"true\">,</mo><mi>S</mi><mi>e</mi><mi>t</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[P , Set]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">t</span><span class=\"mclose\">]</span></span></span></span>, where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span></span> is an ordered monoid (capturing cost), there's heaps of structure about,  such as being a doubly closed monoidal category. Has anyone looked to combine such structure with the Markov structure in the diagram category? It seems one can represent expected costs in probabilistic programs.</p>",
        "id": 524835167,
        "sender_full_name": "David Corfield",
        "timestamp": 1750320360
    },
    {
        "content": "<p>In what sense is [P, Set] doubly closed? Are you looking at two different monoidal structures? If so, which ones?</p>",
        "id": 524866390,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1750333076
    },
    {
        "content": "<p>Say, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span></span> is an ordered monoid considered as a thin monoidal category, then <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo separator=\"true\">,</mo><mi>S</mi><mi>e</mi><mi>t</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[P, Set]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">t</span><span class=\"mclose\">]</span></span></span></span> has Day convolution and the cartesian product as the two monoidal products.</p>",
        "id": 524880427,
        "sender_full_name": "David Corfield",
        "timestamp": 1750337915
    },
    {
        "content": "<p>I have done some work on denotational semantics for expected cost analysis: <a href=\"https://dl.acm.org/doi/10.1145/3720424\">https://dl.acm.org/doi/10.1145/3720424</a></p>\n<p>I haven't connected it to the Markov category literature, but what I can say is that the expected cost monad I have defined in Section 3.3 from the linked paper, is commutative (but not affine) in certain examples, so its Kleisli category will be CD.</p>\n<p>I don't know if this relates to your functorial approach, but I hope that it piques your interest :)</p>",
        "id": 524960367,
        "sender_full_name": "Pedro Amorim",
        "timestamp": 1750377938
    },
    {
        "content": "<p>Thanks, Pedro. I've  taken note of your interesting paper. </p>\n<p>This copresheaf model of ours was arrived at with Vineet Rajani and others at Kent by looking to make category-theoretic sense of </p>\n<ul>\n<li><strong>A unifying type-theory for higher-order (amortized) cost analysis</strong> [<a href=\"https://vineetrajani.github.io/Papers/POPL-21/popl21.pdf\">here</a> | <a href=\"https://vineetrajani.github.io/Papers/POPL-21/popl21-appendix.pdf\">technical appendix</a>]<br>\nWith Marco Gaboardi, Deepak Garg and Jan Hoffmann<br>\n<em>In Proceedings of the ACM on Programming Languages (POPL), 2021</em></li>\n</ul>\n<p>and the probabilistic version</p>\n<ul>\n<li><strong>A modal type theory of expected cost in higher-order probabilistic programs</strong> [<a href=\"https://vineetrajani.github.io/Papers/oopsla24.pdf\">preprint</a> |<a href=\"https://dl.acm.org/doi/10.1145/3689725\">here</a>]<br>\nWith Gilles Barthe and Deepak Garg.</li>\n</ul>\n<p>Without any CT being used in the design, it seems very natural to interpret things in terms of families of copresheaves over an ordered cost monoid. Something on this should appear soon.</p>",
        "id": 524987155,
        "sender_full_name": "David Corfield",
        "timestamp": 1750402713
    },
    {
        "content": "<p>Something else that might be relevant to this copresheaf model of yours is this note by <span class=\"user-mention\" data-user-id=\"275989\">@Paolo Perrone</span> and <span class=\"user-mention\" data-user-id=\"276702\">@Tobias Fritz</span></p>\n<p><a href=\"https://arxiv.org/abs/1809.10481\">https://arxiv.org/abs/1809.10481</a></p>\n<p>They show that under certain circumstances you can Kan extend a graded monad into a monad. In particular, you can recover my expected cost monad from a graded cost monad similar to the one Vineet et al. have used in their proababilistic cost paper above.</p>\n<p>I really think that this note should be better known as this Kan extension construction is very clever and insightful!</p>",
        "id": 525078295,
        "sender_full_name": "Pedro Amorim",
        "timestamp": 1750440048
    }
]