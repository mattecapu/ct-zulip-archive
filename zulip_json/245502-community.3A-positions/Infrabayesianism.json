[
    {
        "content": "<p>[Not category theory]<br>\nNeedless to say, AI has a lot of hype around it. However, the research community in the subfield of AI Safety is rather fragmented. As a result, there are a few very advanced mathematical frameworks in the field which are only currently accessible to a small number of people. I heard yesterday that one such program is seeking to remedy this by creating pedagogical resources (such as a textbook, exercises...) for the wider community working or thinking about AI Safety. Here's the <a href=\"https://www.lesswrong.com/posts/83DimRqppcaoyYAsy/job-offering-help-communicate-infrabayesianism\">job advertisement</a>, but it doesn't include a lot of detail about what the topic is, so here's the authors' <a href=\"https://www.alignmentforum.org/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence\">introductory post</a> on the Alignment Forum. To put it briefly, infrabayesianism is an extension of Bayesian decision theory.</p>\n<p>I am not personally invested in this, but I am interested in getting to know the theory better eventually, and so I wanted to spread the word about this job just in case. I also think that categorically-minded people might have enough insight to identify any flaws in the abstractions they use along the way.</p>",
        "id": 276442311,
        "sender_full_name": "Morgan Rogers (he/him)",
        "timestamp": 1648109287
    }
]