<!DOCTYPE html>
<html>
<head>
  	<meta charset="utf-8" />
  	<meta http-equiv="X-UA-Compatible" content="IE=edge" />
  	<meta name="viewport" content="width=device-width, initial-scale=1" />
  
<link rel="stylesheet" href="https://mattecapu.github.io/ct-zulip-archive/style.css" /><title>Guest Lecture 1: Neural network layers as parametric spans · event: Categories for AI · Zulip Chat Archive</title>
</head>
<body>
<header>
<a href="https://mattecapu.github.io/ct-zulip-archive" class="home-link">
        <img class="logo" src="https://zulip-avatars.s3.amazonaws.com/21317/realm/icon.png?version=3" />
        <h1>Category Theory<br/>Zulip Server<br/>Archive</h1>
        </a>
        <p>
        You're reading the public-facing archive of the <a href="https://categorytheory.zulipchat.com/">Category Theory Zulip server</a>.<br/>
        
        To join the server you need an invite. Anybody can get an invite by contacting <a href="https://matteocapucci.wordpress.com">Matteo Capucci</a> at <em>name dot surname at gmail dot com</em>.<br/>
        
        For all things related to this archive refer to the same person.
        </p>
        </header>
        <hr />
    
<h2>Stream: <a href="https://mattecapu.github.io/ct-zulip-archive/stream/347879-event.3A-Categories-for-AI/index.html">event: Categories for AI</a></h2>
<h3>Topic: <a href="https://mattecapu.github.io/ct-zulip-archive/stream/347879-event.3A-Categories-for-AI/topic/Guest.20Lecture.201.3A.20Neural.20network.20layers.20as.20parametric.20spans.html">Guest Lecture 1: Neural network layers as parametric spans</a></h3>

<hr>

<base href="https://categorytheory.zulipchat.com">

<a name="308826053"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/347879-event%3A%20Categories%20for%20AI/topic/Guest%20Lecture%201%3A%20Neural%20network%20layers%20as%20parametric%20spans/near/308826053" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Petar Veličković <a href="https://mattecapu.github.io/ct-zulip-archive/stream/347879-event.3A-Categories-for-AI/topic/Guest.20Lecture.201.3A.20Neural.20network.20layers.20as.20parametric.20spans.html#308826053">(Nov 09 2022 at 16:41)</a>:</h4>
<p>Dear Attendees,<br>
With the core series of five lectures finishing, it is now time to dive into our exciting <strong>guest lectures</strong>!</p>
<p>The first guest lecture will be given by <span class="user-mention" data-user-id="562555">@Pietro Vertechi</span>, titled: <strong>"Neural network layers as parametric spans"</strong></p>
<p>The abstract for Pietro's lecture is as follows:<br>
Properties such as composability and automatic differentiation made artificial neural networks a pervasive tool in applications. Tackling more challenging problems caused neural networks to progressively become more complex and thus difficult to define from a mathematical perspective. In this talk, we will discuss a general definition of linear layer arising from a categorical framework based on the notions of integration theory and parametric spans. This definition generalizes and encompasses classical layers (e.g., dense, convolutional), while guaranteeing existence and computability of the layer's derivatives for backpropagation.</p>
<p>Pietro's guest lecture will take place in the usual slot, next week (Monday 14 November, starting 4PM UK Time). The lecture will be given on Zoom and live-streamed on YouTube, just as before (the Zoom link should be the same as in previous weeks, but we will confirm the details in advance of the lectures).</p>
<p>This guest lecture will help explain key parts of <a href="https://arxiv.org/abs/2208.00809">Neural network layers as parametric spans</a> (Bergomi and Vertechi, SYCO 9).</p>
<p>Lastly, on behalf of the entire organising team of Cats4AI <span aria-label="cat" class="emoji emoji-1f408" role="img" title="cat">:cat:</span> , I'd like to thank you all for actively engaging with the course so far! <span aria-label="blush" class="emoji emoji-1f60a" role="img" title="blush">:blush:</span> <br>
I'm sure I can speak for all five of us when I say that this was such a daunting but extremely valuable experience: for several of us it was the first time presenting these concepts to such a diverse audience, but seeing you all engaging with the content (whether it be on Zulip, Zoom, or otherwise) made it all the more worthwhile! <span aria-label="boom" class="emoji emoji-1f4a5" role="img" title="boom">:boom:</span> <br>
We will be sure to send out a feedback form in the future, to get a better feel on what could have been done better (for future years? :) )</p>



<a name="309006480"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/347879-event%3A%20Categories%20for%20AI/topic/Guest%20Lecture%201%3A%20Neural%20network%20layers%20as%20parametric%20spans/near/309006480" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Pim de Haan <a href="https://mattecapu.github.io/ct-zulip-archive/stream/347879-event.3A-Categories-for-AI/topic/Guest.20Lecture.201.3A.20Neural.20network.20layers.20as.20parametric.20spans.html#309006480">(Nov 10 2022 at 14:58)</a>:</h4>
<p>The public link is <a href="https://uva-live.zoom.us/j/83816139841">https://uva-live.zoom.us/j/83816139841</a> (same as before)<br>
The talk will be live-streamed to <a href="https://youtu.be/83a-MwlDy6s">https://youtu.be/83a-MwlDy6s</a></p>
<div class="youtube-video message_inline_image"><a data-id="83a-MwlDy6s" href="https://youtu.be/83a-MwlDy6s"><img src="https://uploads.zulipusercontent.net/b97a96f01b43512c7f3badd7d0f70ad2fddd3453/68747470733a2f2f692e7974696d672e636f6d2f76692f3833612d4d776c447936732f64656661756c742e6a7067"></a></div>



<a name="309899999"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/347879-event%3A%20Categories%20for%20AI/topic/Guest%20Lecture%201%3A%20Neural%20network%20layers%20as%20parametric%20spans/near/309899999" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Bruno Gavranović <a href="https://mattecapu.github.io/ct-zulip-archive/stream/347879-event.3A-Categories-for-AI/topic/Guest.20Lecture.201.3A.20Neural.20network.20layers.20as.20parametric.20spans.html#309899999">(Nov 14 2022 at 16:31)</a>:</h4>
<p>Thoughts during the lecture: it looks like there's a correspondence between Propositions 1. and 2. in Pietro's talk and the Definition 3.5.2.16.  in the <a href="http://davidjaz.com/Papers/DynamicalBook.pdf">Categorical Systems Theory</a> book.</p>



<a name="309900336"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/347879-event%3A%20Categories%20for%20AI/topic/Guest%20Lecture%201%3A%20Neural%20network%20layers%20as%20parametric%20spans/near/309900336" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Bruno Gavranović <a href="https://mattecapu.github.io/ct-zulip-archive/stream/347879-event.3A-Categories-for-AI/topic/Guest.20Lecture.201.3A.20Neural.20network.20layers.20as.20parametric.20spans.html#309900336">(Nov 14 2022 at 16:33)</a>:</h4>
<p>An in fact, Pietro does say that this can be interpreted as a general lens <img alt=":grothendieck:" class="emoji" src="https://zulip-avatars.s3.amazonaws.com/21317/emoji/images/19128.jpg" title="grothendieck"></p>



<a name="309905091"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/347879-event%3A%20Categories%20for%20AI/topic/Guest%20Lecture%201%3A%20Neural%20network%20layers%20as%20parametric%20spans/near/309905091" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Petar Veličković <a href="https://mattecapu.github.io/ct-zulip-archive/stream/347879-event.3A-Categories-for-AI/topic/Guest.20Lecture.201.3A.20Neural.20network.20layers.20as.20parametric.20spans.html#309905091">(Nov 14 2022 at 16:57)</a>:</h4>
<p>Fantastic talk <span class="user-mention" data-user-id="562555">@Pietro Vertechi</span> (I watched over YouTube as I was in the office :) )</p>



<a name="309905578"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/347879-event%3A%20Categories%20for%20AI/topic/Guest%20Lecture%201%3A%20Neural%20network%20layers%20as%20parametric%20spans/near/309905578" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ieva Cepaite <a href="https://mattecapu.github.io/ct-zulip-archive/stream/347879-event.3A-Categories-for-AI/topic/Guest.20Lecture.201.3A.20Neural.20network.20layers.20as.20parametric.20spans.html#309905578">(Nov 14 2022 at 16:59)</a>:</h4>
<p>Yes! It was very interesting <span class="user-mention" data-user-id="562555">@Pietro Vertechi</span>, I especially enjoyed your animated illustrations - made everything instantly intuitive :)</p>



<a name="309907100"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/347879-event%3A%20Categories%20for%20AI/topic/Guest%20Lecture%201%3A%20Neural%20network%20layers%20as%20parametric%20spans/near/309907100" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Bruno Gavranović <a href="https://mattecapu.github.io/ct-zulip-archive/stream/347879-event.3A-Categories-for-AI/topic/Guest.20Lecture.201.3A.20Neural.20network.20layers.20as.20parametric.20spans.html#309907100">(Nov 14 2022 at 17:05)</a>:</h4>
<p>The idea of permuting the legs of a span to compute the backward pass reminds me of how you'd implement this differentiation in terms of einsum. Turns out the derivative can be implemented by <a href="https://stackoverflow.com/questions/43686534/how-does-tf-einsum-in-tensorflow-calculates-gradients-for-matrix-multiplicatio/47609896#47609896">permuting the indices</a>.</p>



<a name="312199591"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/347879-event%3A%20Categories%20for%20AI/topic/Guest%20Lecture%201%3A%20Neural%20network%20layers%20as%20parametric%20spans/near/312199591" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Pietro Vertechi <a href="https://mattecapu.github.io/ct-zulip-archive/stream/347879-event.3A-Categories-for-AI/topic/Guest.20Lecture.201.3A.20Neural.20network.20layers.20as.20parametric.20spans.html#312199591">(Nov 25 2022 at 14:29)</a>:</h4>
<p><span class="user-mention" data-user-id="276875">@Bruno Gavranovic</span> ,  I'm a bit late to the party, but I realized that this can be a very helpful comparison (thinking of discrete parametric spans as a generalized Einstein summation). Many layers can be implemented that way (well, I'm not sure the second one is  accepted in <code>einsum</code>, but it's a useful notation)</p>
<div class="codehilite" data-code-language="Python"><pre><span></span><code><span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="c1"># dense</span>
<span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span><span class="p">]</span> <span class="c1"># convolutional</span>
</code></pre></div>
<p>The general discrete parametric span is of the form</p>
<div class="codehilite" data-code-language="Python"><pre><span></span><code><span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">s</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">π</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span>
</code></pre></div>
<p>where <code>p</code> lives in some generalize index space $E$.</p>



<footer class="site-footer">

<hr><p>Last updated: Nov 01 2025 at 12:09 UTC</p>
This archive runs on a customization of <a href="https://github.com/zulip/zulip-archive">zulip-archive</a>
</footer>
</body>

</html>