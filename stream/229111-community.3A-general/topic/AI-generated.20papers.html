<!DOCTYPE html>
<html>
<head>
  	<meta charset="utf-8" />
  	<meta http-equiv="X-UA-Compatible" content="IE=edge" />
  	<meta name="viewport" content="width=device-width, initial-scale=1" />
  
<link rel="stylesheet" href="https://mattecapu.github.io/ct-zulip-archive/style.css" /><title>AI-generated papers · community: general · Zulip Chat Archive</title>
</head>
<body>
<header>
<a href="https://mattecapu.github.io/ct-zulip-archive" class="home-link">
        <img class="logo" src="https://zulip-avatars.s3.amazonaws.com/21317/realm/icon.png?version=3" />
        <h1>Category Theory<br/>Zulip Server<br/>Archive</h1>
        </a>
        <p>
        You're reading the public-facing archive of the <a href="https://categorytheory.zulipchat.com/">Category Theory Zulip server</a>.<br/>
        
        To join the server you need an invite. Anybody can get an invite by contacting <a href="https://matteocapucci.wordpress.com">Matteo Capucci</a> at <em>name dot surname at gmail dot com</em>.<br/>
        
        For all things related to this archive refer to the same person.
        </p>
        </header>
        <hr />
    
<h2>Stream: <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/index.html">community: general</a></h2>
<h3>Topic: <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html">AI-generated papers</a></h3>

<hr>

<base href="https://categorytheory.zulipchat.com">

<a name="507901615"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507901615" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507901615">(Mar 25 2025 at 00:33)</a>:</h4>
<p>I hate to do this, but the four arXiv papers by this author, uploaded in the last week, all look wholly AI-generated to me: <a href="https://arxiv.org/search/?query=Reizi&amp;searchtype=author">https://arxiv.org/search/?query=Reizi&amp;searchtype=author</a></p>



<a name="507901831"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507901831" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507901831">(Mar 25 2025 at 00:35)</a>:</h4>
<p>In particular the appendix of <a href="https://arxiv.org/abs/2503.16555">https://arxiv.org/abs/2503.16555</a> there is a promise of proofs, examples and so on, but the entire text of the appendix is this:</p>
<blockquote>
<p>In this appendix, we present additional proofs, detailed calculations, and further examples<br>
that complement the results in the main text. In particular, the appendix includes:  <br>
* A complete proof of the back-and-forth construction used in Lemma 5.8.  <br>
* Detailed verifications of the functoriality of the Henkin and compactness-based model constructions.<br>
* Concrete examples illustrating the construction of models for specific theories.  </p>
<p>These supplementary materials are provided to offer deeper insight into the technical details and to demonstrate how our unified framework can be applied to various logical systems.</p>
</blockquote>
<p>The next text is the bibliography and that's it. The content is also extremely banal.</p>



<a name="507940187"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507940187" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Chad Nester <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507940187">(Mar 25 2025 at 06:48)</a>:</h4>
<p>After a cursory inspection of <a href="https://arxiv.org/abs/2503.16570">https://arxiv.org/abs/2503.16570</a>, I agree.</p>



<a name="507946343"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507946343" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507946343">(Mar 25 2025 at 07:33)</a>:</h4>
<p>I can't find any information about this supposed person online except an affiliation via their email, but I've made a report to the Arxiv.</p>



<a name="507956956"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507956956" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507956956">(Mar 25 2025 at 08:33)</a>:</h4>
<p><a href="/user_uploads/21317/2XH9nNv7Uy7EppX4uo7NT3lQ/image.png">image.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/2XH9nNv7Uy7EppX4uo7NT3lQ/image.png" title="image.png"><img data-original-content-type="image/png" data-original-dimensions="991x806" src="/user_uploads/thumbnail/21317/2XH9nNv7Uy7EppX4uo7NT3lQ/image.png/840x560.webp"></a></div><p>yep, no way a human wrote this</p>



<a name="507966779"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507966779" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507966779">(Mar 25 2025 at 09:17)</a>:</h4>
<p>Stupid LLM forgetting the syntax for bold in TeX and falling back on Markdown...</p>



<a name="508059186"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508059186" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508059186">(Mar 25 2025 at 15:39)</a>:</h4>
<p>I'm proud to say I called bullshit from the titles alone in my feed lol glad I wasn't wrong</p>



<a name="508067115"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508067115" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508067115">(Mar 25 2025 at 16:10)</a>:</h4>
<p>Heh, we did an experiment on LLMs that produce SQL code, and for many of them, no matter how much you tell them not to format the output, they still do it. Stripping extra comments and markdown/html out of responses turned out to be the hardest part of interacting with the LLM in an automated flow.</p>



<a name="508136651"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508136651" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Joe Moeller <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508136651">(Mar 25 2025 at 22:06)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275932">Matteo Capucci (he/him)</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/508059186">said</a>:</p>
<blockquote>
<p>I'm proud to say I called bullshit from the titles alone in my feed lol glad I wasn't wrong</p>
</blockquote>
<p>Right, natural transformations between theorems.</p>



<a name="508136715"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508136715" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Joe Moeller <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508136715">(Mar 25 2025 at 22:07)</a>:</h4>
<p>I noticed there are two orders of the names used. Two of the papers are JRB, and two are BJR. What could be the point of that?</p>



<a name="508152007"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508152007" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508152007">(Mar 26 2025 at 00:27)</a>:</h4>
<p>The email address seems to be attached to Open University Japan, so name-order may have been auto-generated differently for the different papers?</p>



<a name="508787020"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508787020" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Noah Chrein <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508787020">(Mar 28 2025 at 15:44)</a>:</h4>
<p><span class="user-mention silent" data-user-id="282822">fosco</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/507956956">said</a>:</p>
<blockquote>
<p>yep, no way a human wrote this</p>
</blockquote>
<p>To be fair, I have seen researchers who <em>just</em> learned about category theory writing this way. </p>
<p>Anyway, the AI-generated slop CT papers are coming. I've noticed that <a href="http://chat.qwen.ai">Qwen</a> 2.5 is trained on a lot of higher/formal category theory. It's fun to play with and it can produce approximately accurate references to results, which can sometimes cut down on search time. It's not yet good enough to generate any meaningfully creative results, and is not enough to fool a half-keen eye, but I can imagine an undergrad using qwen to write a undergrad thesis that nobody reads.</p>



<a name="508788675"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508788675" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ivan Di Liberti <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508788675">(Mar 28 2025 at 15:51)</a>:</h4>
<p><span class="user-mention silent" data-user-id="277976">Noah Chrein</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/508787020">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="282822">fosco</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/507956956">said</a>:</p>
<blockquote>
<p>yep, no way a human wrote this</p>
</blockquote>
<p>To be fair, I have seen researchers who <em>just</em> learned about category theory writing this way. </p>
<p>Anyway, the AI-generated slop CT papers are coming. I've noticed that <a href="http://chat.qwen.ai">Qwen</a> 2.5 is trained on a lot of higher/formal category theory. It's fun to play with and it can produce approximately accurate references to results, which can sometimes cut down on search time. It's not yet good enough to generate any meaningfully creative results, and is not enough to fool a half-keen eye, but I can imagine an undergrad using qwen to write a undergrad thesis that nobody reads.</p>
</blockquote>
<p>What is Qwen and how happen it was trained on so much category theory?</p>



<a name="508792542"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508792542" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508792542">(Mar 28 2025 at 16:07)</a>:</h4>
<p>Qwen appears to be Alibaba's language model. I hadn't heard of it till now.</p>



<a name="508792840"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508792840" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Noah Chrein <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508792840">(Mar 28 2025 at 16:08)</a>:</h4>
<p>Perhaps the Chinese understand the importance of category theory to mathematics and hence to generalized cognition</p>



<a name="521179894"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521179894" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521179894">(May 29 2025 at 22:18)</a>:</h4>
<p>There’s an interesting fake paper on the ArXiv today. I can’t really tell if it’s AI crankery or just the old fashioned kind. Did anybody glance at it? <a href="https://arxiv.org/abs/2505.22558">https://arxiv.org/abs/2505.22558</a></p>



<a name="521180392"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521180392" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Cole Comfort <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521180392">(May 29 2025 at 22:22)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521179894">said</a>:</p>
<blockquote>
<p>There’s an interesting fake paper on the ArXiv today. I can’t really tell if it’s AI crankery or just the old fashioned kind. Did anybody glance at it? <a href="https://arxiv.org/abs/2505.22558">https://arxiv.org/abs/2505.22558</a></p>
</blockquote>
<p>The excessive use of lists suggests AI</p>



<a name="521182914"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521182914" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521182914">(May 29 2025 at 22:47)</a>:</h4>
<p>Right, that makes sense. It was harder to find obvious local absurdities than in papers further up this thread, which is disappointing.</p>



<a name="521187274"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521187274" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521187274">(May 29 2025 at 23:32)</a>:</h4>
<p>There's a whole bunch recently that I have been <del>complaining about</del> pointing out elsewhere. The author is uploading a new paper every couple of days, and the title names something after himself. I'm happy to see today that they've been moved to math.GM ! (as I suggested)</p>
<p><a href="https://export.arxiv.org/find/math/1/au:+Alpay_F/0/1/0/all/0/1">https://export.arxiv.org/find/math/1/au:+Alpay_F/0/1/0/all/0/1</a></p>



<a name="521187635"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521187635" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521187635">(May 29 2025 at 23:35)</a>:</h4>
<p>And in the case at the top of the thread, namely <a href="https://arxiv.org/search/?query=Reizi&amp;searchtype=author">https://arxiv.org/search/?query=Reizi&amp;searchtype=author</a> all these are also math.GM classified now, not math.CT.</p>



<a name="521189500"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521189500" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521189500">(May 29 2025 at 23:55)</a>:</h4>
<p>Seems like in theory the arXiv "endorsement system" should deal with AI generated papers just like any other spam, but I guess it doesn't work in practice?    <a href="https://info.arxiv.org/help/endorsement.html">https://info.arxiv.org/help/endorsement.html</a></p>



<a name="521194635"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521194635" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521194635">(May 30 2025 at 00:35)</a>:</h4>
<p>Yes, I'm a bit confused how all these people are getting endorsements.</p>



<a name="521194732"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521194732" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521194732">(May 30 2025 at 00:36)</a>:</h4>
<p>At the very least it should be possible to "un-endorse" them after they've demonstrated their crankiness.</p>



<a name="521215571"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521215571" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521215571">(May 30 2025 at 04:09)</a>:</h4>
<p>Another one! <a href="https://arxiv.org/abs/2505.22931">https://arxiv.org/abs/2505.22931</a></p>



<a name="521216374"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521216374" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521216374">(May 30 2025 at 04:18)</a>:</h4>
<p>Maybe the arXiv needs to appoint a category theorist to the team of moderators...</p>



<a name="521233951"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521233951" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521233951">(May 30 2025 at 07:02)</a>:</h4>
<p>I thought arXiv had a strong stance against crackpottery, so why are these papers allowed to remain under math.GM, rather than being removed entirely?</p>



<a name="521235022"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521235022" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521235022">(May 30 2025 at 07:10)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521182914">said</a>:</p>
<blockquote>
<p>Right, that makes sense. It was harder to find obvious local absurdities than in papers further up this thread, which is disappointing.</p>
</blockquote>
<p>The phrase "discrete conformal field theory" in the abstract made me raise my eyebrows.  As if that were a known thing.  Given how much people try everything, there probably <em>is</em> <strong>some</strong> work on something called discrete conformal field theory, but....</p>
<p>Yeah, there's a paper <a href="https://link.springer.com/article/10.1007/s00220-022-04475-x.">Conformal Field Theory at the Lattice Level: Discrete Complex Analysis and Virasoro Structure</a> trying to understand how conformal field theory is related to field theory on a lattice.  But most conformal transformations don't map a lattice to itself, so this is bound to be rough, and the idea that "Recursive Difference Categories and Topos-Theoretic Universality" would have something to say about it is, umm, questionable.</p>



<a name="521236395"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521236395" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521236395">(May 30 2025 at 07:21)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276092">Nathanael Arkor</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521233951">said</a>:</p>
<blockquote>
<p>I thought arXiv had a strong stance against crackpottery, so why are these papers allowed to remain under math.GM, rather than being removed entirely?</p>
</blockquote>
<p>It can be hard to tell whether a math paper is crazy, and people whose papers are rejected entirely complain a lot, so it seems the arXiv folks find it convenient to put borderline papers into math.GM,  expecting people 'in the know' to beware of such papers.  That's my impression anyway.</p>



<a name="521236804"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521236804" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521236804">(May 30 2025 at 07:24)</a>:</h4>
<p>It's more diplomatic than having math.CP for crackpot math.</p>



<a name="521244418"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521244418" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521244418">(May 30 2025 at 08:16)</a>:</h4>
<p>This is a truly beautiful era to witness first-hand.</p>



<a name="521246836"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521246836" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Areeb SM <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521246836">(May 30 2025 at 08:31)</a>:</h4>
<p>ViXra appears to be <a href="https://ai.vixra.org/">embracing</a> the future...</p>



<a name="521248709"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521248709" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521248709">(May 30 2025 at 08:42)</a>:</h4>
<p>But not unreservedly:</p>
<blockquote>
<p><a href="http://viXra.org">viXra.org</a> only accept scholarly articles written without AI assistance. Please go to <a href="http://ai.viXra.org">ai.viXra.org</a> to submit new scholarly article written with AI assistance.</p>
</blockquote>



<a name="521385995"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521385995" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Joe Moeller <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521385995">(May 30 2025 at 23:38)</a>:</h4>
<p>arxiv could use the exact same disclaimer, only changing the first instance of "vixra".</p>



<a name="521423955"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521423955" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521423955">(May 31 2025 at 08:49)</a>:</h4>
<p><a href="http://ai.viXra.org">ai.viXra.org</a> sounds like a fascinating crackpot sociology experiment.  They have 343 papers so far.   Within the subject of physics, most of the papers are on "relativity and cosmology", so we can guess that part of physics attracts crackpots the most.     Within mathematics, 75% of the papers are on number theory.</p>



<a name="521424158"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521424158" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521424158">(May 31 2025 at 08:52)</a>:</h4>
<p>Yesterday's first submitted paper on general relativity and cosmology:</p>
<p><strong>The Pi-Periodic 22/7ths Dimension: A Quantum Gravity Framework for Dark Energy</strong></p>
<blockquote>
<p>We propose a novel 4+1-dimensional quantum gravity framework incorporating a compactified extra dimension, τ , with a periodicity of π (to 22 decimal places), symbolically tied to the rational approximation 22/7.</p>
</blockquote>
<p>Someone is taking this 22/7 stuff very seriously!  I believe Archimedes came up with this approximation to pi, and it was good enough that by the Middle Ages a bunch of mathematicians believed <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">\pi = </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span></span></span></span> 22/7.</p>



<a name="521425529"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521425529" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521425529">(May 31 2025 at 09:13)</a>:</h4>
<blockquote>
<p>by the Middle Ages a bunch of mathematicians believed <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">\pi =</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span></span></span></span>  22/7.</p>
</blockquote>
<p><span aria-label="surprise" class="emoji emoji-1f62e" role="img" title="surprise">:surprise:</span>  Wait, is it not? /s</p>



<a name="521426011"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521426011" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> James Deikun <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521426011">(May 31 2025 at 09:21)</a>:</h4>
<p>Archimedes squared the circle with this ONE WEIRD TRICK!  Geometers hate him!</p>



<a name="521426330"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521426330" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521426330">(May 31 2025 at 09:26)</a>:</h4>
<p>Actually I learned this when reading about the mathematician <a href="https://johncarlosbaez.wordpress.com/2025/04/24/civilizational-collapse-part-5/">Franco of Liège</a>.  In 1020 he got interested in the ancient Greek problem of squaring the circle. But since he believed that pi is 22/7, he started studying the square root of 22/7.  I don't know if he figured out how to construct the square root of 22/7 with straightedge and compass.  But he did manage to prove that the square root of 22/7 is irrational!</p>
<p>Now, this is better than it sounds, because I believe the old Greek proof that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mn>2</mn></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1328em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9072em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord">2</span></span></span><span style="top:-2.8672em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1328em;"><span></span></span></span></span></span></span></span></span> is irrational had been lost in western Europe at this time.   So it took some serious ingenuity.  </p>
<p>Still, it's a sad reflection on the sorry state of mathematical knowledge in western Europe from around 500 AD to 1000 AD.   It was better elsewhere at that time.   I find this local collapse of civilization, and how people recovered, quite fascinating.</p>



<a name="521426636"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521426636" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521426636">(May 31 2025 at 09:30)</a>:</h4>
<p>Could AI slop prompt some loss of collective intelligence now?</p>



<a name="521429490"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521429490" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Fabrizio Romano Genovese <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521429490">(May 31 2025 at 10:15)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521426636">said</a>:</p>
<blockquote>
<p>Could AI slop prompt some loss of collective intelligence now?</p>
</blockquote>
<p>In general any tool that helps you thinking makes you sloppier in some respect. So yes. For instance, ancient languages are often way more complicated grammatically than new languages. One reason for this is that being able to say "Go around the mammoth, without being heard, by exactly half of a circle" in fewer words may have been a big advantage when we were hunter-gatherers, so languages tended to be more expressive. With civilization, inception of written support etc we lost the need to formulate such complicated statements in a compact way, languages became less expressive, and we probably lost some of our cognitive ability in the process as well. It's always a tradeoff.</p>



<a name="521434376"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521434376" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521434376">(May 31 2025 at 11:30)</a>:</h4>
<p>I'm thinking more about how successive generations of Roman summaries of Greek scientific texts watered them down to a homeopathic dilution of their original strength.  Then many of the originals were lost, at least in western Europe.</p>



<a name="521434597"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521434597" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521434597">(May 31 2025 at 11:32)</a>:</h4>
<p><span class="user-mention" data-user-id="679887">@Fabrizio Romano Genovese</span>: could you share a reference for the claim that older languages have higher entropy than modern languages?</p>



<a name="521525145"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521525145" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Notification Bot <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521525145">(Jun 01 2025 at 08:05)</a>:</h4>
<p>13 messages were moved from this topic to <a class="stream-topic" data-stream-id="229451" href="/#narrow/channel/229451-meta.3A-off-topic/topic/language.3A.20the.20rise.20and.20fall.20of.20complex.20grammars/with/521434597">#meta: off-topic &gt; language: the rise and fall of complex grammars</a> by <span class="user-mention silent" data-user-id="275920">John Baez</span>.</p>



<a name="521805518"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521805518" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521805518">(Jun 02 2025 at 17:32)</a>:</h4>
<p><span class="user-mention silent" data-user-id="679887">Fabrizio Romano Genovese</span> <a href="#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/521429490">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521426636">said</a>:</p>
<blockquote>
<p>Could AI slop prompt some loss of collective intelligence now?</p>
</blockquote>
<p>In general any tool that helps you thinking makes you sloppier in some respect. So yes. For instance, ancient languages are often way more complicated grammatically than new languages. One reason for this is that being able to say "Go around the mammoth, without being heard, by exactly half of a circle" in fewer words may have been a big advantage when we were hunter-gatherers, so languages tended to be more expressive. With civilization, inception of written support etc we lost the need to formulate such complicated statements in a compact way, languages became less expressive, and we probably lost some of our cognitive ability in the process as well. It's always a tradeoff.</p>
</blockquote>
<p>uuuhmm what's a reference for this? smells really funny to me...</p>



<a name="521829583"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521829583" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521829583">(Jun 02 2025 at 19:44)</a>:</h4>
<p><a href="#narrow/channel/229451-meta.3A-off-topic/topic/language.3A.20the.20rise.20and.20fall.20of.20complex.20grammars/with/521434597">https://categorytheory.zulipchat.com/#narrow/channel/229451-meta.3A-off-topic/topic/language.3A.20the.20rise.20and.20fall.20of.20complex.20grammars/with/521434597</a></p>



<a name="523707907"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523707907" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523707907">(Jun 12 2025 at 08:53)</a>:</h4>
<p>New AI paper up:</p>
<ol>
<li><a href="https://arxiv.org/abs/2506.06885">https://arxiv.org/abs/2506.06885</a></li>
</ol>



<a name="523708003"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523708003" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523708003">(Jun 12 2025 at 08:54)</a>:</h4>
<p>This one is funny because it outs itself<br>
<a href="/user_uploads/21317/g4CqaUqHyjbQrd8X6oR_oXBq/image.png">image.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/g4CqaUqHyjbQrd8X6oR_oXBq/image.png" title="image.png"><img data-original-content-type="image/png" data-original-dimensions="818x398" src="/user_uploads/thumbnail/21317/g4CqaUqHyjbQrd8X6oR_oXBq/image.png/840x560.webp"></a></div>



<a name="523708584"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523708584" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523708584">(Jun 12 2025 at 08:57)</a>:</h4>
<p>I actually approve of this way of approaching AI tools: personally, I don't think they automatically disqualify a paper. The principle should be the the author is ultimately responsible to check their results, and remains fully accountable.</p>



<a name="523708600"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523708600" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Patrick Nicodemus <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523708600">(Jun 12 2025 at 08:57)</a>:</h4>
<blockquote>
<p>Our results offer a formal justification for this procedure, suggesting that the analytic<br>
continuation is not arbitrary but is in fact forced by the underlying principles of symmetry<br>
and normalization.</p>
</blockquote>
<p>Kind of a funny quote because the analytic continuation of a function is one of the most rigidly determined and least arbitrary constructions in mathematics</p>



<a name="523708805"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523708805" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523708805">(Jun 12 2025 at 08:58)</a>:</h4>
<p>Indeed, the paper is (from a quick skim) likely formally correct but basically insubtantials, it's a big cargo-cult regurgitation. The whole thing seems circular.</p>



<a name="523709393"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523709393" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Patrick Nicodemus <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523709393">(Jun 12 2025 at 09:00)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275932">Matteo Capucci (he/him)</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523708584">said</a>:</p>
<blockquote>
<p>I actually approve of this way of approaching AI tools: personally, I don't think they automatically disqualify a paper. The principle should be the the author is ultimately responsible to check their results, and remains fully accountable.</p>
</blockquote>
<p>Yeah, I think I agree. At the very least we might have to get used to seeing that writing style everywhere, I can imagine a non-native speaker feeling a lot of pressure to use it to make their wording seem natural. It doesn't inherently disqualify the paper. But, on the other hand, it makes me suspicious and vigilant of errors, and at that point even a small error would be enough to cause me to discard it.</p>



<a name="523712713"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523712713" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523712713">(Jun 12 2025 at 09:18)</a>:</h4>
<p>I suspect at some point someone will try to mass-produce papers and submit them everywhere (it's almost trivial to compile a list of journal inside the math-cs area; let the machine prepare a different paper for each item of the list; let the machine submit, let the machine handle the rebuttals and modify the paper accordingly, resubmit...), relying on small probability of success after a high number of trial. </p>
<p>It's the academia equivalent of asking out 100 girls, one of them will say yes.</p>
<p>These are very interesting times to witness. Especially if you're an irredeemable nihilist.</p>



<a name="523740601"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523740601" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523740601">(Jun 12 2025 at 11:56)</a>:</h4>
<p>I think there's a real risk of the image of CT being tarnished if this type of stuff becomes too common. The number theory people know how to funnel cranks away from their arXiv category, if category theorists can't do this, it's not a good look.</p>



<a name="523741056"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523741056" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523741056">(Jun 12 2025 at 11:59)</a>:</h4>
<p>Also, mathematicians generally are conscious of the circle squarers and the number theory cranks and so on, and can spot this stuff pretty easily, because it's on a hot-button topic and shows the usual obvious signs. But something in category theory applied to other areas (not Applied Category Theory, but to an outsider it's not necessarily obvious) that plays to the stereotypes of CT's abstract nonsense moniker just looks like another silly CT paper that claims to revolutionise our understanding of a piece of classical mathematics when really it's empty of real content.</p>



<a name="523742554"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523742554" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523742554">(Jun 12 2025 at 12:08)</a>:</h4>
<p>Perhaps not among hardcore mathematicians, who would almost surely recognise the problem and commiserate, but anyone merely adjacent, for instance someone with money who might be needed to be convinced to fund some real and good ACT may get wind of this AI nonsense. </p>
<p>Maybe I'm being too pessimistic here. But these are ideas that occur to me</p>



<a name="523756995"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523756995" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523756995">(Jun 12 2025 at 13:26)</a>:</h4>
<blockquote>
<p>The number theory people know how to funnel cranks away from their arXiv category</p>
</blockquote>
<p>How?</p>



<a name="523757655"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523757655" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523757655">(Jun 12 2025 at 13:29)</a>:</h4>
<p>Well, I get math.NT daily announcements and I've never seen a crank number theory paper, and yet I know they do turn up in math.GM.</p>



<a name="523757689"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523757689" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523757689">(Jun 12 2025 at 13:30)</a>:</h4>
<p>So somehow they manage it.</p>



<a name="523782577"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523782577" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523782577">(Jun 12 2025 at 15:29)</a>:</h4>
<p>Have we taken any action about these papers?  Contacted anyone at arXiv about removing them and un-endorsing the submitters?  That seems to me to be the obvious first step.  I'd be willing to help if needed, although I don't have the time to filter the daily submissions for them myself.</p>



<a name="523783104"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523783104" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523783104">(Jun 12 2025 at 15:32)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276422">David Michael Roberts</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523741056">said</a>:</p>
<blockquote>
<p>But something in category theory applied to other areas (not Applied Category Theory, but to an outsider it's not necessarily obvious) that plays to the stereotypes of CT's abstract nonsense moniker just looks like another silly CT paper that claims to revolutionise our understanding of a piece of classical mathematics when really it's empty of real content.</p>
</blockquote>
<p>I wonder if an effect like this could be what's causing the problem by making it easier for cranks to get endorsed with CT papers by lazy non-category-theorists.</p>



<a name="523786721"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523786721" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523786721">(Jun 12 2025 at 15:51)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276422">David Michael Roberts</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523757655">said</a>:</p>
<blockquote>
<p>Well, I get math.NT daily announcements and I've never seen a crank number theory paper, and yet I know they do turn up in math.GM.</p>
</blockquote>
<p>To me, that just suggests that the arXiv editors are better at detecting crank NT papers than crank CT papers, likely because they have had more practice at it.</p>



<a name="523792785"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523792785" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523792785">(Jun 12 2025 at 16:26)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276777">Mike Shulman</span> <a href="#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/523782577">said</a>:</p>
<blockquote>
<p>Have we taken any action about these papers?  Contacted anyone at arXiv about removing them and un-endorsing the submitters?  That seems to me to be the obvious first step.  I'd be willing to help if needed, although I don't have the time to filter the daily submissions for them myself.</p>
</blockquote>
<p>I’ve contacted the ArXiv about the first batch of these that came up. They said they’d look into it but don’t share results of investigations. I haven’t checked whether the papers are down. It feels like fingers in a dike if we can’t figure out who is endorsing these authors though!</p>



<a name="523793705"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523793705" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523793705">(Jun 12 2025 at 16:32)</a>:</h4>
<p>Did your first batch include <a href="https://arxiv.org/abs/2505.22931">Recursive Difference Categories and Topos-Theoretic Universality</a> by <a href="https://arxiv.org/search/math?searchtype=author&amp;query=Santacana,+A+B">Andreu Ballus Santacana</a>?  That was a crank paper discussed here earlier.   It's still up!   Santacana is also responsible for the new one you folks are talking about today, <a href="https://arxiv.org/abs/2506.06885">Analytic Uniqueness of Ball Volume Interpolation: Categorical Invariance and Universal Characterization</a>.</p>



<a name="523793881"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523793881" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523793881">(Jun 12 2025 at 16:33)</a>:</h4>
<p>I checked earlier, and Santacana appears to be in the <a href="https://portalrecerca.uab.cat/en/persons/andreu-ballus-santacana">department of philosophy of UAB Barcelona</a>.</p>
<p>(He's definitely got the Grothendieck bald-head thing going on.)</p>



<a name="523801760"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523801760" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523801760">(Jun 12 2025 at 17:26)</a>:</h4>
<p>I reported the papers of Barreto that David Roberts opened this thread with. Unfortunately they're still up and there have been two more since then. They're all in GM now, though, which I guess is the best it seems we can generally hope for.</p>



<a name="523847620"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523847620" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523847620">(Jun 12 2025 at 23:27)</a>:</h4>
<p>The moving to math.GM has been patchy. Some of the ones by one author whose primary listing is CS.lo haven't moved, while those that were listed under math.CT have. Presumably because computer scientists are even less well-equipped than a generic mathematician to judge what CT is actually AI-generated crank material.</p>



<a name="523848772"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523848772" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523848772">(Jun 12 2025 at 23:44)</a>:</h4>
<p>If the people submitting these things are actually employed by reputable institutions, perhaps we should contact their employers.</p>



<a name="523860779"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523860779" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523860779">(Jun 13 2025 at 02:25)</a>:</h4>
<p>The first person I reported is unlocatable online, IIRC. But that’s apparently not the case for everyone.</p>



<a name="523916691"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523916691" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523916691">(Jun 13 2025 at 10:29)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276422">David Michael Roberts</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523740601">said</a>:</p>
<blockquote>
<p>I think there's a real risk of the image of CT being tarnished if this type of stuff becomes too common. The number theory people know how to funnel cranks away from their arXiv category, if category theorists can't do this, it's not a good look.</p>
</blockquote>
<p>Every week, <em>one or two</em> of these papers make it into the <code>math.LO</code>/<code>cs.LO</code> announcements, which is frankly ridiculous.  We had a person who just had a couple of their articles GM-holed last week get through to <code>cs.LO</code> <em>again</em> this week. Especially disappointing since at the same time, I know multiple people with solid academic affiliations, long records in logic, and academic email addresses who've seen their announcements blocked/delayed while they appealed (e.g. conference extended abstracts misclassified and rejected as "abstract-only submissions", or a PhD thesis randomly rejected) :/</p>
<p>I don't think this tarnishes the image of logic itself, but it's certainly a big source of noise and not a good look for arXiv moderation.</p>



<a name="523927191"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523927191" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523927191">(Jun 13 2025 at 11:33)</a>:</h4>
<p>A possible solution is to set up a small website that collects these papers and flag them as "probably bollocks". A small number of us, committed to express a judgment evaluate these submissions pointing out "this passage is AI generated" "the second sentence at page 2 doesn't make any sense" etc</p>
<p>It takes a lot of work, but we all know what's the rule here:<br>
<a href="/user_uploads/21317/0PygcTIJuTOeZlzGYmIK5wIM/image.png">image.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/0PygcTIJuTOeZlzGYmIK5wIM/image.png" title="image.png"><img data-original-content-type="image/png" data-original-dimensions="640x600" src="/user_uploads/thumbnail/21317/0PygcTIJuTOeZlzGYmIK5wIM/image.png/840x560.webp"></a></div><p>I agree that this state of affairs tarnishes the reputation of category theory/ists and I think there is only one way to nip the problem in the bud, that is taking responsibility and vehemently assert that "yeah, no, we do not recognize this shit as category theory or even as decent math"</p>



<a name="523930411"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523930411" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Chad Nester <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523930411">(Jun 13 2025 at 11:53)</a>:</h4>
<p>I'm not sure this kind of "negative curation", in which we maintain lists of things that are <em>bad</em>, is the way to go. </p>
<p>In an ideal world the function of journals is to be lists of things that are <em>good</em>, or at least probably not bad.</p>



<a name="523933583"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523933583" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523933583">(Jun 13 2025 at 12:13)</a>:</h4>
<p>One option is to make sure all these dodgy AI-generated papers have comments on PubPeer. See eg <a href="https://www.pubpeer.com/publications/D52D1CC22593701472A83CFB9C2FD8">https://www.pubpeer.com/publications/D52D1CC22593701472A83CFB9C2FD8</a> If the obvious red flags are documented here, then a list of links can be curated in a place category theorists have control over, or sent to arXiv admins, or employers of people making this nonsense.</p>



<a name="523951140"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523951140" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523951140">(Jun 13 2025 at 13:52)</a>:</h4>
<p><span class="user-mention silent" data-user-id="690871">Chad Nester</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523930411">said</a>:</p>
<blockquote>
<p>I'm not sure this kind of "negative curation", in which we maintain lists of things that are <em>bad</em>, is the way to go. <br>
</p>
</blockquote>
<p>History disagrees <a href="https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum">https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum</a> lists of things that are bad can be used to repress heresy.</p>



<a name="523951855"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523951855" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523951855">(Jun 13 2025 at 13:56)</a>:</h4>
<p><span class="user-mention silent" data-user-id="282822">fosco</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523951140">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="690871">Chad Nester</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523930411">said</a>:</p>
<blockquote>
<p>I'm not sure this kind of "negative curation", in which we maintain lists of things that are <em>bad</em>, is the way to go. <br>
</p>
</blockquote>
<p>History disagrees <a href="https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum">https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum</a> lists of things that are bad can be used to repress heresy.</p>
</blockquote>
<p><em>Index Paperorum Crackpoti</em></p>



<a name="524110845"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/524110845" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Chad Nester <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#524110845">(Jun 15 2025 at 07:18)</a>:</h4>
<p><span class="user-mention silent" data-user-id="282822">fosco</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523951140">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="690871">Chad Nester</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523930411">said</a>:</p>
<blockquote>
<p>I'm not sure this kind of "negative curation", in which we maintain lists of things that are <em>bad</em>, is the way to go. <br>
</p>
</blockquote>
<p>History disagrees <a href="https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum">https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum</a> lists of things that are bad can be used to repress heresy.</p>
</blockquote>
<p>An inquisition would, at least, be entertaining :)</p>



<a name="524653576"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/524653576" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#524653576">(Jun 18 2025 at 08:23)</a>:</h4>
<p>A <a href="https://arxiv.org/abs/2506.14537">new paper</a> in <a href="http://quant.ph">quant.ph</a>  supposedly connecting modular tensor categories to quantum contextuality smells like LLM to me.</p>



<a name="524660144"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/524660144" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#524660144">(Jun 18 2025 at 09:02)</a>:</h4>
<p>Pages 7 and 8 definitely look like over-optimistic generalities of dot points. And the 'proof' here is altogether lacking in convincing detail in the last two sentences....</p>
<blockquote>
<p><strong>Proposition 4.3.</strong> The braid group representation <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ρ</mi><mi>F</mi></msub></mrow><annotation encoding="application/x-tex">\rho_F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">ρ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> derived from the Fibonacci category violates the KCBS inequality maximally, demonstrating strong contextuality intrinsic to its topological structure.  <br>
<strong>Proof.</strong> Projectors onto fusion basis states corresponding to the object <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span></span></span></span> generate a measurement scenario isomorphic to the pentagon graph underlying the KCBS inequality [9, 11]. The noncommuting braid generators create measurement contexts whose statistical correlations surpass classical bounds. Numerical evaluation of expectation values using explicit ρF matrices confirms maximal violation</p>
</blockquote>



<a name="524661319"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/524661319" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#524661319">(Jun 18 2025 at 09:08)</a>:</h4>
<p>It's just verbiage, with the convenient out that "numerical evaluation" will bear out the claim.    In other words, bullshit.</p>



<a name="526313843"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526313843" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526313843">(Jun 30 2025 at 03:26)</a>:</h4>
<p>Another AI-slop paper from Reizi:</p>
<p><a href="https://arxiv.org/abs/2506.21653">https://arxiv.org/abs/2506.21653</a></p>
<p>Primary subject this time math.LO not math.CT. Also, name changed from Barreto Joaquim Reiz to Higuchi Joaquim Reizi. The formatting is also weirdly broken, with line numbers appearing inconsistently...</p>



<a name="526313964"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526313964" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526313964">(Jun 30 2025 at 03:29)</a>:</h4>
<p>Identical submission email, this person needs to be put on a special watch-list.</p>



<a name="526318854"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526318854" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526318854">(Jun 30 2025 at 04:43)</a>:</h4>
<p>Have you considered emailing folks at the arXiv, where these suggestions can have some effect?</p>



<a name="526319139"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526319139" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526319139">(Jun 30 2025 at 04:47)</a>:</h4>
<p>I'm working on that, too, through a more senior person in the logic community, behind the scenes. I'm just cataloguing them here for the benefit of people who might see it and waste the time looking at it (though this one is pretty blatant). I can stop if it's too much noise.</p>



<a name="526319398"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526319398" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526319398">(Jun 30 2025 at 04:51)</a>:</h4>
<p>It's not too much noise; people can always mute this thread if they want.  I'm just glad you're trying to actually do something about this.</p>



<a name="526328702"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526328702" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526328702">(Jun 30 2025 at 06:38)</a>:</h4>
<p>Someone pointed me at the arXiv moderation contact form, I put my case to them.</p>



<a name="526331329"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526331329" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526331329">(Jun 30 2025 at 07:00)</a>:</h4>
<p>Thanks, yes that's an easy way to contact the moderators.  Since you don't sound like a crackpot, they should take you seriously, though action may be slow, and almost surely near-silent.</p>



<a name="526331547"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526331547" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526331547">(Jun 30 2025 at 07:02)</a>:</h4>
<p>E.g. I asked them whether they had an international backup of the arXiv, and they never replied, but <a href="https://mathstodon.xyz/@johncarlosbaez/114752342143968527">now they have one</a>.</p>
<p>(I'm not claiming I caused this, but it was an obvious thing to want so I'm glad they have it now.)</p>



<a name="526532165"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526532165" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526532165">(Jul 01 2025 at 06:22)</a>:</h4>
<p>Number of days since an AI-generated slop article made it to <code>math.LO</code>: zero. Again.</p>



<a name="526534815"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526534815" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526534815">(Jul 01 2025 at 06:43)</a>:</h4>
<p>Yes. Also a repeat offender. But also it's going to math.FA, and not even cross-listed to math.CT, despite the title and the topic. </p>
<p>I encourage people to use the 'arXiv moderation user support' contact link here: <a href="https://arxiv-org.atlassian.net/servicedesk/customer/portal/2">https://arxiv-org.atlassian.net/servicedesk/customer/portal/2</a> and let the moderators know about the suspect paper(s) that turn up. Be specific in your report as to what makes you think it is LLM-generated, with examples from the paper that no human would write, if possible.</p>



<a name="526551621"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526551621" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526551621">(Jul 01 2025 at 08:20)</a>:</h4>
<p><span class="user-mention silent" data-user-id="510824">Zoltan A. Kocsis (Z.A.K.)</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/526532165">said</a>:</p>
<blockquote>
<p>Number of days since an AI-generated slop article made it to <code>math.LO</code>: zero. Again.</p>
</blockquote>
<p>"Submitted to Inventiones Mathematicae"</p>
<p>LOL</p>



<a name="526556277"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526556277" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Amar Hadzihasanovic <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526556277">(Jul 01 2025 at 08:43)</a>:</h4>
<p>It seems like the author is picking a different primary classification for every paper</p>



<a name="526556355"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526556355" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Amar Hadzihasanovic <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526556355">(Jul 01 2025 at 08:44)</a>:</h4>
<p>First one was math.CT, second cs.LO, third math.RT, fourth math.FA</p>



<a name="526556420"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526556420" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Amar Hadzihasanovic <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526556420">(Jul 01 2025 at 08:44)</a>:</h4>
<p>Looks like it could be a conscious effort to avoid moderation</p>



<a name="526914105"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526914105" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526914105">(Jul 03 2025 at 04:08)</a>:</h4>
<p><span class="user-mention silent" data-user-id="510824">Zoltan A. Kocsis (Z.A.K.)</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/526532165">said</a>:</p>
<blockquote>
<p>Number of days since an AI-generated slop article made it to <code>math.LO</code>: zero. Again.</p>
</blockquote>
<p>And again zero :(</p>



<a name="526914438"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526914438" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526914438">(Jul 03 2025 at 04:13)</a>:</h4>
<p>I wonder what could be a community Plan B if the arXiv moderators are unable to cope with this wave and the arXiv becomes viXraised. ArXiv overlays with additional community filters?</p>



<a name="526914722"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526914722" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526914722">(Jul 03 2025 at 04:18)</a>:</h4>
<p>In particular, it doesn't look like arXiv's "get endorsement or academic-email" system can be tightened any further without causing undue difficulty to regular academics who want to post genuine preprints.</p>



<a name="526918572"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526918572" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526918572">(Jul 03 2025 at 05:09)</a>:</h4>
<p><span class="user-mention" data-user-id="510824">@Zoltan A. Kocsis (Z.A.K.)</span> Is this about "systemic contraints"? ;-) The one graphic in that paper looks like a typical not-that-good LLM trying to make a technical image.</p>



<a name="527444264"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/527444264" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#527444264">(Jul 07 2025 at 10:06)</a>:</h4>
<p>uhm would a webpage with a list of 'suspected slop' preprints be too strong of a reaction to this phenomenon? I'd be willing to setup that, and have people submit me entries</p>



<a name="527463343"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/527463343" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Chad Nester <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#527463343">(Jul 07 2025 at 12:01)</a>:</h4>
<p>"The Wall of Shame"</p>



<a name="527513258"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/527513258" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Morgan Rogers (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#527513258">(Jul 07 2025 at 16:12)</a>:</h4>
<p>Sounds good to me <span class="user-mention" data-user-id="275932">@Matteo Capucci (he/him)</span></p>



<a name="527514058"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/527514058" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#527514058">(Jul 07 2025 at 16:16)</a>:</h4>
<p>By the way, it's wise to be quite polite and cautious in your public description of this web page, to reduce your chance of getting sued and/or harrassed.   Having gotten threats from people I criticized publicly, I can assure you it's not much fun.</p>



<a name="527604656"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/527604656" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#527604656">(Jul 08 2025 at 05:02)</a>:</h4>
<p>If it's not wholly AI-generated, it's at least got lots of LLM fingerprints all over the formatting and structure <a href="https://arxiv.org/abs/2507.04089">https://arxiv.org/abs/2507.04089</a></p>



<a name="529199942"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/529199942" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#529199942">(Jul 17 2025 at 04:22)</a>:</h4>
<p><a href="https://arxiv.org/search/math?searchtype=author&amp;query=Hajebi,+P">https://arxiv.org/search/math?searchtype=author&amp;query=Hajebi,+P</a> :-(</p>



<a name="531552419"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531552419" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Peva Blanchard <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531552419">(Jul 29 2025 at 09:01)</a>:</h4>
<p>Not an AI-generated paper, but I just stumble on a AI-generated <a href="https://www.numberanalytics.com/blog/ultimate-guide-fibration-category-theory">blog article about fibrations</a>. (At the very least, the website is quite honest since it reveals that the author is Llama-4)</p>



<a name="531586258"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531586258" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531586258">(Jul 29 2025 at 11:42)</a>:</h4>
<p>It's amusing how the definition of fibration is wrong.   Also funny how the grammar is frequently wrong in the same way:</p>
<blockquote>
<p>Fibration is a fundamental concept</p>
</blockquote>
<blockquote>
<p>Relationship between Fibration and Other Category Theory Concepts</p>
</blockquote>
<blockquote>
<p>Fibration is closely related to</p>
</blockquote>
<p>etc.</p>
<p>Of course I have to be amused, because otherwise I'd break down and cry about how the pool of human knowledge is getting contaminated by sludge like this.</p>
<p><span aria-label="poop" class="emoji emoji-1f4a9" role="img" title="poop">:poop:</span></p>



<a name="531589210"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531589210" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531589210">(Jul 29 2025 at 11:58)</a>:</h4>
<p>I would guess the places where grammar is funny is where the human used search+replace to prepare the prompts to write the pages. This is all mass produced, it's quite harrowing.</p>



<a name="531672374"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531672374" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531672374">(Jul 29 2025 at 18:14)</a>:</h4>
<p>I'd like to think all this AI slop will drive the value of expertise upward; after all, now the world not only needs John to write blog posts, but to correct AI generated slop posts too.</p>



<a name="531692599"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531692599" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531692599">(Jul 29 2025 at 20:23)</a>:</h4>
<p>If the world needs me to do that, the world is in deep trouble.</p>



<a name="531748666"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531748666" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531748666">(Jul 30 2025 at 05:02)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531586258">said</a>:</p>
<blockquote>
<p>Of course I have to be amused, because otherwise I'd break down and cry about how the pool of human knowledge is getting contaminated by sludge like this.</p>
</blockquote>
<p>This prompted me to wonder (somewhat fancifully) whether we could create a parallel humans-only Internet.  Then we could cede the current Internet to the AIs, who would eventually implode due to model collapse.</p>
<p>At the very least, I'm thinking seriously about not posting new preprints on the arXiv any more, or any other public site from which they could be scraped to train AIs to generate mathematical-sounding slop.  Surely there'd still be some way to make them freely accessible to humans.</p>



<a name="531767420"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531767420" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531767420">(Jul 30 2025 at 07:23)</a>:</h4>
<p>And I suppose the same should apply to blog posts.</p>



<a name="531768321"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531768321" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531768321">(Jul 30 2025 at 07:28)</a>:</h4>
<p>You have to both figure out some this-is-a-real-human verification method, which probably means some kind of biometrics, and also some method of preventing anybody from just downloading the human-only Internet and feeding it to the bots offline...The first one is solvable but invasive but I'm really not sure how to do the second. DRM for every file on your Internet? Bleh</p>



<a name="531769767"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531769767" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Graham Manuell <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531769767">(Jul 30 2025 at 07:35)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276777">Mike Shulman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531748666">said</a>:</p>
<blockquote>
<p>At the very least, I'm thinking seriously about not posting new preprints on the arXiv any more, or any other public site from which they could be scraped to train AIs to generate mathematical-sounding slop.  Surely there'd still be some way to make them freely accessible to humans.</p>
</blockquote>
<p>I don't understand how this helps anyone? AI will still be used to output nonsense whether or not it is trained on your specific papers. It will also be trained on any papers  you publish in journals in any case. All you would be doing is making it harder for humans to access your articles.</p>



<a name="531773951"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531773951" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Peva Blanchard <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531773951">(Jul 30 2025 at 07:57)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531768321">said</a>:</p>
<blockquote>
<p>You have to both figure out some this-is-a-real-human verification method, which probably means some kind of biometrics, and also some method of preventing anybody from just downloading the human-only Internet and feeding it to the bots offline...The first one is solvable but invasive but I'm really not sure how to do the second. DRM for every file on your Internet? Bleh</p>
</blockquote>
<p>I don't know for the second problem. The first problem (distinguishing humans from bots) is indeed already an issue. (e.g., if I remember correctly, Facebook removes billions fake accounts every year). Some people in the cryptography/privacy world work on that, something along the lines of proving that you hold a state-issued ID card without revealing the details (using zero-knowledge proofs).</p>
<p>I guess this will probably trigger an arms race between "human-detectors" and "human-provers".</p>



<a name="531774387"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531774387" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531774387">(Jul 30 2025 at 07:59)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531768321">said</a>:</p>
<blockquote>
<p>also some method of preventing anybody from just downloading the human-only Internet and feeding it to the bots offline</p>
</blockquote>
<p>I was imagining that human users would be constantly verified (however that would work) whenever they access an individual document, so they couldn't just log in once and then click "download the Internet" and get it all.</p>
<p>I did say it was  fanciful.  But if the alternative is ceding the Internet to the AIs and having nothing to replace it with, maybe we should be working harder on it.</p>



<a name="531774647"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531774647" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531774647">(Jul 30 2025 at 08:00)</a>:</h4>
<p>Yes, I'm pretty sympathetic.</p>



<a name="531777576"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531777576" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531777576">(Jul 30 2025 at 08:12)</a>:</h4>
<p><span class="user-mention silent" data-user-id="386922">Graham Manuell</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531769767">said</a>:</p>
<blockquote>
<p>AI will still be used to output nonsense whether or not it is trained on your specific papers.</p>
</blockquote>
<p>It's like voting, or reducing your carbon footprint.  Anything any individual person does has a miniscule effect on the world, but the world is made up of individuals, so we should all follow the categorical imperative.</p>
<p>But I suppose you're right that currently we have no technical solution for disseminating information to humans only, and if we want to stay in this job we have to disseminate our research in some way.  I guess we can hope that the pending copyright lawsuits against AI trainers bear some fruit...</p>



<a name="531778548"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531778548" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Peva Blanchard <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531778548">(Jul 30 2025 at 08:17)</a>:</h4>
<p>There is another line of research focusing on "voluntarily poisoning" your data, so that an AI trained on a dataset including your data could be flagged. See e.g. <a href="https://arxiv.org/abs/2410.09101">this paper</a>.</p>



<a name="531779722"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531779722" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531779722">(Jul 30 2025 at 08:23)</a>:</h4>
<p>That's a nice idea.  I don't suppose it's possible for those of us who don't work with data?</p>



<a name="531790513"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531790513" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531790513">(Jul 30 2025 at 09:14)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276777">Mike Shulman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531777576">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="386922">Graham Manuell</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531769767">said</a>:</p>
<blockquote>
<p>AI will still be used to output nonsense whether or not it is trained on your specific papers.</p>
</blockquote>
<p>It's like voting, or reducing your carbon footprint.  Anything any individual person does has a miniscule effect on the world, but the world is made up of individuals, so we should all follow the categorical imperative.</p>
</blockquote>
<p>That's why we do category theory!   <span aria-label="upside down" class="emoji emoji-1f643" role="img" title="upside down">:upside_down:</span> </p>
<p>I do a lot of things that seem a bit quixotic in that they have a miniscule effect.  But I wouldn't stop posting my papers to the arXiv because the positive effect of spreading my ideas to more human mathematicians seems to grossly outweigh the negative effect due to AIs reading them.   </p>
<p>One slightly quixotic act I've been enjoying is <em>not using Google</em> and instead paying to use a search engine called <a href="https://kagi.com/">Kagi</a> which doesn't show me advertisements, doesn't rely on ad revenue, isn't as susceptible to search engine optimization tricks, doesn't push AI on me, and has a greater variety of search filters, like an "academic" filter.   I heard about this from <a href="https://pluralistic.net/2025/07/28/twiddlehazard/#outboard-brains-considered-harmful">Cory Doctorow</a>, who has very interesting things to say about enshittification (a term he invented).</p>



<a name="531790919"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531790919" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531790919">(Jul 30 2025 at 09:16)</a>:</h4>
<p>I have little hope that the deluge of AI-generated content will abate. However, it seems more realistic to me that a human verification process could be used to whitelist authors who are known not to be bad actors. arXiv's current verification process is currently entirely insufficient, but I feel it can be addressed if arXiv actually take action. However, a severe disadvantage of this is that it makes academia even less accessible to those outside of it than it already is (because most likely the only entrypoint to verification would be via a verified user/institution).</p>



<a name="531791352"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531791352" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531791352">(Jul 30 2025 at 09:18)</a>:</h4>
<p>In some sense, I feel it doesn't matter if huge amounts of AI slop is generated (distasteful as it is), so long as it's possible to filter out. In this case, I think that only permitting trusted users to post research is more important than only permitting trusted users to view research.</p>



<a name="531793942"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531793942" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531793942">(Jul 30 2025 at 09:31)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531790513">said</a>:</p>
<blockquote>
<p>One slightly quixotic act I've been enjoying is <em>not using Google</em> and instead paying to use a search engine called <a href="https://kagi.com/">Kagi</a> which doesn't show me advertisements, doesn't rely on ad revenue, isn't as susceptible to search engine optimization tricks, doesn't push AI on me, and has a greater variety of search filters, like an "academic" filter.</p>
</blockquote>
<p>That's very interesting!  I see that they do also supply an AI, which seems to contradict their goal of "humanizing the web", but I gather from your remarks that you can turn it off.  How does Kagi compare to Google with <a href="https://udm14.com/">udm14</a>, which disables ads and AI?</p>



<a name="531826666"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531826666" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Peva Blanchard <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531826666">(Jul 30 2025 at 12:11)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276777">Mike Shulman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531779722">said</a>:</p>
<blockquote>
<p>That's a nice idea.  I don't suppose it's possible for those of us who don't work with data?</p>
</blockquote>
<p>In principle, the idea works on a vectorial representation of the data, thus should be applicable to text. However, text is more complicated in practice because the mapping "text <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> vector" is less flexible than, e.g., "image <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> vector", so the poison is harder to craft.</p>
<p>Also, since an individual author only provides a "few" samples, I don't know how relevant the technique can be for individual usage. I expect journals or any document archive to be more likely to be "clients" of this approach. Anyway, this is still research, so there is no off-the-shelf software/service available for now, as far as I know.</p>



<a name="531837180"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531837180" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531837180">(Jul 30 2025 at 13:00)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276777">Mike Shulman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531793942">said</a>:</p>
<blockquote>
<p>That's very interesting!  I see that they do also supply an AI, which seems to contradict their goal of "humanizing the web", but I gather from your remarks that you can turn it off. </p>
</blockquote>
<p>I must have turned it off immediately, because I never see it.</p>
<blockquote>
<p>How does Kagi compare to Google with <a href="https://udm14.com/">udm14</a>, which disables ads and AI?</p>
</blockquote>
<p>I'll have to compare them for a while.  So far they look comparable except udm14 doesn't have those various filter settings.   Kagi claims to have boolean search but it seems to be working erratically - maybe I'm not using it right.   It also claims you can search by "least relevant first", which is hilarious.</p>



<a name="531837805"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531837805" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531837805">(Jul 30 2025 at 13:03)</a>:</h4>
<p>Here's Cory Doctorow on Google and related things.    (It's long, but folks can skip it if they don't care.)</p>
<blockquote>
<p>That's where Ardoline and Lenzo's work comes in. They both document the ways in which we turn these online services into cognitive prostheses, and then investigate how the enshittification of these services ends up making us stupider, by taking away the stuff that helps us think. They're drawing a line between platform decay and cognitive decay.</p>
<p>The authors look at examples like the enshittification of Google Search, a product that Google has deliberately and irretrievably enshittified:</p>
<p><a href="https://pluralistic.net/2024/04/24/naming-names/#prabhakar-raghavan">https://pluralistic.net/2024/04/24/naming-names/#prabhakar-raghavan</a></p>
<p>The web is a giant cognitive prosthesis, and early web tools put a lot of emphasis on things like bookmark management and local caching, so that the knowledge and cognition you externalized to the web were under your control. But Google Search was so goddamned magic – before they cynically destroyed it – that a lot of us switched from "not remembering things because you have a bookmark that takes you to a website that remembers it for you" to "not remembering things and not remembering where to find them, and just typing queries into Google." The collapse of Google into a giant pile of shit is like giving every web user a traumatic brain injury.</p>
<p>It's a good paper, but I think the situation is actually more dire than the paper makes it out to be, thanks to the AI bubble –</p>
<p>Wait! I'm not actually going to talk about what AI can do (which is a combination of a small set of boring useful things, a bunch of novelties, and a long list of things that AI can't do but is being used to do anyway). I'm talking about the financial fraud that AI serves.</p>
<p>Tech companies <em>must</em> be perceived as growing, because when a company is growing, it is valued <em>far</em> more highly than a company is once it has "matured." This is called the "price to earnings ratio" – the number of dollars investors are willing to pay for the company compared to the number of dollars a company is bringing in. So long as a company is growing, the PE ratio is very high, and this helps the company to <em>actually</em> grow. That's because the shares in growing companies are highly liquid, and can be traded for equity in other companies and/or the labor of key employees, meaning that growth companies can almost always outbid their mature counterparts when it comes to expanding through acquisition and hiring. That means that while a company is growing, its PE ratio can help it <em>keep</em> growing.</p>
<p>But here's the corollary: when a growth company <em>stops</em> growing, its shares are suddenly and violently revalued as though they were shares in a mature company, which tanks the personal net worth of the company's top managers and key employees (whose portfolios are stuffed with their employer's now-plummeting stock). Worse: in order to retain those employees and hire more (or to acquire key companies), the no-longer-growing company has to pay with cash, which is <em>much</em> harder to get than its own shares. Even worse: they have to bid against <em>growing</em> companies.</p>
<p>A growth company is like an airplane that has two modes: climbing and nose-diving, and while it's easy to go from climbing to crashing, it's <em>much</em> harder to go the other way. Ironically, the moment at which a company's growth is most likely to stall is right after its greatest triumph: after a company conquers its market, it has nowhere else to go. Google's got a 90% Search market-share – how can it possibly grow Search?</p>
<p>It can't (just like Meta can't really grow social, and Microsoft can't grow office suites, etc), so it has to convince Wall Street that it has a shot at conquering some <em>other</em> market that the street perceives as unimaginably vast and thus capable of keeping the growth engine going. Tech has pulled a lot of sweaty tricks to create this impression, inflating bubbles like "pivot to video" and "metaverse" and "cryptocurrency," and now it's AI.</p>
<p>The problem is that AI just isn't very popular. People go out of their way to avoid AI products:</p>
<p><a href="https://www.tandfonline.com/doi/full/10.1080/19368623.2024.2368040">https://www.tandfonline.com/doi/full/10.1080/19368623.2024.2368040</a></p>
<p>For an AI-driven growth story to work, tech companies have to produce a stream of charts depicting lines that go up and to the right, reflecting some carefully chosen set of metrics demonstrating AI's increasing popularity. One way to produce these increasing trend-lines on demand is to replace all the most commonly used parts of a service that you love and rely on with buttons that summon an AI. This is the "fatfinger AI economy," a set of trendlines produced by bombarding people who graze their screens with a stray fingertip with a bunch of AI bullshit, so you can claim that your users are "engaging" with AI:</p>
<p><a href="https://pluralistic.net/2025/05/02/kpis-off/#principal-agentic-ai-problem">https://pluralistic.net/2025/05/02/kpis-off/#principal-agentic-ai-problem</a></p>
<p>It's a form of "twiddling" – changing how a service works on a per-user, per-interaction basis in order to shift value from the user to the company:</p>
<p><a href="https://pluralistic.net/2023/02/19/twiddler/">https://pluralistic.net/2023/02/19/twiddler/</a></p>
<p>Twiddling represents <em>the</em> big cognitive hazard from enshittification during the AI bubble: the parts of your UI that matter most to you are the parts that you use as vital cognitive prostheses. A product team whose KPI is "get users to tap on an AI button" is going to use the fine-grained data they have on your technological activities to preferentially target these UI elements that you rely on with AI boobytraps. You are too happy, so they are leaving money on the table, and they're coming for it.</p>
<p>This is a form of "attention rent": the companies are taxing your muscle-memory, forcing you to produce deceptive usage statistics at the price of either diverting your cognition from completing a task to hunt around for the button that banishes the AI and lets you get back to what you were doing; or to simply abandon that cognitive prosthesis:</p>
<p><a href="https://pluralistic.net/2023/11/03/subprime-attention-rent-crisis/#euthanize-rentiers">https://pluralistic.net/2023/11/03/subprime-attention-rent-crisis/#euthanize-rentiers</a></p>
<p>It's <em>true</em> "engagement-hacking": not performing acts of dopamine manipulation; but rather, spying on your habitual usage of a digital tool in order to swap buttons around in order to get you to make a number go up. It's exploiting the fact that you engage with something useful and good to make it less useful and worse, because if you're too happy, some enshittifier is leaving money on the table.</p>
</blockquote>



<a name="532396447"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532396447" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ben Kaminsky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532396447">(Aug 02 2025 at 00:19)</a>:</h4>
<p>I think there is a new danger in a kind of "crank singularity" happening.</p>
<p>I'll admit I am not an experienced crankologist with decades under my belt like Dr. Baez, but I've noticed there is a stark difference in two different classes of cranks. I often stop by reddit and view the local "alt-physics" subreddits. I refer to these as "crank aquariums".</p>
<p>The "lower cranks" are mostly mystical, don't know much math and talk about the typical "woo" topics. Consciousness collapses the wavefunction, sacred geometry, you know the drill.</p>
<p>The "higher cranks" often use very sophisticated math to prove "new theorems". But their math is PDE heavy and fundamentally brittle. They discover "new terms" that Maxwell and Schrodinger "forgot" in their equations. They never use things like category theory, homology, moduli spaces, Lie groups, etc. They only do very heavy analysis.</p>
<p>The problem is that with new LLMs, these two separate classes of cranks could merge into a new form of "hybridized super-crank" generating endless reams of "self-conscious quantum operator algebras" and "quasi-cosmic graviton quantum field theories" with actual PDEs that are potentially sophisticated enough to overwhelm hapless journal editors.</p>
<p>Maybe there could be an additional filter (Category Theory CAPCHA?) of some kind where people hoping to publish could go into front of an AI interviewer and answer randomly generated questions about group theory, topology, cohomology classes, functors and sheaves. If you pass you receive a badge of some kind (this used to be referred to as a "degree" I believe?) These kinds of topics are usually too abstract for cranks to actually understand, so it could be a "mental block".</p>
<p>Anyway, I'm partly spitballing here, but the true lesson to remember is that mathematics chases deep structure, whereas cranks may only imitate it shallowly. (I am serious about that, but I felt <em>some</em> comic relief is also in order here. I hope the mods won't exile me to the crank aquarium!)</p>



<a name="532417410"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532417410" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532417410">(Aug 02 2025 at 05:07)</a>:</h4>
<p>I'll add my piece of comic relief. There's a certain resistance in flagging some content as AI generated slop, for a similar reason that people are very wary of flagging a text as plagiarism... No problem, I can do it <span aria-label="smiling devil" class="emoji emoji-1f608" role="img" title="smiling devil">:smiling_devil:</span> I have a certain experience and it's very pleasurable for me to tell someone who deserves it "shut up and com me back when you know what a determinant is"</p>



<a name="532431169"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532431169" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532431169">(Aug 02 2025 at 08:22)</a>:</h4>
<p><span class="user-mention silent" data-user-id="930833">Ben Kaminsky</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532396447">said</a>:</p>
<blockquote>
<p>I think there is a new danger in a kind of "crank singularity" happening.</p>
</blockquote>
<p>It's happening.  I'm getting <em>many</em> more emails from cranks, who are mostly working with LLMs to develop their 'theories'.   A quote from one of these emails:</p>
<p>"developed rigorously with the help of large language models"</p>
<p><span aria-label="rolling eyes" class="emoji emoji-1f644" role="img" title="rolling eyes">:rolling_eyes:</span> </p>
<blockquote>
<p>The "higher cranks" often use very sophisticated math to prove "new theorems". But their math is PDE heavy and fundamentally brittle. </p>
</blockquote>
<p>I haven't seen any cranks using very sophisticated math.  Some <em>pretend</em> to do so.   And I agree with you that the number is dramatically increasing now that fake math is easy to get from a LLM.  Luckily anyone who really knows math can see this stuff is fake.</p>



<a name="532480521"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532480521" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532480521">(Aug 02 2025 at 19:13)</a>:</h4>
<p>maybe this has been brought up before, but what about adding "community notes" to the arxiv, like how it works on twitter?  That's how twitter was able to pass moderation responsibility onto the public at scale.</p>



<a name="532548885"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532548885" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532548885">(Aug 03 2025 at 11:53)</a>:</h4>
<p>This is a dangerous plan, for several reasons that people noticed about 15 minutes after first thinking of this idea a couple of decades ago.  Some still favor it, but the arXiv moderators aren't going to take those chances.</p>



<a name="532548959"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532548959" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532548959">(Aug 03 2025 at 11:54)</a>:</h4>
<p>Note however that anyone can start their own "arXiv reviews".</p>



<a name="532588555"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532588555" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532588555">(Aug 03 2025 at 19:00)</a>:</h4>
<p>just out of curiosity, why would it not work for the arxiv (what are those reasons?) if it does work for twitter (or maybe it doesn't work for twitter?)?  the only thing I can think of off the top of my head is that there is too small of a "public of experts" for the community notes to be accurate</p>



<a name="532593382"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532593382" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532593382">(Aug 03 2025 at 19:56)</a>:</h4>
<p>My impression is that it's at least questionable whether it works for twitter.</p>



<a name="532593399"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532593399" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532593399">(Aug 03 2025 at 19:56)</a>:</h4>
<p>Although I don't use twitter myself so I can't say of personal experience.</p>



<a name="532593403"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532593403" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Evan Patterson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532593403">(Aug 03 2025 at 19:57)</a>:</h4>
<p>I'm also curious. Though I'm no longer on X/Twitter, my impression is that community notes continued to function quite well even as the platform deteriorated in other respects, pretty reliably flagging false or misleading posts.</p>



<a name="532594434"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532594434" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532594434">(Aug 03 2025 at 20:09)</a>:</h4>
<p>Imagine if everyone got to say shit about each other's papers on the arXiv.  It would be a bloodbath.  It would quickly degenerate into obscenities and lawsuits unless the comments were moderated.  Some of those lawsuits would even target the arXiv itself.  <em>All</em> of this is the last thing the arXiv moderators want.  They don't have time for this.</p>



<a name="532595032"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532595032" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Evan Patterson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532595032">(Aug 03 2025 at 20:16)</a>:</h4>
<p>To be successful, such an approach would need to carefully circumscribe the allowed claims for a community note. The allowed claims would <em>not</em> include random opinions about or reviews of papers. Rather, the point would be to use the community to reach consensus on matters of fact, such as:</p>
<ul>
<li>was this paper largely/completely generated by an LLM?</li>
<li>does this paper plagiarize from existing sources?</li>
<li>is this paper a "crank" paper, in the specific sense that it fails to conform to widely shared norms about mathematical/scientific communication?</li>
</ul>



<a name="532599213"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532599213" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532599213">(Aug 03 2025 at 21:02)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275965">Evan Patterson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532595032">said</a>:</p>
<blockquote>
<p>such an approach would need to carefully circumscribe the allowed claims for a community note</p>
</blockquote>
<p>To enforce that, all the comments would have to be moderated, right?</p>



<a name="532600252"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532600252" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532600252">(Aug 03 2025 at 21:13)</a>:</h4>
<p>The arXiv staff might prefer to spend their limited time/energy/money on moderating papers rather than moderating comments on papers.</p>
<p>I suppose one approach to avoid moderating comments would be a form which allowed no freely written text, just yes/no answers to questions like "this paper is AI-generated".   This would have its own problems.</p>



<a name="532601705"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532601705" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532601705">(Aug 03 2025 at 21:29)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532600252">said</a>:</p>
<blockquote>
<p>The arXiv staff might prefer to spend their limited time/energy/money on moderating papers</p>
</blockquote>
<p>And apparently they don't even have enough time/energy/money to do a good enough job of that, which is what led to this whole conversation.  (Not intended as a criticism of them, just an observation about lack of resources.)</p>



<a name="532601852"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532601852" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532601852">(Aug 03 2025 at 21:31)</a>:</h4>
<p>I don't suppose any agency would be likely to award a grant to support work to keep AI-generated slop off of the arXiv.</p>



<a name="532603451"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532603451" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532603451">(Aug 03 2025 at 21:50)</a>:</h4>
<p>here's how it works on twitter:  The Community Notes algorithm publishes notes based on agreement from contributors who have a history of disagreeing.[21] Rather than based on majority rule,[34] the program's algorithm prioritizes notes that receive ratings from a "diverse range of perspectives".[28][35] For a note to be published, a contributor must first propose a note under a tweet.[21] The program assigns different values to contributors' ratings, categorising users with similar rating histories as a form of "opinion classification", determined by a vague alignment with the left and right-wing political spectrum. The bridging-based machine-learning algorithm requires ratings from both sides of the spectrum in order to publish notes, that can have the intended effect of decreasing interaction with such content.[35][36][37]</p>
<p>Contributors are volunteers with access to an interface from which they have the ability to monitor tweets and replies that may be misleading.[21][9][38] Notes in need of ratings by contributors are located under a "Needs your help" section of the interface. Other contributors then give their opinion on the usefulness of the note, identifying notes as "Helpful" or "Not Helpful".[21][39] The contributor gets points if their note is validated,[40][21] known as "Rating Impact", that reflects how helpful a contributors' ratings have been.[39][41][42] X users are able to vote on whether they find notes helpful or not,[18] but must apply to become contributors in order to write notes, the latter being restricted by "Rating Impact" as well as the Community Notes guidelines.[39][41]</p>



<a name="532610646"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532610646" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532610646">(Aug 03 2025 at 23:14)</a>:</h4>
<p>OK, so how many people in the world do you think a) will have a legitmate informed opinion on a somewhat niche research (sub)field and b) will engage in the commenting process on arXiv papers? Twitter notes worked (or "worked") because of scale. If you have a hundred thousand people, a million people, engaging on a topic that doesn't require PhD-level education to understand even the words, then this type of approach might achieve some level of community consensus.</p>



<a name="532613451"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532613451" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ruby Khondaker (she/her) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532613451">(Aug 03 2025 at 23:46)</a>:</h4>
<p>This reminds me of the “wisdom of the crowd” phenomenon!</p>



<a name="532631112"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532631112" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532631112">(Aug 04 2025 at 02:48)</a>:</h4>
<p>maybe it would be enough for the experts to be more numerous than the crackpots, rather than needing huge numbers of experts? (hopefully there are more experts than crackpots, but I have no idea tbh). But I suppose the discussion is moot without the arxiv actually doing it.</p>



<a name="532648279"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532648279" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532648279">(Aug 04 2025 at 05:25)</a>:</h4>
<p><a href="https://www.daniellitt.com/blog/2025/7/17/arxiv-in-trouble">https://www.daniellitt.com/blog/2025/7/17/arxiv-in-trouble</a></p>



<a name="532650306"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532650306" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ruby Khondaker (she/her) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532650306">(Aug 04 2025 at 05:50)</a>:</h4>
<p><span class="user-mention silent" data-user-id="281326">Ryan Wisnesky</span> <a href="#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/532631112">said</a>:</p>
<blockquote>
<p>maybe it would be enough for the experts to be more numerous than the crackpots, rather than needing huge numbers of experts? (hopefully there are more experts than crackpots, but I have no idea tbh). But I suppose the discussion is moot without the arxiv actually doing it.</p>
</blockquote>
<p>I feel like almost definitionally there would have to be more crackpots than experts, owing to the relative difficulty in becoming either?</p>



<a name="532667301"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532667301" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532667301">(Aug 04 2025 at 07:50)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532594434">said</a>:</p>
<blockquote>
<p>Imagine if everyone got to say shit about each other's papers on the arXiv.  It would be a bloodbath.  It would quickly degenerate into obscenities and lawsuits unless the comments were moderated.</p>
</blockquote>
<p>I think you're too pessimistic John, people tend to be decent 99% of the time, especially if their full name is on display. I agree there would be the need to moderate the remaining 1% though.</p>



<a name="532667322"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532667322" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532667322">(Aug 04 2025 at 07:50)</a>:</h4>
<p><span class="user-mention silent" data-user-id="934428">Ruby Khondaker (she/her)</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532650306">said</a>:</p>
<blockquote>
<p>I feel like almost definitionally there would have to be more crackpots than experts, owing to the relative difficulty in becoming either?</p>
</blockquote>
<p>I assumed the only people allowed to play this game would be people who have been endorsed to write papers on the arXiv.   In this population there are more experts than crackpots.... though not everyone is an expert on every topic, indeed quite the opposite. </p>
<p>If you let random passers-by evaluate arXiv papers, there will definitely be lots of crackpots and people with grudges and other unproductive motivations.</p>



<a name="532667606"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532667606" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532667606">(Aug 04 2025 at 07:52)</a>:</h4>
<p>Yeah of course! And it'd be spamland very quickly...</p>



<a name="532669299"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532669299" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532669299">(Aug 04 2025 at 08:01)</a>:</h4>
<p>I think the whole problem is addressed by fixing the endorsement system. If arXiv is unable to moderate by themselves (which appears to be the case), then they need to either hire more people, or ask for trusted volunteers.</p>



<a name="532675539"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532675539" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532675539">(Aug 04 2025 at 08:33)</a>:</h4>
<p>So much of the academic publishing system already depends upon volunteers, it doesn't feel like a stretch to have people contributing to moderating arXiv. It'd be great to have it become a place of recorded scientific discussions re a piece of work, including errata, reviews, and comments, so that it can also be a starting point for journal reviews.</p>



<a name="532677607"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532677607" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532677607">(Aug 04 2025 at 08:45)</a>:</h4>
<p>The arXiv does a lot of moderating, and it's sometimes too strict: the case of Phillip Helbig comes to mind:</p>
<ul>
<li>Phillip Helbig, <a href="https://johncarlosbaez.wordpress.com/2022/02/04/submission-to-arxiv/">Submission to arXiv</a>, <em>Azimuth</em>.</li>
</ul>
<p>I've had a paper shifted from the group theory section to combinatorics against my will.   I think their problem with AI-generated papers is that they're not used to filtering out papers of this sort.  Most crackpots write in a way that sends off a particular vibe, which is easy for experienced moderators to detect.  But LLMs are different.  I think one can learn to detect them, <em>at least so far</em>.</p>



<a name="532681838"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532681838" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532681838">(Aug 04 2025 at 09:07)</a>:</h4>
<p>In comments awaiting moderation (heh) on Peter Woit's blog that pointed out Daniel Litt's blog post on the topic, I wrote</p>
<blockquote>
<p>I would like to see anyone whose papers were deemed to be AI-generated have all their endorsements stripped, and they should need to get fresh endorsements, probably more than one.</p>
</blockquote>
<blockquote>
<p>Moreover, I would even go so far as to propose that anyone who endorses an AI-generated paper should have their endorser-status reset, so that they cannot immediately re-endorse the person who they originally endorsed, until they have submitted more papers as usual. It would be a bit of an incentive to actually look at the paper for fear of a relatively harmless removal of a privilege. Active researchers would get back to having endorser powers before too long...</p>
</blockquote>



<a name="532687391"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532687391" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532687391">(Aug 04 2025 at 09:34)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532677607">said</a>:</p>
<blockquote>
<p>The arXiv does a lot of moderating, and it's sometimes too strict</p>
</blockquote>
<p>Clearly not enough (though being sometimes too strict is another problem).</p>



<a name="532690328"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532690328" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532690328">(Aug 04 2025 at 09:50)</a>:</h4>
<p>I don't think a sheer increase in quantity is the best solution, given that the arXiv has an approximately zero budget for doing this.   Moderation needs to be focused on the key problems.  Now that AI is a big problem, the moderators need to be pointed to AI-generated papers, and they need to learn to spot them.  They do look at every paper, I believe.</p>



<a name="532703725"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532703725" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532703725">(Aug 04 2025 at 11:06)</a>:</h4>
<blockquote>
<p>I don't think a sheer increase in quantity is the best solution, given that the arXiv has an approximately zero budget for doing this.</p>
</blockquote>
<p>That would be solved by having volunteers. It would not take many volunteers for each category, and I think this would be relatively easy to achieve, as I think many people would be willing to help.</p>



<a name="532703874"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532703874" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532703874">(Aug 04 2025 at 11:07)</a>:</h4>
<p>And now we're back full circle to the question we started with: how do we contact the arXiv moderators and get them to do something?</p>



<a name="532706774"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532706774" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532706774">(Aug 04 2025 at 11:25)</a>:</h4>
<p>Possibly someone could email <a href="mailto:moderators@arxiv.org">moderators@arxiv.org</a> (mentioned on <a href="https://info.arxiv.org/help/moderation/index.html">https://info.arxiv.org/help/moderation/index.html</a>) and see whether they can give any information?</p>



<a name="532707106"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532707106" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532707106">(Aug 04 2025 at 11:28)</a>:</h4>
<p>One can just email them.  The moderators are listed <a href="https://arxiv.org/moderators/">here</a> and the people in charge are listed starting <a href="https://info.arxiv.org/about/people/index.html">here</a>.  </p>
<p>They seem fairly quiet and secretive.   People with problems report getting little response.  When I emailed some of the leaders about the virtues of getting a backup hosted outside the US, they never replied.   They did develop such a backup system.  But I'm not claiming they did that in response to my email.</p>
<p>It probably helps a lot to contact someone in charge whom you know personally.  I could contact Jacques Distler, for example.   I'm betting he'll say they are already familiar with the AI problem.</p>



<a name="532707944"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532707944" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532707944">(Aug 04 2025 at 11:33)</a>:</h4>
<p>I find the lack of any official statement from arXiv on the matter a little disappointing, if they are aware of it.</p>



<a name="532711954"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532711954" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532711954">(Aug 04 2025 at 11:57)</a>:</h4>
<p>They tend to talk as little as possible.</p>



<a name="532781938"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532781938" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532781938">(Aug 04 2025 at 19:04)</a>:</h4>
<p>I'm a smidge nervous that this is getting awfully close to the point of someone like this actually getting a grant from a place like Templeton.</p>



<a name="532859335"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532859335" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532859335">(Aug 05 2025 at 08:10)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/532711954">said</a>:</p>
<blockquote>
<p>We contend that the result—autoequivalence with the Monster Group—is statistically improbable unless the theory holds validity. Consequently, AI consensus would not have been achieved erroneously.</p>
</blockquote>
<p>My eyes are bleeding <span aria-label="skull" class="emoji emoji-1f480" role="img" title="skull">:skull:</span><span aria-label="skull" class="emoji emoji-1f480" role="img" title="skull">:skull:</span></p>



<a name="532859517"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532859517" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532859517">(Aug 05 2025 at 08:11)</a>:</h4>
<p>'What are the chances I lost the lottery with this very specific ticket? So low it must be the lottery which is wrong'</p>



<a name="532860404"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532860404" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532860404">(Aug 05 2025 at 08:16)</a>:</h4>
<p>Yes, this would be hilarious if it were intended as a joke.    I also like the misuse of "autoequivalence" - which usually means an equivalence of something with <em>itself</em> - in the claim that U-category theory (whatever that is) is autoequivalent with the Monster Group.</p>



<a name="532865604"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532865604" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532865604">(Aug 05 2025 at 08:41)</a>:</h4>
<blockquote>
<p>They tend to talk as little as possible.</p>
</blockquote>
<p><em>[...] With its customary discretion, the Company did not reply directly; instead, it scrawled its brief argument in the rubble of a mask factory. [...]</em></p>



<a name="532919217"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532919217" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Daniel Rogozin <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532919217">(Aug 05 2025 at 13:40)</a>:</h4>
<p>Blimey, it looks like some overcomplicated topics for 1st year students.<br>
<a href="/user_uploads/21317/JnuRrz16uz1RSqNwxlLekz1Y/Screenshot-2025-08-05-at-14.39.43.png">Screenshot 2025-08-05 at 14.39.43.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/JnuRrz16uz1RSqNwxlLekz1Y/Screenshot-2025-08-05-at-14.39.43.png" title="Screenshot 2025-08-05 at 14.39.43.png"><img data-original-content-type="image/png" data-original-dimensions="1662x1368" src="/user_uploads/thumbnail/21317/JnuRrz16uz1RSqNwxlLekz1Y/Screenshot-2025-08-05-at-14.39.43.png/840x560.webp"></a></div>



<a name="532942336"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532942336" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532942336">(Aug 05 2025 at 15:39)</a>:</h4>
<p>Here's a counter-point from twitter: "I don’t get why Arxiv containing slop is a bad thing. I mean sure it’s frustrating and annoying, but merely having been put on Arxiv should give a paper draft exactly zero additional credibility".  I suppose I feel the same way; I always thought of the arxiv as simply a substitute for putting pdfs on a personal website, with endorsement meant to keep the arxiv from turning into a public API for storing PDFs as opposed to technical vetting.</p>



<a name="532948882"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532948882" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532948882">(Aug 05 2025 at 16:18)</a>:</h4>
<p>arXiv is a tool for researchers. It becomes useless if there is absolutely no moderation. If anyone can upload whatever they like, you might as well refer to the <a href="https://libraryofbabel.info/">Library of Babel</a> instead.</p>



<a name="532950769"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532950769" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532950769">(Aug 05 2025 at 16:29)</a>:</h4>
<p>"I don't see why this thing that two bad adjectives apply to is bad"</p>



<a name="532951144"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532951144" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532951144">(Aug 05 2025 at 16:31)</a>:</h4>
<p>Besides being frustrating and annoying: Almost everything on arXiv is real research, which is a huge benefit for discoverability, and does actually mean that something being on arXiv increases its credibility far over "a random thing on the Internet." Both those values could be mostly destroyed by too high a slop ratio.</p>



<a name="532952547"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532952547" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532952547">(Aug 05 2025 at 16:40)</a>:</h4>
<p>Yes, I too don't want all the pseudoscientific vomit in the world to be on the arXiv.   That's called the <em>internet</em>.</p>
<p>When I search for papers with a given keyword on the arXiv, I want a majority of them to actually make sense!  And they do, so far.  </p>
<p>The arXiv is a tremendously useful tool, which I use several times a day, largely because of what's <em>not</em> on it.</p>



<a name="535171697"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/535171697" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Stay <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#535171697">(Aug 19 2025 at 17:07)</a>:</h4>
<p>For now, I'd expect custom tools devoted to detecting AI to still work.  If the detect/evade war gets to the point where AI is providing LEAN proofs of its work, I think that's a win.</p>
<ul>
<li><a href="https://github.com/mbzuai-nlp/DetectLLM">https://github.com/mbzuai-nlp/DetectLLM</a></li>
<li><a href="https://arxiv.org/abs/2301.11305">https://arxiv.org/abs/2301.11305</a></li>
<li><a href="https://www.scribbr.com/ai-tools/best-ai-detector/">https://www.scribbr.com/ai-tools/best-ai-detector/</a></li>
</ul>



<a name="535228109"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/535228109" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#535228109">(Aug 20 2025 at 02:21)</a>:</h4>
<p>See Figure 3 in this paper <a href="https://arxiv.org/abs/2410.02457">https://arxiv.org/abs/2410.02457</a> It's a midjourney dream of what a mathematical image should look like.</p>



<a name="535265213"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/535265213" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#535265213">(Aug 20 2025 at 08:37)</a>:</h4>
<p>you're talking about figure 3, but I would like to draw your attention to page 2....</p>



<a name="535295077"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/535295077" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Josh Chen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#535295077">(Aug 20 2025 at 11:42)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276422">David Michael Roberts</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/535228109">said</a>:</p>
<blockquote>
<p>See Figure 3 in this paper <a href="https://arxiv.org/abs/2410.02457">https://arxiv.org/abs/2410.02457</a> It's a midjourney dream of what a mathematical image should look like.</p>
</blockquote>
<p>The escalation from Figs 1--2 to Fig 3 gave me a good laugh.</p>



<a name="546399334"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546399334" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546399334">(Oct 22 2025 at 08:50)</a>:</h4>
<p><a href="https://arxiv.org/abs/2510.17829">This work</a> claims a full lean verification of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo mathvariant="normal">≠</mo><mi>N</mi><mi>P</mi></mrow><annotation encoding="application/x-tex">P\neq NP</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">NP</span></span></span></span>, but only provides snippets of the code in their paper that reads like the output of an LLM.</p>



<a name="546409368"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546409368" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546409368">(Oct 22 2025 at 09:40)</a>:</h4>
<p>I saw the title and comments in the daily arXiv mailing, and <em>knew</em> it was LLM-generated.</p>



<a name="546409473"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546409473" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546409473">(Oct 22 2025 at 09:40)</a>:</h4>
<p>Opening it to find many, many short subsections consisting just of dot points was almost not needed.</p>



<a name="546413701"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546413701" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Damiano Mazza <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546413701">(Oct 22 2025 at 10:00)</a>:</h4>
<p>Out of curiosity, I went straight to the technical part (Sect. 3) and randomly checked one of  the results (Theorem 3.14), finding that it is obviously false.  (The category <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi><mi mathvariant="bold">o</mi><mi mathvariant="bold">m</mi><mi mathvariant="bold">p</mi></mrow><annotation encoding="application/x-tex">\mathbf{Comp}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8805em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">Comp</span></span></span></span></span> mentioned in the theorem is formally introduced and makes sense per se, but it is certainly <em>not</em> additive with the proposed definition).</p>



<a name="546438780"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546438780" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546438780">(Oct 22 2025 at 11:58)</a>:</h4>
<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>



<a name="546440181"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546440181" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546440181">(Oct 22 2025 at 12:05)</a>:</h4>
<p>The Lean code is not publically available as far as I can tell, so we only have the word of the author/LLM that such a verification exists.</p>



<a name="546466317"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546466317" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> daniel gratzer <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546466317">(Oct 22 2025 at 14:02)</a>:</h4>
<p><span class="user-mention silent" data-user-id="971326">Elisha Goldman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546438780">said</a>:</p>
<blockquote>
<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>
</blockquote>
<p>Just to note, while it's highly likely the Lean code is proof of _something_ (assuming it's not just full of <code>sorry</code>s), (un)subtly incorrect definitions and theorem statements mean the code almost surely does not prove P /= NP.</p>
<p>[Back in the day, someone won a full bitcoin for proving false in Rocq. They did this by simply defining <code>False := True</code> and then proving "false" according to this new definition. The moral of this story is that theorem provers are not really designed to be adversary-proof and mechanized proofs are only as good as the formalized statement is]</p>



<a name="546466591"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546466591" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546466591">(Oct 22 2025 at 14:03)</a>:</h4>
<p><span class="user-mention silent" data-user-id="277285">Martti Karvonen</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546440181">said</a>:</p>
<blockquote>
<p>The Lean code is not publically available as far as I can tell, so we only have the word of the author/LLM that such a verification exists.</p>
</blockquote>
<p>It's in the paper itself, it just takes a bit to get there (the main proof is on page 40)</p>



<a name="546467088"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546467088" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546467088">(Oct 22 2025 at 14:05)</a>:</h4>
<p>Lol, I wanna hear more about the Rocq story</p>



<a name="546468666"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546468666" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> daniel gratzer <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546468666">(Oct 22 2025 at 14:11)</a>:</h4>
<p>Here are <a href="https://yoichihirai.com/edcon-yoichi-hirai.pdf">some slides</a> by the person who created the site (and lost the bitcoin). I can't find the original cite now searching quickly, but more of the story is here. Gosh... can't believe this was all already 10 years ago.</p>



<a name="546542529"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546542529" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546542529">(Oct 22 2025 at 20:10)</a>:</h4>
<p><span class="user-mention silent" data-user-id="971326">Elisha Goldman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546466591">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="277285">Martti Karvonen</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546440181">said</a>:</p>
<blockquote>
<p>The Lean code is not publically available as far as I can tell, so we only have the word of the author/LLM that such a verification exists.</p>
</blockquote>
<p>It's in the paper itself, it just takes a bit to get there (the main proof is on page 40)</p>
</blockquote>
<p>Oh oops, I hadn't realized they seemingly give all of it in bits and pieces, rather than just showing an excerpt or two.</p>



<a name="546564071"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546564071" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546564071">(Oct 22 2025 at 23:03)</a>:</h4>
<p><span class="user-mention silent" data-user-id="971326">Elisha Goldman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546438780">said</a>:</p>
<blockquote>
<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>
</blockquote>
<p>To be more specific, people usually misunderstand things in very vague ways, so being able to see exactly what they misunderstand here seems useful pedagogically since it shows where students might be confused.<br>
Also, even if it doesn't say what they think it does, the fundamental idea (computational complexity via homology) seems novel and is formally-verified non-gibberish, so maybe some form of it could be interesting (though I definitely don't know enough CS to say anything meaningful here)</p>



<a name="546564869"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546564869" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546564869">(Oct 22 2025 at 23:11)</a>:</h4>
<p>Imo the best thing to tell cranks is to provide a formal verification, it filters out the slop and helps them realize where they're wrong/too vague<br>
(the other response is getting annoyed, which doesn't really help anyone)</p>



<a name="546569105"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546569105" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546569105">(Oct 23 2025 at 00:00)</a>:</h4>
<p>Interestingly, in the formalization section, the author does not claim to have formally verified the result on additivity of Comp that <span class="user-mention" data-user-id="276839">@Damiano Mazza</span> correctly points out is obviously false.</p>



<a name="546571175"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546571175" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546571175">(Oct 23 2025 at 00:24)</a>:</h4>
<p>Elisha, I'm afraid you are massively begging the question by assuming that "the fundamental idea" exists. It seems to me that this paper constitutes nothing less than an adversarial attack on the scientific community, or at least on its non-expert fringe who cannot check its claims for themselves.</p>



<a name="546571332"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546571332" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546571332">(Oct 23 2025 at 00:27)</a>:</h4>
<p>I spent almost an hour poking through here carefully to see where the more central claims begin to fall apart. Theorems 3.24 and 4.1 brazenly contradict each other, proving respectively that problems in P are homologically trivial and that all NP-complete problems are homologically isomorphic to all problems in NP. Even more to the point, the <em>proof</em> of 3.24 really shows the lie where it says "The detailed argument uses the functoriality of the computational homology construction and the fact that homology isomorphisms preserve the 'computational topology' of problems." The last claim is, naturally, not mathematically defined. The computational chain complex also appears not to be genuinely defined, as far as I can tell. I haven't compared to see what the author chucked into the formalized definition.</p>



<a name="546571425"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546571425" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546571425">(Oct 23 2025 at 00:28)</a>:</h4>
<p>Hopefully nobody here is surprised that this is entirely nonsense, but I think it was still worth looking at a little more closely to continue tracking the progress of these adversarial attacks. I was just discussing the notion of a slop artist (whether or not they believe their own slop) trying to justify themselves via submitting formal verifications with a friend last week, and here we are now! This is a clear big step less obviously wrong than the papers we were discussing even a couple of months ago.</p>



<a name="546571860"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546571860" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546571860">(Oct 23 2025 at 00:34)</a>:</h4>
<p>OK, looking at the formalization, the author specifically attempts to bamboozle the reader by exhibiting a detailed-looking verification of the chain complex property of an alleged chain complex which would, if it existed, be the singular chain complex of the graph of "computation paths" in a computational problem; but a computation path <em>itself</em> depends on some concept of a "Configuration" of a verifier of a computational problem which is never actually defined.</p>



<a name="546571861"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546571861" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546571861">(Oct 23 2025 at 00:34)</a>:</h4>
<p>I think this work reduces upon inspection to ordinary crankery about "imagine the simplicial complexes of all efficient computation paths, man, and then, like, the cycles in this complex would represent the irreducible computational steps, bro!" But it's really buried quite a lot deeper than it otherwise would be with all the LLM help.</p>



<a name="546580875"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546580875" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546580875">(Oct 23 2025 at 02:49)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546571175">said</a>:</p>
<blockquote>
<p>Elisha, I'm afraid you are massively begging the question by assuming that "the fundamental idea" exists. It seems to me that this paper constitutes nothing less than an adversarial attack on the scientific community, or at least on its non-expert fringe who cannot check its claims for themselves.</p>
</blockquote>
<p>Ah, well, that's what I get for only skimming it, fair enough</p>



<a name="546582792"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546582792" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546582792">(Oct 23 2025 at 03:18)</a>:</h4>
<p>You're right though, my internal model of the crank was that they start as a passionate amateur, invest way too much ego in their theories, and only then become antagonistic due to perceived rejection; and that formal verification can replace the social rejection with a learning experience where they find their mistake and salvage what they can. I hadn't really considered any deliberate deception (aside from self-deception). The psychology must be different, I guess the antagonism comes before actually appreciating the subject, maybe for political reasons, like Sokal?</p>



<a name="546583103"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546583103" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546583103">(Oct 23 2025 at 03:23)</a>:</h4>
<p>There are very sad stories of people being completely sent off the deep end by AI-hallucinated theories of everything/conspiracies. Something like this is rather mild by comparison. I don't think the author doubts this paper at all.</p>



<a name="546583558"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546583558" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546583558">(Oct 23 2025 at 03:29)</a>:</h4>
<p>Fwiw I think that when people get to this stage, telling them that LLM output is meaningless (while correct) is not gonna be accepted, since it's already replaced half their brain</p>



<a name="546621262"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546621262" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ivan Di Liberti <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546621262">(Oct 23 2025 at 08:40)</a>:</h4>
<p><a href="https://arxiv.org/pdf/2510.19444">https://arxiv.org/pdf/2510.19444</a></p>



<a name="546696273"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546696273" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546696273">(Oct 23 2025 at 14:30)</a>:</h4>
<p><span class="user-mention silent" data-user-id="971326">Elisha Goldman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546438780">said</a>:</p>
<blockquote>
<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>
</blockquote>
<p>It may be good, but it's not important.   It may not even be good, since it wasted the time of several smart people here, who could have been doing something more useful.</p>



<a name="546762726"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546762726" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546762726">(Oct 23 2025 at 20:18)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276422">David Michael Roberts</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546583103">said</a>:</p>
<blockquote>
<p>There are very sad stories of people being completely sent off the deep end by AI-hallucinated theories of everything/conspiracies. Something like this is rather mild by comparison. I don't think the author doubts this paper at all.</p>
</blockquote>
<p>Maybe. I was taking from the deeply buried gaps in the allegedly formalized exposition, plus the assertion that all code was shared on a GitHub repo which remains private (if it exists), that this might be an intentional scam. It's hard for me to see how you'd believe your Lean code was really proving something true if you hadn't actually defined all the types in it.</p>



<a name="547042102"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/547042102" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#547042102">(Oct 25 2025 at 09:06)</a>:</h4>
<p><span class="user-mention silent" data-user-id="358258">daniel gratzer</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546466317">said</a>:</p>
<blockquote>
<p>[Back in the day, someone won a full bitcoin for proving false in Rocq. They did this by simply defining <code>False := True</code> and then proving "false" according to this new definition.</p>
</blockquote>
<p>Wow that's an easy 100k <span aria-label="open mouth" class="emoji emoji-1f62e" role="img" title="open mouth">:open_mouth:</span></p>



<a name="547042620"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/547042620" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#547042620">(Oct 25 2025 at 09:13)</a>:</h4>
<p><span class="user-mention silent" data-user-id="277318">Ivan Di Liberti</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546621262">said</a>:</p>
<blockquote>
<p><a href="https://arxiv.org/pdf/2510.19444">https://arxiv.org/pdf/2510.19444</a></p>
</blockquote>
<p>What's wrong about this one?</p>



<footer class="site-footer">

<hr><p>Last updated: Nov 01 2025 at 12:09 UTC</p>
This archive runs on a customization of <a href="https://github.com/zulip/zulip-archive">zulip-archive</a>
</footer>
</body>

</html>