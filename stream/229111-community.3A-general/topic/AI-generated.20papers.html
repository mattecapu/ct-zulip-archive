<!DOCTYPE html>
<html>
<head>
  	<meta charset="utf-8" />
  	<meta http-equiv="X-UA-Compatible" content="IE=edge" />
  	<meta name="viewport" content="width=device-width, initial-scale=1" />
  
<link rel="stylesheet" href="https://mattecapu.github.io/ct-zulip-archive/style.css" /><title>AI-generated papers · community: general · Zulip Chat Archive</title>
</head>
<body>
<header>
<a href="https://mattecapu.github.io/ct-zulip-archive" class="home-link">
        <img class="logo" src="https://zulip-avatars.s3.amazonaws.com/21317/realm/icon.png?version=3" />
        <h1>Category Theory<br/>Zulip Server<br/>Archive</h1>
        </a>
        <p>
        You're reading the public-facing archive of the <a href="https://categorytheory.zulipchat.com/">Category Theory Zulip server</a>.<br/>
        
        To join the server you need an invite. Anybody can get an invite by contacting <a href="https://matteocapucci.wordpress.com">Matteo Capucci</a> at <em>name dot surname at gmail dot com</em>.<br/>
        
        For all things related to this archive refer to the same person.
        </p>
        </header>
        <hr />
    
<h2>Stream: <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/index.html">community: general</a></h2>
<h3>Topic: <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html">AI-generated papers</a></h3>

<hr>

<base href="https://categorytheory.zulipchat.com">

<a name="507901615"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507901615" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507901615">(Mar 25 2025 at 00:33)</a>:</h4>
<p>I hate to do this, but the four arXiv papers by this author, uploaded in the last week, all look wholly AI-generated to me: <a href="https://arxiv.org/search/?query=Reizi&amp;searchtype=author">https://arxiv.org/search/?query=Reizi&amp;searchtype=author</a></p>



<a name="507901831"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507901831" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507901831">(Mar 25 2025 at 00:35)</a>:</h4>
<p>In particular the appendix of <a href="https://arxiv.org/abs/2503.16555">https://arxiv.org/abs/2503.16555</a> there is a promise of proofs, examples and so on, but the entire text of the appendix is this:</p>
<blockquote>
<p>In this appendix, we present additional proofs, detailed calculations, and further examples<br>
that complement the results in the main text. In particular, the appendix includes:  <br>
* A complete proof of the back-and-forth construction used in Lemma 5.8.  <br>
* Detailed verifications of the functoriality of the Henkin and compactness-based model constructions.<br>
* Concrete examples illustrating the construction of models for specific theories.  </p>
<p>These supplementary materials are provided to offer deeper insight into the technical details and to demonstrate how our unified framework can be applied to various logical systems.</p>
</blockquote>
<p>The next text is the bibliography and that's it. The content is also extremely banal.</p>



<a name="507940187"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507940187" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Chad Nester <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507940187">(Mar 25 2025 at 06:48)</a>:</h4>
<p>After a cursory inspection of <a href="https://arxiv.org/abs/2503.16570">https://arxiv.org/abs/2503.16570</a>, I agree.</p>



<a name="507946343"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507946343" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507946343">(Mar 25 2025 at 07:33)</a>:</h4>
<p>I can't find any information about this supposed person online except an affiliation via their email, but I've made a report to the Arxiv.</p>



<a name="507956956"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507956956" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507956956">(Mar 25 2025 at 08:33)</a>:</h4>
<p><a href="/user_uploads/21317/2XH9nNv7Uy7EppX4uo7NT3lQ/image.png">image.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/2XH9nNv7Uy7EppX4uo7NT3lQ/image.png" title="image.png"><img data-original-content-type="image/png" data-original-dimensions="991x806" src="/user_uploads/thumbnail/21317/2XH9nNv7Uy7EppX4uo7NT3lQ/image.png/840x560.webp"></a></div><p>yep, no way a human wrote this</p>



<a name="507966779"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/507966779" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#507966779">(Mar 25 2025 at 09:17)</a>:</h4>
<p>Stupid LLM forgetting the syntax for bold in TeX and falling back on Markdown...</p>



<a name="508059186"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508059186" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508059186">(Mar 25 2025 at 15:39)</a>:</h4>
<p>I'm proud to say I called bullshit from the titles alone in my feed lol glad I wasn't wrong</p>



<a name="508067115"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508067115" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508067115">(Mar 25 2025 at 16:10)</a>:</h4>
<p>Heh, we did an experiment on LLMs that produce SQL code, and for many of them, no matter how much you tell them not to format the output, they still do it. Stripping extra comments and markdown/html out of responses turned out to be the hardest part of interacting with the LLM in an automated flow.</p>



<a name="508136651"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508136651" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Joe Moeller <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508136651">(Mar 25 2025 at 22:06)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275932">Matteo Capucci (he/him)</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/508059186">said</a>:</p>
<blockquote>
<p>I'm proud to say I called bullshit from the titles alone in my feed lol glad I wasn't wrong</p>
</blockquote>
<p>Right, natural transformations between theorems.</p>



<a name="508136715"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508136715" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Joe Moeller <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508136715">(Mar 25 2025 at 22:07)</a>:</h4>
<p>I noticed there are two orders of the names used. Two of the papers are JRB, and two are BJR. What could be the point of that?</p>



<a name="508152007"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508152007" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508152007">(Mar 26 2025 at 00:27)</a>:</h4>
<p>The email address seems to be attached to Open University Japan, so name-order may have been auto-generated differently for the different papers?</p>



<a name="508787020"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508787020" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Noah Chrein <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508787020">(Mar 28 2025 at 15:44)</a>:</h4>
<p><span class="user-mention silent" data-user-id="282822">fosco</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/507956956">said</a>:</p>
<blockquote>
<p>yep, no way a human wrote this</p>
</blockquote>
<p>To be fair, I have seen researchers who <em>just</em> learned about category theory writing this way. </p>
<p>Anyway, the AI-generated slop CT papers are coming. I've noticed that <a href="http://chat.qwen.ai">Qwen</a> 2.5 is trained on a lot of higher/formal category theory. It's fun to play with and it can produce approximately accurate references to results, which can sometimes cut down on search time. It's not yet good enough to generate any meaningfully creative results, and is not enough to fool a half-keen eye, but I can imagine an undergrad using qwen to write a undergrad thesis that nobody reads.</p>



<a name="508788675"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508788675" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ivan Di Liberti <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508788675">(Mar 28 2025 at 15:51)</a>:</h4>
<p><span class="user-mention silent" data-user-id="277976">Noah Chrein</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/508787020">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="282822">fosco</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/507956956">said</a>:</p>
<blockquote>
<p>yep, no way a human wrote this</p>
</blockquote>
<p>To be fair, I have seen researchers who <em>just</em> learned about category theory writing this way. </p>
<p>Anyway, the AI-generated slop CT papers are coming. I've noticed that <a href="http://chat.qwen.ai">Qwen</a> 2.5 is trained on a lot of higher/formal category theory. It's fun to play with and it can produce approximately accurate references to results, which can sometimes cut down on search time. It's not yet good enough to generate any meaningfully creative results, and is not enough to fool a half-keen eye, but I can imagine an undergrad using qwen to write a undergrad thesis that nobody reads.</p>
</blockquote>
<p>What is Qwen and how happen it was trained on so much category theory?</p>



<a name="508792542"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508792542" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508792542">(Mar 28 2025 at 16:07)</a>:</h4>
<p>Qwen appears to be Alibaba's language model. I hadn't heard of it till now.</p>



<a name="508792840"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/508792840" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Noah Chrein <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#508792840">(Mar 28 2025 at 16:08)</a>:</h4>
<p>Perhaps the Chinese understand the importance of category theory to mathematics and hence to generalized cognition</p>



<a name="521179894"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521179894" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521179894">(May 29 2025 at 22:18)</a>:</h4>
<p>There’s an interesting fake paper on the ArXiv today. I can’t really tell if it’s AI crankery or just the old fashioned kind. Did anybody glance at it? <a href="https://arxiv.org/abs/2505.22558">https://arxiv.org/abs/2505.22558</a></p>



<a name="521180392"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521180392" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Cole Comfort <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521180392">(May 29 2025 at 22:22)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521179894">said</a>:</p>
<blockquote>
<p>There’s an interesting fake paper on the ArXiv today. I can’t really tell if it’s AI crankery or just the old fashioned kind. Did anybody glance at it? <a href="https://arxiv.org/abs/2505.22558">https://arxiv.org/abs/2505.22558</a></p>
</blockquote>
<p>The excessive use of lists suggests AI</p>



<a name="521182914"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521182914" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521182914">(May 29 2025 at 22:47)</a>:</h4>
<p>Right, that makes sense. It was harder to find obvious local absurdities than in papers further up this thread, which is disappointing.</p>



<a name="521187274"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521187274" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521187274">(May 29 2025 at 23:32)</a>:</h4>
<p>There's a whole bunch recently that I have been <del>complaining about</del> pointing out elsewhere. The author is uploading a new paper every couple of days, and the title names something after himself. I'm happy to see today that they've been moved to math.GM ! (as I suggested)</p>
<p><a href="https://export.arxiv.org/find/math/1/au:+Alpay_F/0/1/0/all/0/1">https://export.arxiv.org/find/math/1/au:+Alpay_F/0/1/0/all/0/1</a></p>



<a name="521187635"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521187635" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521187635">(May 29 2025 at 23:35)</a>:</h4>
<p>And in the case at the top of the thread, namely <a href="https://arxiv.org/search/?query=Reizi&amp;searchtype=author">https://arxiv.org/search/?query=Reizi&amp;searchtype=author</a> all these are also math.GM classified now, not math.CT.</p>



<a name="521189500"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521189500" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521189500">(May 29 2025 at 23:55)</a>:</h4>
<p>Seems like in theory the arXiv "endorsement system" should deal with AI generated papers just like any other spam, but I guess it doesn't work in practice?    <a href="https://info.arxiv.org/help/endorsement.html">https://info.arxiv.org/help/endorsement.html</a></p>



<a name="521194635"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521194635" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521194635">(May 30 2025 at 00:35)</a>:</h4>
<p>Yes, I'm a bit confused how all these people are getting endorsements.</p>



<a name="521194732"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521194732" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521194732">(May 30 2025 at 00:36)</a>:</h4>
<p>At the very least it should be possible to "un-endorse" them after they've demonstrated their crankiness.</p>



<a name="521215571"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521215571" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521215571">(May 30 2025 at 04:09)</a>:</h4>
<p>Another one! <a href="https://arxiv.org/abs/2505.22931">https://arxiv.org/abs/2505.22931</a></p>



<a name="521216374"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521216374" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521216374">(May 30 2025 at 04:18)</a>:</h4>
<p>Maybe the arXiv needs to appoint a category theorist to the team of moderators...</p>



<a name="521233951"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521233951" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521233951">(May 30 2025 at 07:02)</a>:</h4>
<p>I thought arXiv had a strong stance against crackpottery, so why are these papers allowed to remain under math.GM, rather than being removed entirely?</p>



<a name="521235022"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521235022" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521235022">(May 30 2025 at 07:10)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521182914">said</a>:</p>
<blockquote>
<p>Right, that makes sense. It was harder to find obvious local absurdities than in papers further up this thread, which is disappointing.</p>
</blockquote>
<p>The phrase "discrete conformal field theory" in the abstract made me raise my eyebrows.  As if that were a known thing.  Given how much people try everything, there probably <em>is</em> <strong>some</strong> work on something called discrete conformal field theory, but....</p>
<p>Yeah, there's a paper <a href="https://link.springer.com/article/10.1007/s00220-022-04475-x.">Conformal Field Theory at the Lattice Level: Discrete Complex Analysis and Virasoro Structure</a> trying to understand how conformal field theory is related to field theory on a lattice.  But most conformal transformations don't map a lattice to itself, so this is bound to be rough, and the idea that "Recursive Difference Categories and Topos-Theoretic Universality" would have something to say about it is, umm, questionable.</p>



<a name="521236395"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521236395" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521236395">(May 30 2025 at 07:21)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276092">Nathanael Arkor</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521233951">said</a>:</p>
<blockquote>
<p>I thought arXiv had a strong stance against crackpottery, so why are these papers allowed to remain under math.GM, rather than being removed entirely?</p>
</blockquote>
<p>It can be hard to tell whether a math paper is crazy, and people whose papers are rejected entirely complain a lot, so it seems the arXiv folks find it convenient to put borderline papers into math.GM,  expecting people 'in the know' to beware of such papers.  That's my impression anyway.</p>



<a name="521236804"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521236804" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521236804">(May 30 2025 at 07:24)</a>:</h4>
<p>It's more diplomatic than having math.CP for crackpot math.</p>



<a name="521244418"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521244418" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521244418">(May 30 2025 at 08:16)</a>:</h4>
<p>This is a truly beautiful era to witness first-hand.</p>



<a name="521246836"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521246836" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Areeb SM <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521246836">(May 30 2025 at 08:31)</a>:</h4>
<p>ViXra appears to be <a href="https://ai.vixra.org/">embracing</a> the future...</p>



<a name="521248709"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521248709" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521248709">(May 30 2025 at 08:42)</a>:</h4>
<p>But not unreservedly:</p>
<blockquote>
<p><a href="http://viXra.org">viXra.org</a> only accept scholarly articles written without AI assistance. Please go to <a href="http://ai.viXra.org">ai.viXra.org</a> to submit new scholarly article written with AI assistance.</p>
</blockquote>



<a name="521385995"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521385995" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Joe Moeller <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521385995">(May 30 2025 at 23:38)</a>:</h4>
<p>arxiv could use the exact same disclaimer, only changing the first instance of "vixra".</p>



<a name="521423955"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521423955" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521423955">(May 31 2025 at 08:49)</a>:</h4>
<p><a href="http://ai.viXra.org">ai.viXra.org</a> sounds like a fascinating crackpot sociology experiment.  They have 343 papers so far.   Within the subject of physics, most of the papers are on "relativity and cosmology", so we can guess that part of physics attracts crackpots the most.     Within mathematics, 75% of the papers are on number theory.</p>



<a name="521424158"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521424158" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521424158">(May 31 2025 at 08:52)</a>:</h4>
<p>Yesterday's first submitted paper on general relativity and cosmology:</p>
<p><strong>The Pi-Periodic 22/7ths Dimension: A Quantum Gravity Framework for Dark Energy</strong></p>
<blockquote>
<p>We propose a novel 4+1-dimensional quantum gravity framework incorporating a compactified extra dimension, τ , with a periodicity of π (to 22 decimal places), symbolically tied to the rational approximation 22/7.</p>
</blockquote>
<p>Someone is taking this 22/7 stuff very seriously!  I believe Archimedes came up with this approximation to pi, and it was good enough that by the Middle Ages a bunch of mathematicians believed <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">\pi = </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span></span></span></span> 22/7.</p>



<a name="521425529"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521425529" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521425529">(May 31 2025 at 09:13)</a>:</h4>
<blockquote>
<p>by the Middle Ages a bunch of mathematicians believed <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">\pi =</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span></span></span></span>  22/7.</p>
</blockquote>
<p><span aria-label="surprise" class="emoji emoji-1f62e" role="img" title="surprise">:surprise:</span>  Wait, is it not? /s</p>



<a name="521426011"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521426011" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> James Deikun <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521426011">(May 31 2025 at 09:21)</a>:</h4>
<p>Archimedes squared the circle with this ONE WEIRD TRICK!  Geometers hate him!</p>



<a name="521426330"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521426330" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521426330">(May 31 2025 at 09:26)</a>:</h4>
<p>Actually I learned this when reading about the mathematician <a href="https://johncarlosbaez.wordpress.com/2025/04/24/civilizational-collapse-part-5/">Franco of Liège</a>.  In 1020 he got interested in the ancient Greek problem of squaring the circle. But since he believed that pi is 22/7, he started studying the square root of 22/7.  I don't know if he figured out how to construct the square root of 22/7 with straightedge and compass.  But he did manage to prove that the square root of 22/7 is irrational!</p>
<p>Now, this is better than it sounds, because I believe the old Greek proof that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mn>2</mn></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1328em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9072em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord">2</span></span></span><span style="top:-2.8672em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1328em;"><span></span></span></span></span></span></span></span></span> is irrational had been lost in western Europe at this time.   So it took some serious ingenuity.  </p>
<p>Still, it's a sad reflection on the sorry state of mathematical knowledge in western Europe from around 500 AD to 1000 AD.   It was better elsewhere at that time.   I find this local collapse of civilization, and how people recovered, quite fascinating.</p>



<a name="521426636"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521426636" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521426636">(May 31 2025 at 09:30)</a>:</h4>
<p>Could AI slop prompt some loss of collective intelligence now?</p>



<a name="521429490"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521429490" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Fabrizio Romano Genovese <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521429490">(May 31 2025 at 10:15)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521426636">said</a>:</p>
<blockquote>
<p>Could AI slop prompt some loss of collective intelligence now?</p>
</blockquote>
<p>In general any tool that helps you thinking makes you sloppier in some respect. So yes. For instance, ancient languages are often way more complicated grammatically than new languages. One reason for this is that being able to say "Go around the mammoth, without being heard, by exactly half of a circle" in fewer words may have been a big advantage when we were hunter-gatherers, so languages tended to be more expressive. With civilization, inception of written support etc we lost the need to formulate such complicated statements in a compact way, languages became less expressive, and we probably lost some of our cognitive ability in the process as well. It's always a tradeoff.</p>



<a name="521434376"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521434376" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521434376">(May 31 2025 at 11:30)</a>:</h4>
<p>I'm thinking more about how successive generations of Roman summaries of Greek scientific texts watered them down to a homeopathic dilution of their original strength.  Then many of the originals were lost, at least in western Europe.</p>



<a name="521434597"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521434597" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521434597">(May 31 2025 at 11:32)</a>:</h4>
<p><span class="user-mention" data-user-id="679887">@Fabrizio Romano Genovese</span>: could you share a reference for the claim that older languages have higher entropy than modern languages?</p>



<a name="521525145"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521525145" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Notification Bot <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521525145">(Jun 01 2025 at 08:05)</a>:</h4>
<p>13 messages were moved from this topic to <a class="stream-topic" data-stream-id="229451" href="/#narrow/channel/229451-meta.3A-off-topic/topic/language.3A.20the.20rise.20and.20fall.20of.20complex.20grammars/with/521434597">#meta: off-topic &gt; language: the rise and fall of complex grammars</a> by <span class="user-mention silent" data-user-id="275920">John Baez</span>.</p>



<a name="521805518"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521805518" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521805518">(Jun 02 2025 at 17:32)</a>:</h4>
<p><span class="user-mention silent" data-user-id="679887">Fabrizio Romano Genovese</span> <a href="#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/521429490">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/521426636">said</a>:</p>
<blockquote>
<p>Could AI slop prompt some loss of collective intelligence now?</p>
</blockquote>
<p>In general any tool that helps you thinking makes you sloppier in some respect. So yes. For instance, ancient languages are often way more complicated grammatically than new languages. One reason for this is that being able to say "Go around the mammoth, without being heard, by exactly half of a circle" in fewer words may have been a big advantage when we were hunter-gatherers, so languages tended to be more expressive. With civilization, inception of written support etc we lost the need to formulate such complicated statements in a compact way, languages became less expressive, and we probably lost some of our cognitive ability in the process as well. It's always a tradeoff.</p>
</blockquote>
<p>uuuhmm what's a reference for this? smells really funny to me...</p>



<a name="521829583"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/521829583" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#521829583">(Jun 02 2025 at 19:44)</a>:</h4>
<p><a href="#narrow/channel/229451-meta.3A-off-topic/topic/language.3A.20the.20rise.20and.20fall.20of.20complex.20grammars/with/521434597">https://categorytheory.zulipchat.com/#narrow/channel/229451-meta.3A-off-topic/topic/language.3A.20the.20rise.20and.20fall.20of.20complex.20grammars/with/521434597</a></p>



<a name="523707907"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523707907" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523707907">(Jun 12 2025 at 08:53)</a>:</h4>
<p>New AI paper up:</p>
<ol>
<li><a href="https://arxiv.org/abs/2506.06885">https://arxiv.org/abs/2506.06885</a></li>
</ol>



<a name="523708003"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523708003" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523708003">(Jun 12 2025 at 08:54)</a>:</h4>
<p>This one is funny because it outs itself<br>
<a href="/user_uploads/21317/g4CqaUqHyjbQrd8X6oR_oXBq/image.png">image.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/g4CqaUqHyjbQrd8X6oR_oXBq/image.png" title="image.png"><img data-original-content-type="image/png" data-original-dimensions="818x398" src="/user_uploads/thumbnail/21317/g4CqaUqHyjbQrd8X6oR_oXBq/image.png/840x560.webp"></a></div>



<a name="523708584"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523708584" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523708584">(Jun 12 2025 at 08:57)</a>:</h4>
<p>I actually approve of this way of approaching AI tools: personally, I don't think they automatically disqualify a paper. The principle should be the the author is ultimately responsible to check their results, and remains fully accountable.</p>



<a name="523708600"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523708600" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Patrick Nicodemus <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523708600">(Jun 12 2025 at 08:57)</a>:</h4>
<blockquote>
<p>Our results offer a formal justification for this procedure, suggesting that the analytic<br>
continuation is not arbitrary but is in fact forced by the underlying principles of symmetry<br>
and normalization.</p>
</blockquote>
<p>Kind of a funny quote because the analytic continuation of a function is one of the most rigidly determined and least arbitrary constructions in mathematics</p>



<a name="523708805"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523708805" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523708805">(Jun 12 2025 at 08:58)</a>:</h4>
<p>Indeed, the paper is (from a quick skim) likely formally correct but basically insubtantials, it's a big cargo-cult regurgitation. The whole thing seems circular.</p>



<a name="523709393"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523709393" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Patrick Nicodemus <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523709393">(Jun 12 2025 at 09:00)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275932">Matteo Capucci (he/him)</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523708584">said</a>:</p>
<blockquote>
<p>I actually approve of this way of approaching AI tools: personally, I don't think they automatically disqualify a paper. The principle should be the the author is ultimately responsible to check their results, and remains fully accountable.</p>
</blockquote>
<p>Yeah, I think I agree. At the very least we might have to get used to seeing that writing style everywhere, I can imagine a non-native speaker feeling a lot of pressure to use it to make their wording seem natural. It doesn't inherently disqualify the paper. But, on the other hand, it makes me suspicious and vigilant of errors, and at that point even a small error would be enough to cause me to discard it.</p>



<a name="523712713"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523712713" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523712713">(Jun 12 2025 at 09:18)</a>:</h4>
<p>I suspect at some point someone will try to mass-produce papers and submit them everywhere (it's almost trivial to compile a list of journal inside the math-cs area; let the machine prepare a different paper for each item of the list; let the machine submit, let the machine handle the rebuttals and modify the paper accordingly, resubmit...), relying on small probability of success after a high number of trial. </p>
<p>It's the academia equivalent of asking out 100 girls, one of them will say yes.</p>
<p>These are very interesting times to witness. Especially if you're an irredeemable nihilist.</p>



<a name="523740601"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523740601" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523740601">(Jun 12 2025 at 11:56)</a>:</h4>
<p>I think there's a real risk of the image of CT being tarnished if this type of stuff becomes too common. The number theory people know how to funnel cranks away from their arXiv category, if category theorists can't do this, it's not a good look.</p>



<a name="523741056"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523741056" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523741056">(Jun 12 2025 at 11:59)</a>:</h4>
<p>Also, mathematicians generally are conscious of the circle squarers and the number theory cranks and so on, and can spot this stuff pretty easily, because it's on a hot-button topic and shows the usual obvious signs. But something in category theory applied to other areas (not Applied Category Theory, but to an outsider it's not necessarily obvious) that plays to the stereotypes of CT's abstract nonsense moniker just looks like another silly CT paper that claims to revolutionise our understanding of a piece of classical mathematics when really it's empty of real content.</p>



<a name="523742554"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523742554" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523742554">(Jun 12 2025 at 12:08)</a>:</h4>
<p>Perhaps not among hardcore mathematicians, who would almost surely recognise the problem and commiserate, but anyone merely adjacent, for instance someone with money who might be needed to be convinced to fund some real and good ACT may get wind of this AI nonsense. </p>
<p>Maybe I'm being too pessimistic here. But these are ideas that occur to me</p>



<a name="523756995"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523756995" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523756995">(Jun 12 2025 at 13:26)</a>:</h4>
<blockquote>
<p>The number theory people know how to funnel cranks away from their arXiv category</p>
</blockquote>
<p>How?</p>



<a name="523757655"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523757655" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523757655">(Jun 12 2025 at 13:29)</a>:</h4>
<p>Well, I get math.NT daily announcements and I've never seen a crank number theory paper, and yet I know they do turn up in math.GM.</p>



<a name="523757689"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523757689" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523757689">(Jun 12 2025 at 13:30)</a>:</h4>
<p>So somehow they manage it.</p>



<a name="523782577"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523782577" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523782577">(Jun 12 2025 at 15:29)</a>:</h4>
<p>Have we taken any action about these papers?  Contacted anyone at arXiv about removing them and un-endorsing the submitters?  That seems to me to be the obvious first step.  I'd be willing to help if needed, although I don't have the time to filter the daily submissions for them myself.</p>



<a name="523783104"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523783104" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523783104">(Jun 12 2025 at 15:32)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276422">David Michael Roberts</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523741056">said</a>:</p>
<blockquote>
<p>But something in category theory applied to other areas (not Applied Category Theory, but to an outsider it's not necessarily obvious) that plays to the stereotypes of CT's abstract nonsense moniker just looks like another silly CT paper that claims to revolutionise our understanding of a piece of classical mathematics when really it's empty of real content.</p>
</blockquote>
<p>I wonder if an effect like this could be what's causing the problem by making it easier for cranks to get endorsed with CT papers by lazy non-category-theorists.</p>



<a name="523786721"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523786721" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523786721">(Jun 12 2025 at 15:51)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276422">David Michael Roberts</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523757655">said</a>:</p>
<blockquote>
<p>Well, I get math.NT daily announcements and I've never seen a crank number theory paper, and yet I know they do turn up in math.GM.</p>
</blockquote>
<p>To me, that just suggests that the arXiv editors are better at detecting crank NT papers than crank CT papers, likely because they have had more practice at it.</p>



<a name="523792785"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523792785" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523792785">(Jun 12 2025 at 16:26)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276777">Mike Shulman</span> <a href="#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/523782577">said</a>:</p>
<blockquote>
<p>Have we taken any action about these papers?  Contacted anyone at arXiv about removing them and un-endorsing the submitters?  That seems to me to be the obvious first step.  I'd be willing to help if needed, although I don't have the time to filter the daily submissions for them myself.</p>
</blockquote>
<p>I’ve contacted the ArXiv about the first batch of these that came up. They said they’d look into it but don’t share results of investigations. I haven’t checked whether the papers are down. It feels like fingers in a dike if we can’t figure out who is endorsing these authors though!</p>



<a name="523793705"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523793705" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523793705">(Jun 12 2025 at 16:32)</a>:</h4>
<p>Did your first batch include <a href="https://arxiv.org/abs/2505.22931">Recursive Difference Categories and Topos-Theoretic Universality</a> by <a href="https://arxiv.org/search/math?searchtype=author&amp;query=Santacana,+A+B">Andreu Ballus Santacana</a>?  That was a crank paper discussed here earlier.   It's still up!   Santacana is also responsible for the new one you folks are talking about today, <a href="https://arxiv.org/abs/2506.06885">Analytic Uniqueness of Ball Volume Interpolation: Categorical Invariance and Universal Characterization</a>.</p>



<a name="523793881"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523793881" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523793881">(Jun 12 2025 at 16:33)</a>:</h4>
<p>I checked earlier, and Santacana appears to be in the <a href="https://portalrecerca.uab.cat/en/persons/andreu-ballus-santacana">department of philosophy of UAB Barcelona</a>.</p>
<p>(He's definitely got the Grothendieck bald-head thing going on.)</p>



<a name="523801760"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523801760" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523801760">(Jun 12 2025 at 17:26)</a>:</h4>
<p>I reported the papers of Barreto that David Roberts opened this thread with. Unfortunately they're still up and there have been two more since then. They're all in GM now, though, which I guess is the best it seems we can generally hope for.</p>



<a name="523847620"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523847620" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523847620">(Jun 12 2025 at 23:27)</a>:</h4>
<p>The moving to math.GM has been patchy. Some of the ones by one author whose primary listing is CS.lo haven't moved, while those that were listed under math.CT have. Presumably because computer scientists are even less well-equipped than a generic mathematician to judge what CT is actually AI-generated crank material.</p>



<a name="523848772"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523848772" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523848772">(Jun 12 2025 at 23:44)</a>:</h4>
<p>If the people submitting these things are actually employed by reputable institutions, perhaps we should contact their employers.</p>



<a name="523860779"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523860779" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523860779">(Jun 13 2025 at 02:25)</a>:</h4>
<p>The first person I reported is unlocatable online, IIRC. But that’s apparently not the case for everyone.</p>



<a name="523916691"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523916691" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523916691">(Jun 13 2025 at 10:29)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276422">David Michael Roberts</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523740601">said</a>:</p>
<blockquote>
<p>I think there's a real risk of the image of CT being tarnished if this type of stuff becomes too common. The number theory people know how to funnel cranks away from their arXiv category, if category theorists can't do this, it's not a good look.</p>
</blockquote>
<p>Every week, <em>one or two</em> of these papers make it into the <code>math.LO</code>/<code>cs.LO</code> announcements, which is frankly ridiculous.  We had a person who just had a couple of their articles GM-holed last week get through to <code>cs.LO</code> <em>again</em> this week. Especially disappointing since at the same time, I know multiple people with solid academic affiliations, long records in logic, and academic email addresses who've seen their announcements blocked/delayed while they appealed (e.g. conference extended abstracts misclassified and rejected as "abstract-only submissions", or a PhD thesis randomly rejected) :/</p>
<p>I don't think this tarnishes the image of logic itself, but it's certainly a big source of noise and not a good look for arXiv moderation.</p>



<a name="523927191"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523927191" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523927191">(Jun 13 2025 at 11:33)</a>:</h4>
<p>A possible solution is to set up a small website that collects these papers and flag them as "probably bollocks". A small number of us, committed to express a judgment evaluate these submissions pointing out "this passage is AI generated" "the second sentence at page 2 doesn't make any sense" etc</p>
<p>It takes a lot of work, but we all know what's the rule here:<br>
<a href="/user_uploads/21317/0PygcTIJuTOeZlzGYmIK5wIM/image.png">image.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/0PygcTIJuTOeZlzGYmIK5wIM/image.png" title="image.png"><img data-original-content-type="image/png" data-original-dimensions="640x600" src="/user_uploads/thumbnail/21317/0PygcTIJuTOeZlzGYmIK5wIM/image.png/840x560.webp"></a></div><p>I agree that this state of affairs tarnishes the reputation of category theory/ists and I think there is only one way to nip the problem in the bud, that is taking responsibility and vehemently assert that "yeah, no, we do not recognize this shit as category theory or even as decent math"</p>



<a name="523930411"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523930411" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Chad Nester <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523930411">(Jun 13 2025 at 11:53)</a>:</h4>
<p>I'm not sure this kind of "negative curation", in which we maintain lists of things that are <em>bad</em>, is the way to go. </p>
<p>In an ideal world the function of journals is to be lists of things that are <em>good</em>, or at least probably not bad.</p>



<a name="523933583"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523933583" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523933583">(Jun 13 2025 at 12:13)</a>:</h4>
<p>One option is to make sure all these dodgy AI-generated papers have comments on PubPeer. See eg <a href="https://www.pubpeer.com/publications/D52D1CC22593701472A83CFB9C2FD8">https://www.pubpeer.com/publications/D52D1CC22593701472A83CFB9C2FD8</a> If the obvious red flags are documented here, then a list of links can be curated in a place category theorists have control over, or sent to arXiv admins, or employers of people making this nonsense.</p>



<a name="523951140"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523951140" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523951140">(Jun 13 2025 at 13:52)</a>:</h4>
<p><span class="user-mention silent" data-user-id="690871">Chad Nester</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523930411">said</a>:</p>
<blockquote>
<p>I'm not sure this kind of "negative curation", in which we maintain lists of things that are <em>bad</em>, is the way to go. <br>
</p>
</blockquote>
<p>History disagrees <a href="https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum">https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum</a> lists of things that are bad can be used to repress heresy.</p>



<a name="523951855"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/523951855" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#523951855">(Jun 13 2025 at 13:56)</a>:</h4>
<p><span class="user-mention silent" data-user-id="282822">fosco</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523951140">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="690871">Chad Nester</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523930411">said</a>:</p>
<blockquote>
<p>I'm not sure this kind of "negative curation", in which we maintain lists of things that are <em>bad</em>, is the way to go. <br>
</p>
</blockquote>
<p>History disagrees <a href="https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum">https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum</a> lists of things that are bad can be used to repress heresy.</p>
</blockquote>
<p><em>Index Paperorum Crackpoti</em></p>



<a name="524110845"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/524110845" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Chad Nester <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#524110845">(Jun 15 2025 at 07:18)</a>:</h4>
<p><span class="user-mention silent" data-user-id="282822">fosco</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523951140">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="690871">Chad Nester</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/523930411">said</a>:</p>
<blockquote>
<p>I'm not sure this kind of "negative curation", in which we maintain lists of things that are <em>bad</em>, is the way to go. <br>
</p>
</blockquote>
<p>History disagrees <a href="https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum">https://en.wikipedia.org/wiki/Index_Librorum_Prohibitorum</a> lists of things that are bad can be used to repress heresy.</p>
</blockquote>
<p>An inquisition would, at least, be entertaining :)</p>



<a name="524653576"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/524653576" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#524653576">(Jun 18 2025 at 08:23)</a>:</h4>
<p>A <a href="https://arxiv.org/abs/2506.14537">new paper</a> in <a href="http://quant.ph">quant.ph</a>  supposedly connecting modular tensor categories to quantum contextuality smells like LLM to me.</p>



<a name="524660144"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/524660144" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#524660144">(Jun 18 2025 at 09:02)</a>:</h4>
<p>Pages 7 and 8 definitely look like over-optimistic generalities of dot points. And the 'proof' here is altogether lacking in convincing detail in the last two sentences....</p>
<blockquote>
<p><strong>Proposition 4.3.</strong> The braid group representation <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ρ</mi><mi>F</mi></msub></mrow><annotation encoding="application/x-tex">\rho_F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">ρ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> derived from the Fibonacci category violates the KCBS inequality maximally, demonstrating strong contextuality intrinsic to its topological structure.  <br>
<strong>Proof.</strong> Projectors onto fusion basis states corresponding to the object <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span></span></span></span> generate a measurement scenario isomorphic to the pentagon graph underlying the KCBS inequality [9, 11]. The noncommuting braid generators create measurement contexts whose statistical correlations surpass classical bounds. Numerical evaluation of expectation values using explicit ρF matrices confirms maximal violation</p>
</blockquote>



<a name="524661319"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/524661319" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#524661319">(Jun 18 2025 at 09:08)</a>:</h4>
<p>It's just verbiage, with the convenient out that "numerical evaluation" will bear out the claim.    In other words, bullshit.</p>



<a name="526313843"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526313843" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526313843">(Jun 30 2025 at 03:26)</a>:</h4>
<p>Another AI-slop paper from Reizi:</p>
<p><a href="https://arxiv.org/abs/2506.21653">https://arxiv.org/abs/2506.21653</a></p>
<p>Primary subject this time math.LO not math.CT. Also, name changed from Barreto Joaquim Reiz to Higuchi Joaquim Reizi. The formatting is also weirdly broken, with line numbers appearing inconsistently...</p>



<a name="526313964"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526313964" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526313964">(Jun 30 2025 at 03:29)</a>:</h4>
<p>Identical submission email, this person needs to be put on a special watch-list.</p>



<a name="526318854"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526318854" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526318854">(Jun 30 2025 at 04:43)</a>:</h4>
<p>Have you considered emailing folks at the arXiv, where these suggestions can have some effect?</p>



<a name="526319139"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526319139" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526319139">(Jun 30 2025 at 04:47)</a>:</h4>
<p>I'm working on that, too, through a more senior person in the logic community, behind the scenes. I'm just cataloguing them here for the benefit of people who might see it and waste the time looking at it (though this one is pretty blatant). I can stop if it's too much noise.</p>



<a name="526319398"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526319398" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526319398">(Jun 30 2025 at 04:51)</a>:</h4>
<p>It's not too much noise; people can always mute this thread if they want.  I'm just glad you're trying to actually do something about this.</p>



<a name="526328702"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526328702" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526328702">(Jun 30 2025 at 06:38)</a>:</h4>
<p>Someone pointed me at the arXiv moderation contact form, I put my case to them.</p>



<a name="526331329"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526331329" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526331329">(Jun 30 2025 at 07:00)</a>:</h4>
<p>Thanks, yes that's an easy way to contact the moderators.  Since you don't sound like a crackpot, they should take you seriously, though action may be slow, and almost surely near-silent.</p>



<a name="526331547"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526331547" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526331547">(Jun 30 2025 at 07:02)</a>:</h4>
<p>E.g. I asked them whether they had an international backup of the arXiv, and they never replied, but <a href="https://mathstodon.xyz/@johncarlosbaez/114752342143968527">now they have one</a>.</p>
<p>(I'm not claiming I caused this, but it was an obvious thing to want so I'm glad they have it now.)</p>



<a name="526532165"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526532165" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526532165">(Jul 01 2025 at 06:22)</a>:</h4>
<p>Number of days since an AI-generated slop article made it to <code>math.LO</code>: zero. Again.</p>



<a name="526534815"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526534815" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526534815">(Jul 01 2025 at 06:43)</a>:</h4>
<p>Yes. Also a repeat offender. But also it's going to math.FA, and not even cross-listed to math.CT, despite the title and the topic. </p>
<p>I encourage people to use the 'arXiv moderation user support' contact link here: <a href="https://arxiv-org.atlassian.net/servicedesk/customer/portal/2">https://arxiv-org.atlassian.net/servicedesk/customer/portal/2</a> and let the moderators know about the suspect paper(s) that turn up. Be specific in your report as to what makes you think it is LLM-generated, with examples from the paper that no human would write, if possible.</p>



<a name="526551621"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526551621" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526551621">(Jul 01 2025 at 08:20)</a>:</h4>
<p><span class="user-mention silent" data-user-id="510824">Zoltan A. Kocsis (Z.A.K.)</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/526532165">said</a>:</p>
<blockquote>
<p>Number of days since an AI-generated slop article made it to <code>math.LO</code>: zero. Again.</p>
</blockquote>
<p>"Submitted to Inventiones Mathematicae"</p>
<p>LOL</p>



<a name="526556277"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526556277" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Amar Hadzihasanovic <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526556277">(Jul 01 2025 at 08:43)</a>:</h4>
<p>It seems like the author is picking a different primary classification for every paper</p>



<a name="526556355"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526556355" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Amar Hadzihasanovic <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526556355">(Jul 01 2025 at 08:44)</a>:</h4>
<p>First one was math.CT, second cs.LO, third math.RT, fourth math.FA</p>



<a name="526556420"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526556420" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Amar Hadzihasanovic <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526556420">(Jul 01 2025 at 08:44)</a>:</h4>
<p>Looks like it could be a conscious effort to avoid moderation</p>



<a name="526914105"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526914105" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526914105">(Jul 03 2025 at 04:08)</a>:</h4>
<p><span class="user-mention silent" data-user-id="510824">Zoltan A. Kocsis (Z.A.K.)</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/526532165">said</a>:</p>
<blockquote>
<p>Number of days since an AI-generated slop article made it to <code>math.LO</code>: zero. Again.</p>
</blockquote>
<p>And again zero :(</p>



<a name="526914438"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526914438" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526914438">(Jul 03 2025 at 04:13)</a>:</h4>
<p>I wonder what could be a community Plan B if the arXiv moderators are unable to cope with this wave and the arXiv becomes viXraised. ArXiv overlays with additional community filters?</p>



<a name="526914722"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526914722" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526914722">(Jul 03 2025 at 04:18)</a>:</h4>
<p>In particular, it doesn't look like arXiv's "get endorsement or academic-email" system can be tightened any further without causing undue difficulty to regular academics who want to post genuine preprints.</p>



<a name="526918572"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/526918572" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#526918572">(Jul 03 2025 at 05:09)</a>:</h4>
<p><span class="user-mention" data-user-id="510824">@Zoltan A. Kocsis (Z.A.K.)</span> Is this about "systemic contraints"? ;-) The one graphic in that paper looks like a typical not-that-good LLM trying to make a technical image.</p>



<a name="527444264"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/527444264" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#527444264">(Jul 07 2025 at 10:06)</a>:</h4>
<p>uhm would a webpage with a list of 'suspected slop' preprints be too strong of a reaction to this phenomenon? I'd be willing to setup that, and have people submit me entries</p>



<a name="527463343"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/527463343" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Chad Nester <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#527463343">(Jul 07 2025 at 12:01)</a>:</h4>
<p>"The Wall of Shame"</p>



<a name="527513258"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/527513258" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Morgan Rogers (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#527513258">(Jul 07 2025 at 16:12)</a>:</h4>
<p>Sounds good to me <span class="user-mention" data-user-id="275932">@Matteo Capucci (he/him)</span></p>



<a name="527514058"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/527514058" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#527514058">(Jul 07 2025 at 16:16)</a>:</h4>
<p>By the way, it's wise to be quite polite and cautious in your public description of this web page, to reduce your chance of getting sued and/or harrassed.   Having gotten threats from people I criticized publicly, I can assure you it's not much fun.</p>



<a name="527604656"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/527604656" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#527604656">(Jul 08 2025 at 05:02)</a>:</h4>
<p>If it's not wholly AI-generated, it's at least got lots of LLM fingerprints all over the formatting and structure <a href="https://arxiv.org/abs/2507.04089">https://arxiv.org/abs/2507.04089</a></p>



<a name="529199942"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/529199942" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#529199942">(Jul 17 2025 at 04:22)</a>:</h4>
<p><a href="https://arxiv.org/search/math?searchtype=author&amp;query=Hajebi,+P">https://arxiv.org/search/math?searchtype=author&amp;query=Hajebi,+P</a> :-(</p>



<a name="531552419"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531552419" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Peva Blanchard <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531552419">(Jul 29 2025 at 09:01)</a>:</h4>
<p>Not an AI-generated paper, but I just stumble on a AI-generated <a href="https://www.numberanalytics.com/blog/ultimate-guide-fibration-category-theory">blog article about fibrations</a>. (At the very least, the website is quite honest since it reveals that the author is Llama-4)</p>



<a name="531586258"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531586258" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531586258">(Jul 29 2025 at 11:42)</a>:</h4>
<p>It's amusing how the definition of fibration is wrong.   Also funny how the grammar is frequently wrong in the same way:</p>
<blockquote>
<p>Fibration is a fundamental concept</p>
</blockquote>
<blockquote>
<p>Relationship between Fibration and Other Category Theory Concepts</p>
</blockquote>
<blockquote>
<p>Fibration is closely related to</p>
</blockquote>
<p>etc.</p>
<p>Of course I have to be amused, because otherwise I'd break down and cry about how the pool of human knowledge is getting contaminated by sludge like this.</p>
<p><span aria-label="poop" class="emoji emoji-1f4a9" role="img" title="poop">:poop:</span></p>



<a name="531589210"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531589210" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531589210">(Jul 29 2025 at 11:58)</a>:</h4>
<p>I would guess the places where grammar is funny is where the human used search+replace to prepare the prompts to write the pages. This is all mass produced, it's quite harrowing.</p>



<a name="531672374"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531672374" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531672374">(Jul 29 2025 at 18:14)</a>:</h4>
<p>I'd like to think all this AI slop will drive the value of expertise upward; after all, now the world not only needs John to write blog posts, but to correct AI generated slop posts too.</p>



<a name="531692599"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531692599" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531692599">(Jul 29 2025 at 20:23)</a>:</h4>
<p>If the world needs me to do that, the world is in deep trouble.</p>



<a name="531748666"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531748666" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531748666">(Jul 30 2025 at 05:02)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531586258">said</a>:</p>
<blockquote>
<p>Of course I have to be amused, because otherwise I'd break down and cry about how the pool of human knowledge is getting contaminated by sludge like this.</p>
</blockquote>
<p>This prompted me to wonder (somewhat fancifully) whether we could create a parallel humans-only Internet.  Then we could cede the current Internet to the AIs, who would eventually implode due to model collapse.</p>
<p>At the very least, I'm thinking seriously about not posting new preprints on the arXiv any more, or any other public site from which they could be scraped to train AIs to generate mathematical-sounding slop.  Surely there'd still be some way to make them freely accessible to humans.</p>



<a name="531767420"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531767420" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531767420">(Jul 30 2025 at 07:23)</a>:</h4>
<p>And I suppose the same should apply to blog posts.</p>



<a name="531768321"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531768321" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531768321">(Jul 30 2025 at 07:28)</a>:</h4>
<p>You have to both figure out some this-is-a-real-human verification method, which probably means some kind of biometrics, and also some method of preventing anybody from just downloading the human-only Internet and feeding it to the bots offline...The first one is solvable but invasive but I'm really not sure how to do the second. DRM for every file on your Internet? Bleh</p>



<a name="531769767"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531769767" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Graham Manuell <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531769767">(Jul 30 2025 at 07:35)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276777">Mike Shulman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531748666">said</a>:</p>
<blockquote>
<p>At the very least, I'm thinking seriously about not posting new preprints on the arXiv any more, or any other public site from which they could be scraped to train AIs to generate mathematical-sounding slop.  Surely there'd still be some way to make them freely accessible to humans.</p>
</blockquote>
<p>I don't understand how this helps anyone? AI will still be used to output nonsense whether or not it is trained on your specific papers. It will also be trained on any papers  you publish in journals in any case. All you would be doing is making it harder for humans to access your articles.</p>



<a name="531773951"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531773951" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Peva Blanchard <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531773951">(Jul 30 2025 at 07:57)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531768321">said</a>:</p>
<blockquote>
<p>You have to both figure out some this-is-a-real-human verification method, which probably means some kind of biometrics, and also some method of preventing anybody from just downloading the human-only Internet and feeding it to the bots offline...The first one is solvable but invasive but I'm really not sure how to do the second. DRM for every file on your Internet? Bleh</p>
</blockquote>
<p>I don't know for the second problem. The first problem (distinguishing humans from bots) is indeed already an issue. (e.g., if I remember correctly, Facebook removes billions fake accounts every year). Some people in the cryptography/privacy world work on that, something along the lines of proving that you hold a state-issued ID card without revealing the details (using zero-knowledge proofs).</p>
<p>I guess this will probably trigger an arms race between "human-detectors" and "human-provers".</p>



<a name="531774387"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531774387" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531774387">(Jul 30 2025 at 07:59)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531768321">said</a>:</p>
<blockquote>
<p>also some method of preventing anybody from just downloading the human-only Internet and feeding it to the bots offline</p>
</blockquote>
<p>I was imagining that human users would be constantly verified (however that would work) whenever they access an individual document, so they couldn't just log in once and then click "download the Internet" and get it all.</p>
<p>I did say it was  fanciful.  But if the alternative is ceding the Internet to the AIs and having nothing to replace it with, maybe we should be working harder on it.</p>



<a name="531774647"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531774647" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531774647">(Jul 30 2025 at 08:00)</a>:</h4>
<p>Yes, I'm pretty sympathetic.</p>



<a name="531777576"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531777576" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531777576">(Jul 30 2025 at 08:12)</a>:</h4>
<p><span class="user-mention silent" data-user-id="386922">Graham Manuell</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531769767">said</a>:</p>
<blockquote>
<p>AI will still be used to output nonsense whether or not it is trained on your specific papers.</p>
</blockquote>
<p>It's like voting, or reducing your carbon footprint.  Anything any individual person does has a miniscule effect on the world, but the world is made up of individuals, so we should all follow the categorical imperative.</p>
<p>But I suppose you're right that currently we have no technical solution for disseminating information to humans only, and if we want to stay in this job we have to disseminate our research in some way.  I guess we can hope that the pending copyright lawsuits against AI trainers bear some fruit...</p>



<a name="531778548"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531778548" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Peva Blanchard <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531778548">(Jul 30 2025 at 08:17)</a>:</h4>
<p>There is another line of research focusing on "voluntarily poisoning" your data, so that an AI trained on a dataset including your data could be flagged. See e.g. <a href="https://arxiv.org/abs/2410.09101">this paper</a>.</p>



<a name="531779722"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531779722" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531779722">(Jul 30 2025 at 08:23)</a>:</h4>
<p>That's a nice idea.  I don't suppose it's possible for those of us who don't work with data?</p>



<a name="531790513"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531790513" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531790513">(Jul 30 2025 at 09:14)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276777">Mike Shulman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531777576">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="386922">Graham Manuell</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531769767">said</a>:</p>
<blockquote>
<p>AI will still be used to output nonsense whether or not it is trained on your specific papers.</p>
</blockquote>
<p>It's like voting, or reducing your carbon footprint.  Anything any individual person does has a miniscule effect on the world, but the world is made up of individuals, so we should all follow the categorical imperative.</p>
</blockquote>
<p>That's why we do category theory!   <span aria-label="upside down" class="emoji emoji-1f643" role="img" title="upside down">:upside_down:</span> </p>
<p>I do a lot of things that seem a bit quixotic in that they have a miniscule effect.  But I wouldn't stop posting my papers to the arXiv because the positive effect of spreading my ideas to more human mathematicians seems to grossly outweigh the negative effect due to AIs reading them.   </p>
<p>One slightly quixotic act I've been enjoying is <em>not using Google</em> and instead paying to use a search engine called <a href="https://kagi.com/">Kagi</a> which doesn't show me advertisements, doesn't rely on ad revenue, isn't as susceptible to search engine optimization tricks, doesn't push AI on me, and has a greater variety of search filters, like an "academic" filter.   I heard about this from <a href="https://pluralistic.net/2025/07/28/twiddlehazard/#outboard-brains-considered-harmful">Cory Doctorow</a>, who has very interesting things to say about enshittification (a term he invented).</p>



<a name="531790919"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531790919" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531790919">(Jul 30 2025 at 09:16)</a>:</h4>
<p>I have little hope that the deluge of AI-generated content will abate. However, it seems more realistic to me that a human verification process could be used to whitelist authors who are known not to be bad actors. arXiv's current verification process is currently entirely insufficient, but I feel it can be addressed if arXiv actually take action. However, a severe disadvantage of this is that it makes academia even less accessible to those outside of it than it already is (because most likely the only entrypoint to verification would be via a verified user/institution).</p>



<a name="531791352"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531791352" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531791352">(Jul 30 2025 at 09:18)</a>:</h4>
<p>In some sense, I feel it doesn't matter if huge amounts of AI slop is generated (distasteful as it is), so long as it's possible to filter out. In this case, I think that only permitting trusted users to post research is more important than only permitting trusted users to view research.</p>



<a name="531793942"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531793942" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531793942">(Jul 30 2025 at 09:31)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531790513">said</a>:</p>
<blockquote>
<p>One slightly quixotic act I've been enjoying is <em>not using Google</em> and instead paying to use a search engine called <a href="https://kagi.com/">Kagi</a> which doesn't show me advertisements, doesn't rely on ad revenue, isn't as susceptible to search engine optimization tricks, doesn't push AI on me, and has a greater variety of search filters, like an "academic" filter.</p>
</blockquote>
<p>That's very interesting!  I see that they do also supply an AI, which seems to contradict their goal of "humanizing the web", but I gather from your remarks that you can turn it off.  How does Kagi compare to Google with <a href="https://udm14.com/">udm14</a>, which disables ads and AI?</p>



<a name="531826666"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531826666" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Peva Blanchard <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531826666">(Jul 30 2025 at 12:11)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276777">Mike Shulman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531779722">said</a>:</p>
<blockquote>
<p>That's a nice idea.  I don't suppose it's possible for those of us who don't work with data?</p>
</blockquote>
<p>In principle, the idea works on a vectorial representation of the data, thus should be applicable to text. However, text is more complicated in practice because the mapping "text <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> vector" is less flexible than, e.g., "image <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> vector", so the poison is harder to craft.</p>
<p>Also, since an individual author only provides a "few" samples, I don't know how relevant the technique can be for individual usage. I expect journals or any document archive to be more likely to be "clients" of this approach. Anyway, this is still research, so there is no off-the-shelf software/service available for now, as far as I know.</p>



<a name="531837180"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531837180" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531837180">(Jul 30 2025 at 13:00)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276777">Mike Shulman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/531793942">said</a>:</p>
<blockquote>
<p>That's very interesting!  I see that they do also supply an AI, which seems to contradict their goal of "humanizing the web", but I gather from your remarks that you can turn it off. </p>
</blockquote>
<p>I must have turned it off immediately, because I never see it.</p>
<blockquote>
<p>How does Kagi compare to Google with <a href="https://udm14.com/">udm14</a>, which disables ads and AI?</p>
</blockquote>
<p>I'll have to compare them for a while.  So far they look comparable except udm14 doesn't have those various filter settings.   Kagi claims to have boolean search but it seems to be working erratically - maybe I'm not using it right.   It also claims you can search by "least relevant first", which is hilarious.</p>



<a name="531837805"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/531837805" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#531837805">(Jul 30 2025 at 13:03)</a>:</h4>
<p>Here's Cory Doctorow on Google and related things.    (It's long, but folks can skip it if they don't care.)</p>
<blockquote>
<p>That's where Ardoline and Lenzo's work comes in. They both document the ways in which we turn these online services into cognitive prostheses, and then investigate how the enshittification of these services ends up making us stupider, by taking away the stuff that helps us think. They're drawing a line between platform decay and cognitive decay.</p>
<p>The authors look at examples like the enshittification of Google Search, a product that Google has deliberately and irretrievably enshittified:</p>
<p><a href="https://pluralistic.net/2024/04/24/naming-names/#prabhakar-raghavan">https://pluralistic.net/2024/04/24/naming-names/#prabhakar-raghavan</a></p>
<p>The web is a giant cognitive prosthesis, and early web tools put a lot of emphasis on things like bookmark management and local caching, so that the knowledge and cognition you externalized to the web were under your control. But Google Search was so goddamned magic – before they cynically destroyed it – that a lot of us switched from "not remembering things because you have a bookmark that takes you to a website that remembers it for you" to "not remembering things and not remembering where to find them, and just typing queries into Google." The collapse of Google into a giant pile of shit is like giving every web user a traumatic brain injury.</p>
<p>It's a good paper, but I think the situation is actually more dire than the paper makes it out to be, thanks to the AI bubble –</p>
<p>Wait! I'm not actually going to talk about what AI can do (which is a combination of a small set of boring useful things, a bunch of novelties, and a long list of things that AI can't do but is being used to do anyway). I'm talking about the financial fraud that AI serves.</p>
<p>Tech companies <em>must</em> be perceived as growing, because when a company is growing, it is valued <em>far</em> more highly than a company is once it has "matured." This is called the "price to earnings ratio" – the number of dollars investors are willing to pay for the company compared to the number of dollars a company is bringing in. So long as a company is growing, the PE ratio is very high, and this helps the company to <em>actually</em> grow. That's because the shares in growing companies are highly liquid, and can be traded for equity in other companies and/or the labor of key employees, meaning that growth companies can almost always outbid their mature counterparts when it comes to expanding through acquisition and hiring. That means that while a company is growing, its PE ratio can help it <em>keep</em> growing.</p>
<p>But here's the corollary: when a growth company <em>stops</em> growing, its shares are suddenly and violently revalued as though they were shares in a mature company, which tanks the personal net worth of the company's top managers and key employees (whose portfolios are stuffed with their employer's now-plummeting stock). Worse: in order to retain those employees and hire more (or to acquire key companies), the no-longer-growing company has to pay with cash, which is <em>much</em> harder to get than its own shares. Even worse: they have to bid against <em>growing</em> companies.</p>
<p>A growth company is like an airplane that has two modes: climbing and nose-diving, and while it's easy to go from climbing to crashing, it's <em>much</em> harder to go the other way. Ironically, the moment at which a company's growth is most likely to stall is right after its greatest triumph: after a company conquers its market, it has nowhere else to go. Google's got a 90% Search market-share – how can it possibly grow Search?</p>
<p>It can't (just like Meta can't really grow social, and Microsoft can't grow office suites, etc), so it has to convince Wall Street that it has a shot at conquering some <em>other</em> market that the street perceives as unimaginably vast and thus capable of keeping the growth engine going. Tech has pulled a lot of sweaty tricks to create this impression, inflating bubbles like "pivot to video" and "metaverse" and "cryptocurrency," and now it's AI.</p>
<p>The problem is that AI just isn't very popular. People go out of their way to avoid AI products:</p>
<p><a href="https://www.tandfonline.com/doi/full/10.1080/19368623.2024.2368040">https://www.tandfonline.com/doi/full/10.1080/19368623.2024.2368040</a></p>
<p>For an AI-driven growth story to work, tech companies have to produce a stream of charts depicting lines that go up and to the right, reflecting some carefully chosen set of metrics demonstrating AI's increasing popularity. One way to produce these increasing trend-lines on demand is to replace all the most commonly used parts of a service that you love and rely on with buttons that summon an AI. This is the "fatfinger AI economy," a set of trendlines produced by bombarding people who graze their screens with a stray fingertip with a bunch of AI bullshit, so you can claim that your users are "engaging" with AI:</p>
<p><a href="https://pluralistic.net/2025/05/02/kpis-off/#principal-agentic-ai-problem">https://pluralistic.net/2025/05/02/kpis-off/#principal-agentic-ai-problem</a></p>
<p>It's a form of "twiddling" – changing how a service works on a per-user, per-interaction basis in order to shift value from the user to the company:</p>
<p><a href="https://pluralistic.net/2023/02/19/twiddler/">https://pluralistic.net/2023/02/19/twiddler/</a></p>
<p>Twiddling represents <em>the</em> big cognitive hazard from enshittification during the AI bubble: the parts of your UI that matter most to you are the parts that you use as vital cognitive prostheses. A product team whose KPI is "get users to tap on an AI button" is going to use the fine-grained data they have on your technological activities to preferentially target these UI elements that you rely on with AI boobytraps. You are too happy, so they are leaving money on the table, and they're coming for it.</p>
<p>This is a form of "attention rent": the companies are taxing your muscle-memory, forcing you to produce deceptive usage statistics at the price of either diverting your cognition from completing a task to hunt around for the button that banishes the AI and lets you get back to what you were doing; or to simply abandon that cognitive prosthesis:</p>
<p><a href="https://pluralistic.net/2023/11/03/subprime-attention-rent-crisis/#euthanize-rentiers">https://pluralistic.net/2023/11/03/subprime-attention-rent-crisis/#euthanize-rentiers</a></p>
<p>It's <em>true</em> "engagement-hacking": not performing acts of dopamine manipulation; but rather, spying on your habitual usage of a digital tool in order to swap buttons around in order to get you to make a number go up. It's exploiting the fact that you engage with something useful and good to make it less useful and worse, because if you're too happy, some enshittifier is leaving money on the table.</p>
</blockquote>



<a name="532396447"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532396447" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ben Kaminsky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532396447">(Aug 02 2025 at 00:19)</a>:</h4>
<p>I think there is a new danger in a kind of "crank singularity" happening.</p>
<p>I'll admit I am not an experienced crankologist with decades under my belt like Dr. Baez, but I've noticed there is a stark difference in two different classes of cranks. I often stop by reddit and view the local "alt-physics" subreddits. I refer to these as "crank aquariums".</p>
<p>The "lower cranks" are mostly mystical, don't know much math and talk about the typical "woo" topics. Consciousness collapses the wavefunction, sacred geometry, you know the drill.</p>
<p>The "higher cranks" often use very sophisticated math to prove "new theorems". But their math is PDE heavy and fundamentally brittle. They discover "new terms" that Maxwell and Schrodinger "forgot" in their equations. They never use things like category theory, homology, moduli spaces, Lie groups, etc. They only do very heavy analysis.</p>
<p>The problem is that with new LLMs, these two separate classes of cranks could merge into a new form of "hybridized super-crank" generating endless reams of "self-conscious quantum operator algebras" and "quasi-cosmic graviton quantum field theories" with actual PDEs that are potentially sophisticated enough to overwhelm hapless journal editors.</p>
<p>Maybe there could be an additional filter (Category Theory CAPCHA?) of some kind where people hoping to publish could go into front of an AI interviewer and answer randomly generated questions about group theory, topology, cohomology classes, functors and sheaves. If you pass you receive a badge of some kind (this used to be referred to as a "degree" I believe?) These kinds of topics are usually too abstract for cranks to actually understand, so it could be a "mental block".</p>
<p>Anyway, I'm partly spitballing here, but the true lesson to remember is that mathematics chases deep structure, whereas cranks may only imitate it shallowly. (I am serious about that, but I felt <em>some</em> comic relief is also in order here. I hope the mods won't exile me to the crank aquarium!)</p>



<a name="532417410"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532417410" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532417410">(Aug 02 2025 at 05:07)</a>:</h4>
<p>I'll add my piece of comic relief. There's a certain resistance in flagging some content as AI generated slop, for a similar reason that people are very wary of flagging a text as plagiarism... No problem, I can do it <span aria-label="smiling devil" class="emoji emoji-1f608" role="img" title="smiling devil">:smiling_devil:</span> I have a certain experience and it's very pleasurable for me to tell someone who deserves it "shut up and com me back when you know what a determinant is"</p>



<a name="532431169"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532431169" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532431169">(Aug 02 2025 at 08:22)</a>:</h4>
<p><span class="user-mention silent" data-user-id="930833">Ben Kaminsky</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532396447">said</a>:</p>
<blockquote>
<p>I think there is a new danger in a kind of "crank singularity" happening.</p>
</blockquote>
<p>It's happening.  I'm getting <em>many</em> more emails from cranks, who are mostly working with LLMs to develop their 'theories'.   A quote from one of these emails:</p>
<p>"developed rigorously with the help of large language models"</p>
<p><span aria-label="rolling eyes" class="emoji emoji-1f644" role="img" title="rolling eyes">:rolling_eyes:</span> </p>
<blockquote>
<p>The "higher cranks" often use very sophisticated math to prove "new theorems". But their math is PDE heavy and fundamentally brittle. </p>
</blockquote>
<p>I haven't seen any cranks using very sophisticated math.  Some <em>pretend</em> to do so.   And I agree with you that the number is dramatically increasing now that fake math is easy to get from a LLM.  Luckily anyone who really knows math can see this stuff is fake.</p>



<a name="532480521"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532480521" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532480521">(Aug 02 2025 at 19:13)</a>:</h4>
<p>maybe this has been brought up before, but what about adding "community notes" to the arxiv, like how it works on twitter?  That's how twitter was able to pass moderation responsibility onto the public at scale.</p>



<a name="532548885"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532548885" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532548885">(Aug 03 2025 at 11:53)</a>:</h4>
<p>This is a dangerous plan, for several reasons that people noticed about 15 minutes after first thinking of this idea a couple of decades ago.  Some still favor it, but the arXiv moderators aren't going to take those chances.</p>



<a name="532548959"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532548959" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532548959">(Aug 03 2025 at 11:54)</a>:</h4>
<p>Note however that anyone can start their own "arXiv reviews".</p>



<a name="532588555"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532588555" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532588555">(Aug 03 2025 at 19:00)</a>:</h4>
<p>just out of curiosity, why would it not work for the arxiv (what are those reasons?) if it does work for twitter (or maybe it doesn't work for twitter?)?  the only thing I can think of off the top of my head is that there is too small of a "public of experts" for the community notes to be accurate</p>



<a name="532593382"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532593382" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532593382">(Aug 03 2025 at 19:56)</a>:</h4>
<p>My impression is that it's at least questionable whether it works for twitter.</p>



<a name="532593399"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532593399" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532593399">(Aug 03 2025 at 19:56)</a>:</h4>
<p>Although I don't use twitter myself so I can't say of personal experience.</p>



<a name="532593403"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532593403" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Evan Patterson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532593403">(Aug 03 2025 at 19:57)</a>:</h4>
<p>I'm also curious. Though I'm no longer on X/Twitter, my impression is that community notes continued to function quite well even as the platform deteriorated in other respects, pretty reliably flagging false or misleading posts.</p>



<a name="532594434"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532594434" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532594434">(Aug 03 2025 at 20:09)</a>:</h4>
<p>Imagine if everyone got to say shit about each other's papers on the arXiv.  It would be a bloodbath.  It would quickly degenerate into obscenities and lawsuits unless the comments were moderated.  Some of those lawsuits would even target the arXiv itself.  <em>All</em> of this is the last thing the arXiv moderators want.  They don't have time for this.</p>



<a name="532595032"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532595032" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Evan Patterson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532595032">(Aug 03 2025 at 20:16)</a>:</h4>
<p>To be successful, such an approach would need to carefully circumscribe the allowed claims for a community note. The allowed claims would <em>not</em> include random opinions about or reviews of papers. Rather, the point would be to use the community to reach consensus on matters of fact, such as:</p>
<ul>
<li>was this paper largely/completely generated by an LLM?</li>
<li>does this paper plagiarize from existing sources?</li>
<li>is this paper a "crank" paper, in the specific sense that it fails to conform to widely shared norms about mathematical/scientific communication?</li>
</ul>



<a name="532599213"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532599213" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532599213">(Aug 03 2025 at 21:02)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275965">Evan Patterson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532595032">said</a>:</p>
<blockquote>
<p>such an approach would need to carefully circumscribe the allowed claims for a community note</p>
</blockquote>
<p>To enforce that, all the comments would have to be moderated, right?</p>



<a name="532600252"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532600252" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532600252">(Aug 03 2025 at 21:13)</a>:</h4>
<p>The arXiv staff might prefer to spend their limited time/energy/money on moderating papers rather than moderating comments on papers.</p>
<p>I suppose one approach to avoid moderating comments would be a form which allowed no freely written text, just yes/no answers to questions like "this paper is AI-generated".   This would have its own problems.</p>



<a name="532601705"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532601705" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532601705">(Aug 03 2025 at 21:29)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532600252">said</a>:</p>
<blockquote>
<p>The arXiv staff might prefer to spend their limited time/energy/money on moderating papers</p>
</blockquote>
<p>And apparently they don't even have enough time/energy/money to do a good enough job of that, which is what led to this whole conversation.  (Not intended as a criticism of them, just an observation about lack of resources.)</p>



<a name="532601852"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532601852" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532601852">(Aug 03 2025 at 21:31)</a>:</h4>
<p>I don't suppose any agency would be likely to award a grant to support work to keep AI-generated slop off of the arXiv.</p>



<a name="532603451"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532603451" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532603451">(Aug 03 2025 at 21:50)</a>:</h4>
<p>here's how it works on twitter:  The Community Notes algorithm publishes notes based on agreement from contributors who have a history of disagreeing.[21] Rather than based on majority rule,[34] the program's algorithm prioritizes notes that receive ratings from a "diverse range of perspectives".[28][35] For a note to be published, a contributor must first propose a note under a tweet.[21] The program assigns different values to contributors' ratings, categorising users with similar rating histories as a form of "opinion classification", determined by a vague alignment with the left and right-wing political spectrum. The bridging-based machine-learning algorithm requires ratings from both sides of the spectrum in order to publish notes, that can have the intended effect of decreasing interaction with such content.[35][36][37]</p>
<p>Contributors are volunteers with access to an interface from which they have the ability to monitor tweets and replies that may be misleading.[21][9][38] Notes in need of ratings by contributors are located under a "Needs your help" section of the interface. Other contributors then give their opinion on the usefulness of the note, identifying notes as "Helpful" or "Not Helpful".[21][39] The contributor gets points if their note is validated,[40][21] known as "Rating Impact", that reflects how helpful a contributors' ratings have been.[39][41][42] X users are able to vote on whether they find notes helpful or not,[18] but must apply to become contributors in order to write notes, the latter being restricted by "Rating Impact" as well as the Community Notes guidelines.[39][41]</p>



<a name="532610646"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532610646" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532610646">(Aug 03 2025 at 23:14)</a>:</h4>
<p>OK, so how many people in the world do you think a) will have a legitmate informed opinion on a somewhat niche research (sub)field and b) will engage in the commenting process on arXiv papers? Twitter notes worked (or "worked") because of scale. If you have a hundred thousand people, a million people, engaging on a topic that doesn't require PhD-level education to understand even the words, then this type of approach might achieve some level of community consensus.</p>



<a name="532613451"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532613451" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ruby Khondaker (she/her) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532613451">(Aug 03 2025 at 23:46)</a>:</h4>
<p>This reminds me of the “wisdom of the crowd” phenomenon!</p>



<a name="532631112"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532631112" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532631112">(Aug 04 2025 at 02:48)</a>:</h4>
<p>maybe it would be enough for the experts to be more numerous than the crackpots, rather than needing huge numbers of experts? (hopefully there are more experts than crackpots, but I have no idea tbh). But I suppose the discussion is moot without the arxiv actually doing it.</p>



<a name="532648279"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532648279" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532648279">(Aug 04 2025 at 05:25)</a>:</h4>
<p><a href="https://www.daniellitt.com/blog/2025/7/17/arxiv-in-trouble">https://www.daniellitt.com/blog/2025/7/17/arxiv-in-trouble</a></p>



<a name="532650306"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532650306" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ruby Khondaker (she/her) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532650306">(Aug 04 2025 at 05:50)</a>:</h4>
<p><span class="user-mention silent" data-user-id="281326">Ryan Wisnesky</span> <a href="#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/532631112">said</a>:</p>
<blockquote>
<p>maybe it would be enough for the experts to be more numerous than the crackpots, rather than needing huge numbers of experts? (hopefully there are more experts than crackpots, but I have no idea tbh). But I suppose the discussion is moot without the arxiv actually doing it.</p>
</blockquote>
<p>I feel like almost definitionally there would have to be more crackpots than experts, owing to the relative difficulty in becoming either?</p>



<a name="532667301"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532667301" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532667301">(Aug 04 2025 at 07:50)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532594434">said</a>:</p>
<blockquote>
<p>Imagine if everyone got to say shit about each other's papers on the arXiv.  It would be a bloodbath.  It would quickly degenerate into obscenities and lawsuits unless the comments were moderated.</p>
</blockquote>
<p>I think you're too pessimistic John, people tend to be decent 99% of the time, especially if their full name is on display. I agree there would be the need to moderate the remaining 1% though.</p>



<a name="532667322"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532667322" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532667322">(Aug 04 2025 at 07:50)</a>:</h4>
<p><span class="user-mention silent" data-user-id="934428">Ruby Khondaker (she/her)</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532650306">said</a>:</p>
<blockquote>
<p>I feel like almost definitionally there would have to be more crackpots than experts, owing to the relative difficulty in becoming either?</p>
</blockquote>
<p>I assumed the only people allowed to play this game would be people who have been endorsed to write papers on the arXiv.   In this population there are more experts than crackpots.... though not everyone is an expert on every topic, indeed quite the opposite. </p>
<p>If you let random passers-by evaluate arXiv papers, there will definitely be lots of crackpots and people with grudges and other unproductive motivations.</p>



<a name="532667606"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532667606" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532667606">(Aug 04 2025 at 07:52)</a>:</h4>
<p>Yeah of course! And it'd be spamland very quickly...</p>



<a name="532669299"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532669299" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532669299">(Aug 04 2025 at 08:01)</a>:</h4>
<p>I think the whole problem is addressed by fixing the endorsement system. If arXiv is unable to moderate by themselves (which appears to be the case), then they need to either hire more people, or ask for trusted volunteers.</p>



<a name="532675539"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532675539" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532675539">(Aug 04 2025 at 08:33)</a>:</h4>
<p>So much of the academic publishing system already depends upon volunteers, it doesn't feel like a stretch to have people contributing to moderating arXiv. It'd be great to have it become a place of recorded scientific discussions re a piece of work, including errata, reviews, and comments, so that it can also be a starting point for journal reviews.</p>



<a name="532677607"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532677607" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532677607">(Aug 04 2025 at 08:45)</a>:</h4>
<p>The arXiv does a lot of moderating, and it's sometimes too strict: the case of Phillip Helbig comes to mind:</p>
<ul>
<li>Phillip Helbig, <a href="https://johncarlosbaez.wordpress.com/2022/02/04/submission-to-arxiv/">Submission to arXiv</a>, <em>Azimuth</em>.</li>
</ul>
<p>I've had a paper shifted from the group theory section to combinatorics against my will.   I think their problem with AI-generated papers is that they're not used to filtering out papers of this sort.  Most crackpots write in a way that sends off a particular vibe, which is easy for experienced moderators to detect.  But LLMs are different.  I think one can learn to detect them, <em>at least so far</em>.</p>



<a name="532681838"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532681838" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532681838">(Aug 04 2025 at 09:07)</a>:</h4>
<p>In comments awaiting moderation (heh) on Peter Woit's blog that pointed out Daniel Litt's blog post on the topic, I wrote</p>
<blockquote>
<p>I would like to see anyone whose papers were deemed to be AI-generated have all their endorsements stripped, and they should need to get fresh endorsements, probably more than one.</p>
</blockquote>
<blockquote>
<p>Moreover, I would even go so far as to propose that anyone who endorses an AI-generated paper should have their endorser-status reset, so that they cannot immediately re-endorse the person who they originally endorsed, until they have submitted more papers as usual. It would be a bit of an incentive to actually look at the paper for fear of a relatively harmless removal of a privilege. Active researchers would get back to having endorser powers before too long...</p>
</blockquote>



<a name="532687391"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532687391" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532687391">(Aug 04 2025 at 09:34)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/532677607">said</a>:</p>
<blockquote>
<p>The arXiv does a lot of moderating, and it's sometimes too strict</p>
</blockquote>
<p>Clearly not enough (though being sometimes too strict is another problem).</p>



<a name="532690328"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532690328" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532690328">(Aug 04 2025 at 09:50)</a>:</h4>
<p>I don't think a sheer increase in quantity is the best solution, given that the arXiv has an approximately zero budget for doing this.   Moderation needs to be focused on the key problems.  Now that AI is a big problem, the moderators need to be pointed to AI-generated papers, and they need to learn to spot them.  They do look at every paper, I believe.</p>



<a name="532703725"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532703725" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532703725">(Aug 04 2025 at 11:06)</a>:</h4>
<blockquote>
<p>I don't think a sheer increase in quantity is the best solution, given that the arXiv has an approximately zero budget for doing this.</p>
</blockquote>
<p>That would be solved by having volunteers. It would not take many volunteers for each category, and I think this would be relatively easy to achieve, as I think many people would be willing to help.</p>



<a name="532703874"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532703874" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Shulman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532703874">(Aug 04 2025 at 11:07)</a>:</h4>
<p>And now we're back full circle to the question we started with: how do we contact the arXiv moderators and get them to do something?</p>



<a name="532706774"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532706774" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532706774">(Aug 04 2025 at 11:25)</a>:</h4>
<p>Possibly someone could email <a href="mailto:moderators@arxiv.org">moderators@arxiv.org</a> (mentioned on <a href="https://info.arxiv.org/help/moderation/index.html">https://info.arxiv.org/help/moderation/index.html</a>) and see whether they can give any information?</p>



<a name="532707106"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532707106" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532707106">(Aug 04 2025 at 11:28)</a>:</h4>
<p>One can just email them.  The moderators are listed <a href="https://arxiv.org/moderators/">here</a> and the people in charge are listed starting <a href="https://info.arxiv.org/about/people/index.html">here</a>.  </p>
<p>They seem fairly quiet and secretive.   People with problems report getting little response.  When I emailed some of the leaders about the virtues of getting a backup hosted outside the US, they never replied.   They did develop such a backup system.  But I'm not claiming they did that in response to my email.</p>
<p>It probably helps a lot to contact someone in charge whom you know personally.  I could contact Jacques Distler, for example.   I'm betting he'll say they are already familiar with the AI problem.</p>



<a name="532707944"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532707944" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532707944">(Aug 04 2025 at 11:33)</a>:</h4>
<p>I find the lack of any official statement from arXiv on the matter a little disappointing, if they are aware of it.</p>



<a name="532711954"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532711954" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532711954">(Aug 04 2025 at 11:57)</a>:</h4>
<p>They tend to talk as little as possible.</p>



<a name="532781938"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532781938" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532781938">(Aug 04 2025 at 19:04)</a>:</h4>
<p>I'm a smidge nervous that this is getting awfully close to the point of someone like this actually getting a grant from a place like Templeton.</p>



<a name="532859335"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532859335" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532859335">(Aug 05 2025 at 08:10)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/stream/229111-community.3A-general/topic/AI-generated.20papers/near/532711954">said</a>:</p>
<blockquote>
<p>We contend that the result—autoequivalence with the Monster Group—is statistically improbable unless the theory holds validity. Consequently, AI consensus would not have been achieved erroneously.</p>
</blockquote>
<p>My eyes are bleeding <span aria-label="skull" class="emoji emoji-1f480" role="img" title="skull">:skull:</span><span aria-label="skull" class="emoji emoji-1f480" role="img" title="skull">:skull:</span></p>



<a name="532859517"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532859517" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532859517">(Aug 05 2025 at 08:11)</a>:</h4>
<p>'What are the chances I lost the lottery with this very specific ticket? So low it must be the lottery which is wrong'</p>



<a name="532860404"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532860404" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532860404">(Aug 05 2025 at 08:16)</a>:</h4>
<p>Yes, this would be hilarious if it were intended as a joke.    I also like the misuse of "autoequivalence" - which usually means an equivalence of something with <em>itself</em> - in the claim that U-category theory (whatever that is) is autoequivalent with the Monster Group.</p>



<a name="532865604"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532865604" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532865604">(Aug 05 2025 at 08:41)</a>:</h4>
<blockquote>
<p>They tend to talk as little as possible.</p>
</blockquote>
<p><em>[...] With its customary discretion, the Company did not reply directly; instead, it scrawled its brief argument in the rubble of a mask factory. [...]</em></p>



<a name="532919217"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532919217" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Daniel Rogozin <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532919217">(Aug 05 2025 at 13:40)</a>:</h4>
<p>Blimey, it looks like some overcomplicated topics for 1st year students.<br>
<a href="/user_uploads/21317/JnuRrz16uz1RSqNwxlLekz1Y/Screenshot-2025-08-05-at-14.39.43.png">Screenshot 2025-08-05 at 14.39.43.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/JnuRrz16uz1RSqNwxlLekz1Y/Screenshot-2025-08-05-at-14.39.43.png" title="Screenshot 2025-08-05 at 14.39.43.png"><img data-original-content-type="image/png" data-original-dimensions="1662x1368" src="/user_uploads/thumbnail/21317/JnuRrz16uz1RSqNwxlLekz1Y/Screenshot-2025-08-05-at-14.39.43.png/840x560.webp"></a></div>



<a name="532942336"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532942336" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532942336">(Aug 05 2025 at 15:39)</a>:</h4>
<p>Here's a counter-point from twitter: "I don’t get why Arxiv containing slop is a bad thing. I mean sure it’s frustrating and annoying, but merely having been put on Arxiv should give a paper draft exactly zero additional credibility".  I suppose I feel the same way; I always thought of the arxiv as simply a substitute for putting pdfs on a personal website, with endorsement meant to keep the arxiv from turning into a public API for storing PDFs as opposed to technical vetting.</p>



<a name="532948882"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532948882" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532948882">(Aug 05 2025 at 16:18)</a>:</h4>
<p>arXiv is a tool for researchers. It becomes useless if there is absolutely no moderation. If anyone can upload whatever they like, you might as well refer to the <a href="https://libraryofbabel.info/">Library of Babel</a> instead.</p>



<a name="532950769"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532950769" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532950769">(Aug 05 2025 at 16:29)</a>:</h4>
<p>"I don't see why this thing that two bad adjectives apply to is bad"</p>



<a name="532951144"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532951144" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532951144">(Aug 05 2025 at 16:31)</a>:</h4>
<p>Besides being frustrating and annoying: Almost everything on arXiv is real research, which is a huge benefit for discoverability, and does actually mean that something being on arXiv increases its credibility far over "a random thing on the Internet." Both those values could be mostly destroyed by too high a slop ratio.</p>



<a name="532952547"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/532952547" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#532952547">(Aug 05 2025 at 16:40)</a>:</h4>
<p>Yes, I too don't want all the pseudoscientific vomit in the world to be on the arXiv.   That's called the <em>internet</em>.</p>
<p>When I search for papers with a given keyword on the arXiv, I want a majority of them to actually make sense!  And they do, so far.  </p>
<p>The arXiv is a tremendously useful tool, which I use several times a day, largely because of what's <em>not</em> on it.</p>



<a name="535171697"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/535171697" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mike Stay <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#535171697">(Aug 19 2025 at 17:07)</a>:</h4>
<p>For now, I'd expect custom tools devoted to detecting AI to still work.  If the detect/evade war gets to the point where AI is providing LEAN proofs of its work, I think that's a win.</p>
<ul>
<li><a href="https://github.com/mbzuai-nlp/DetectLLM">https://github.com/mbzuai-nlp/DetectLLM</a></li>
<li><a href="https://arxiv.org/abs/2301.11305">https://arxiv.org/abs/2301.11305</a></li>
<li><a href="https://www.scribbr.com/ai-tools/best-ai-detector/">https://www.scribbr.com/ai-tools/best-ai-detector/</a></li>
</ul>



<a name="535228109"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/535228109" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#535228109">(Aug 20 2025 at 02:21)</a>:</h4>
<p>See Figure 3 in this paper <a href="https://arxiv.org/abs/2410.02457">https://arxiv.org/abs/2410.02457</a> It's a midjourney dream of what a mathematical image should look like.</p>



<a name="535265213"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/535265213" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#535265213">(Aug 20 2025 at 08:37)</a>:</h4>
<p>you're talking about figure 3, but I would like to draw your attention to page 2....</p>



<a name="535295077"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/535295077" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Josh Chen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#535295077">(Aug 20 2025 at 11:42)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276422">David Michael Roberts</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/535228109">said</a>:</p>
<blockquote>
<p>See Figure 3 in this paper <a href="https://arxiv.org/abs/2410.02457">https://arxiv.org/abs/2410.02457</a> It's a midjourney dream of what a mathematical image should look like.</p>
</blockquote>
<p>The escalation from Figs 1--2 to Fig 3 gave me a good laugh.</p>



<a name="546399334"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546399334" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546399334">(Oct 22 2025 at 08:50)</a>:</h4>
<p><a href="https://arxiv.org/abs/2510.17829">This work</a> claims a full lean verification of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo mathvariant="normal">≠</mo><mi>N</mi><mi>P</mi></mrow><annotation encoding="application/x-tex">P\neq NP</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">NP</span></span></span></span>, but only provides snippets of the code in their paper that reads like the output of an LLM.</p>



<a name="546409368"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546409368" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546409368">(Oct 22 2025 at 09:40)</a>:</h4>
<p>I saw the title and comments in the daily arXiv mailing, and <em>knew</em> it was LLM-generated.</p>



<a name="546409473"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546409473" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546409473">(Oct 22 2025 at 09:40)</a>:</h4>
<p>Opening it to find many, many short subsections consisting just of dot points was almost not needed.</p>



<a name="546413701"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546413701" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Damiano Mazza <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546413701">(Oct 22 2025 at 10:00)</a>:</h4>
<p>Out of curiosity, I went straight to the technical part (Sect. 3) and randomly checked one of  the results (Theorem 3.14), finding that it is obviously false.  (The category <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi><mi mathvariant="bold">o</mi><mi mathvariant="bold">m</mi><mi mathvariant="bold">p</mi></mrow><annotation encoding="application/x-tex">\mathbf{Comp}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8805em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">Comp</span></span></span></span></span> mentioned in the theorem is formally introduced and makes sense per se, but it is certainly <em>not</em> additive with the proposed definition).</p>



<a name="546438780"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546438780" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546438780">(Oct 22 2025 at 11:58)</a>:</h4>
<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>



<a name="546440181"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546440181" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546440181">(Oct 22 2025 at 12:05)</a>:</h4>
<p>The Lean code is not publically available as far as I can tell, so we only have the word of the author/LLM that such a verification exists.</p>



<a name="546466317"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546466317" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> daniel gratzer <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546466317">(Oct 22 2025 at 14:02)</a>:</h4>
<p><span class="user-mention silent" data-user-id="971326">Elisha Goldman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546438780">said</a>:</p>
<blockquote>
<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>
</blockquote>
<p>Just to note, while it's highly likely the Lean code is proof of _something_ (assuming it's not just full of <code>sorry</code>s), (un)subtly incorrect definitions and theorem statements mean the code almost surely does not prove P /= NP.</p>
<p>[Back in the day, someone won a full bitcoin for proving false in Rocq. They did this by simply defining <code>False := True</code> and then proving "false" according to this new definition. The moral of this story is that theorem provers are not really designed to be adversary-proof and mechanized proofs are only as good as the formalized statement is]</p>



<a name="546466591"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546466591" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546466591">(Oct 22 2025 at 14:03)</a>:</h4>
<p><span class="user-mention silent" data-user-id="277285">Martti Karvonen</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546440181">said</a>:</p>
<blockquote>
<p>The Lean code is not publically available as far as I can tell, so we only have the word of the author/LLM that such a verification exists.</p>
</blockquote>
<p>It's in the paper itself, it just takes a bit to get there (the main proof is on page 40)</p>



<a name="546467088"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546467088" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546467088">(Oct 22 2025 at 14:05)</a>:</h4>
<p>Lol, I wanna hear more about the Rocq story</p>



<a name="546468666"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546468666" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> daniel gratzer <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546468666">(Oct 22 2025 at 14:11)</a>:</h4>
<p>Here are <a href="https://yoichihirai.com/edcon-yoichi-hirai.pdf">some slides</a> by the person who created the site (and lost the bitcoin). I can't find the original cite now searching quickly, but more of the story is here. Gosh... can't believe this was all already 10 years ago.</p>



<a name="546542529"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546542529" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546542529">(Oct 22 2025 at 20:10)</a>:</h4>
<p><span class="user-mention silent" data-user-id="971326">Elisha Goldman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546466591">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="277285">Martti Karvonen</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546440181">said</a>:</p>
<blockquote>
<p>The Lean code is not publically available as far as I can tell, so we only have the word of the author/LLM that such a verification exists.</p>
</blockquote>
<p>It's in the paper itself, it just takes a bit to get there (the main proof is on page 40)</p>
</blockquote>
<p>Oh oops, I hadn't realized they seemingly give all of it in bits and pieces, rather than just showing an excerpt or two.</p>



<a name="546564071"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546564071" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546564071">(Oct 22 2025 at 23:03)</a>:</h4>
<p><span class="user-mention silent" data-user-id="971326">Elisha Goldman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546438780">said</a>:</p>
<blockquote>
<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>
</blockquote>
<p>To be more specific, people usually misunderstand things in very vague ways, so being able to see exactly what they misunderstand here seems useful pedagogically since it shows where students might be confused.<br>
Also, even if it doesn't say what they think it does, the fundamental idea (computational complexity via homology) seems novel and is formally-verified non-gibberish, so maybe some form of it could be interesting (though I definitely don't know enough CS to say anything meaningful here)</p>



<a name="546564869"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546564869" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546564869">(Oct 22 2025 at 23:11)</a>:</h4>
<p>Imo the best thing to tell cranks is to provide a formal verification, it filters out the slop and helps them realize where they're wrong/too vague<br>
(the other response is getting annoyed, which doesn't really help anyone)</p>



<a name="546569105"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546569105" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546569105">(Oct 23 2025 at 00:00)</a>:</h4>
<p>Interestingly, in the formalization section, the author does not claim to have formally verified the result on additivity of Comp that <span class="user-mention" data-user-id="276839">@Damiano Mazza</span> correctly points out is obviously false.</p>



<a name="546571175"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546571175" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546571175">(Oct 23 2025 at 00:24)</a>:</h4>
<p>Elisha, I'm afraid you are massively begging the question by assuming that "the fundamental idea" exists. It seems to me that this paper constitutes nothing less than an adversarial attack on the scientific community, or at least on its non-expert fringe who cannot check its claims for themselves.</p>



<a name="546571332"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546571332" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546571332">(Oct 23 2025 at 00:27)</a>:</h4>
<p>I spent almost an hour poking through here carefully to see where the more central claims begin to fall apart. Theorems 3.24 and 4.1 brazenly contradict each other, proving respectively that problems in P are homologically trivial and that all NP-complete problems are homologically isomorphic to all problems in NP. Even more to the point, the <em>proof</em> of 3.24 really shows the lie where it says "The detailed argument uses the functoriality of the computational homology construction and the fact that homology isomorphisms preserve the 'computational topology' of problems." The last claim is, naturally, not mathematically defined. The computational chain complex also appears not to be genuinely defined, as far as I can tell. I haven't compared to see what the author chucked into the formalized definition.</p>



<a name="546571425"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546571425" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546571425">(Oct 23 2025 at 00:28)</a>:</h4>
<p>Hopefully nobody here is surprised that this is entirely nonsense, but I think it was still worth looking at a little more closely to continue tracking the progress of these adversarial attacks. I was just discussing the notion of a slop artist (whether or not they believe their own slop) trying to justify themselves via submitting formal verifications with a friend last week, and here we are now! This is a clear big step less obviously wrong than the papers we were discussing even a couple of months ago.</p>



<a name="546571860"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546571860" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546571860">(Oct 23 2025 at 00:34)</a>:</h4>
<p>OK, looking at the formalization, the author specifically attempts to bamboozle the reader by exhibiting a detailed-looking verification of the chain complex property of an alleged chain complex which would, if it existed, be the singular chain complex of the graph of "computation paths" in a computational problem; but a computation path <em>itself</em> depends on some concept of a "Configuration" of a verifier of a computational problem which is never actually defined.</p>



<a name="546571861"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546571861" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546571861">(Oct 23 2025 at 00:34)</a>:</h4>
<p>I think this work reduces upon inspection to ordinary crankery about "imagine the simplicial complexes of all efficient computation paths, man, and then, like, the cycles in this complex would represent the irreducible computational steps, bro!" But it's really buried quite a lot deeper than it otherwise would be with all the LLM help.</p>



<a name="546580875"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546580875" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546580875">(Oct 23 2025 at 02:49)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546571175">said</a>:</p>
<blockquote>
<p>Elisha, I'm afraid you are massively begging the question by assuming that "the fundamental idea" exists. It seems to me that this paper constitutes nothing less than an adversarial attack on the scientific community, or at least on its non-expert fringe who cannot check its claims for themselves.</p>
</blockquote>
<p>Ah, well, that's what I get for only skimming it, fair enough</p>



<a name="546582792"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546582792" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546582792">(Oct 23 2025 at 03:18)</a>:</h4>
<p>You're right though, my internal model of the crank was that they start as a passionate amateur, invest way too much ego in their theories, and only then become antagonistic due to perceived rejection; and that formal verification can replace the social rejection with a learning experience where they find their mistake and salvage what they can. I hadn't really considered any deliberate deception (aside from self-deception). The psychology must be different, I guess the antagonism comes before actually appreciating the subject, maybe for political reasons, like Sokal?</p>



<a name="546583103"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546583103" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546583103">(Oct 23 2025 at 03:23)</a>:</h4>
<p>There are very sad stories of people being completely sent off the deep end by AI-hallucinated theories of everything/conspiracies. Something like this is rather mild by comparison. I don't think the author doubts this paper at all.</p>



<a name="546583558"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546583558" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Elisha Goldman <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546583558">(Oct 23 2025 at 03:29)</a>:</h4>
<p>Fwiw I think that when people get to this stage, telling them that LLM output is meaningless (while correct) is not gonna be accepted, since it's already replaced half their brain</p>



<a name="546621262"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546621262" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ivan Di Liberti <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546621262">(Oct 23 2025 at 08:40)</a>:</h4>
<p><a href="https://arxiv.org/pdf/2510.19444">https://arxiv.org/pdf/2510.19444</a></p>



<a name="546696273"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546696273" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546696273">(Oct 23 2025 at 14:30)</a>:</h4>
<p><span class="user-mention silent" data-user-id="971326">Elisha Goldman</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546438780">said</a>:</p>
<blockquote>
<p>Even if the paper itself is slop, it's good/important to figure out where it fails, because Lean verifications are supposed to be infallible</p>
</blockquote>
<p>It may be good, but it's not important.   It may not even be good, since it wasted the time of several smart people here, who could have been doing something more useful.</p>



<a name="546762726"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/546762726" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#546762726">(Oct 23 2025 at 20:18)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276422">David Michael Roberts</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546583103">said</a>:</p>
<blockquote>
<p>There are very sad stories of people being completely sent off the deep end by AI-hallucinated theories of everything/conspiracies. Something like this is rather mild by comparison. I don't think the author doubts this paper at all.</p>
</blockquote>
<p>Maybe. I was taking from the deeply buried gaps in the allegedly formalized exposition, plus the assertion that all code was shared on a GitHub repo which remains private (if it exists), that this might be an intentional scam. It's hard for me to see how you'd believe your Lean code was really proving something true if you hadn't actually defined all the types in it.</p>



<a name="547042102"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/547042102" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#547042102">(Oct 25 2025 at 09:06)</a>:</h4>
<p><span class="user-mention silent" data-user-id="358258">daniel gratzer</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546466317">said</a>:</p>
<blockquote>
<p>[Back in the day, someone won a full bitcoin for proving false in Rocq. They did this by simply defining <code>False := True</code> and then proving "false" according to this new definition.</p>
</blockquote>
<p>Wow that's an easy 100k <span aria-label="open mouth" class="emoji emoji-1f62e" role="img" title="open mouth">:open_mouth:</span></p>



<a name="547042620"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/547042620" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matteo Capucci (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#547042620">(Oct 25 2025 at 09:13)</a>:</h4>
<p><span class="user-mention silent" data-user-id="277318">Ivan Di Liberti</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/546621262">said</a>:</p>
<blockquote>
<p><a href="https://arxiv.org/pdf/2510.19444">https://arxiv.org/pdf/2510.19444</a></p>
</blockquote>
<p>What's wrong about this one?</p>



<a name="548332558"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/548332558" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#548332558">(Nov 01 2025 at 19:24)</a>:</h4>
<p>The arxiv just announced it will stop taking position or survey papers in CS that haven't been peer reviewed: <a href="https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/">https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/</a></p>



<a name="553239761"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/553239761" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#553239761">(Nov 02 2025 at 11:47)</a>:</h4>
<p>They're getting swamped by AI-generated papers.   But AI won't only invade this one particular category.    Perhaps under this pressure the arXiv will slowly morph into a "meta-journal", only allowing papers that have been peer-reviewed by some other means.   Of course this would have huge downsides.</p>



<a name="553245760"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/553245760" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> James Deikun <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#553245760">(Nov 02 2025 at 13:37)</a>:</h4>
<p>They've had to close this category down first because the AI-generated papers in it are not as bad as most AI-generated papers, they're not wrong so much as just useless.  I think there isn't a direct equivalent for other categories, because something with a new result can only be <em>mostly</em> useless.  In other categories, it will be necessary to watch out for, as AI gets better, first plagiarism, and then usually-true-but-usually-boring research papers.  Of course if AI gets beyond being slop entirely we will no longer have this problem with the arXiv, but will probably have much worse problems.</p>



<a name="553566960"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/553566960" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#553566960">(Nov 04 2025 at 09:46)</a>:</h4>
<p>Looks like <a href="https://arxiv.org/abs/2511.00058">another one</a>. Categories appear in section 4.</p>



<a name="553572188"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/553572188" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#553572188">(Nov 04 2025 at 10:09)</a>:</h4>
<p><a href="/user_uploads/21317/7rvpmMlL1c6CnBFKvUyC6MNp/image.png">image.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/7rvpmMlL1c6CnBFKvUyC6MNp/image.png" title="image.png"><img data-original-content-type="image/png" data-original-dimensions="930x225" src="/user_uploads/thumbnail/21317/7rvpmMlL1c6CnBFKvUyC6MNp/image.png/840x560.webp"></a></div><p><span aria-label="explode" class="emoji emoji-1f92f" role="img" title="explode">:explode:</span> mind blowing</p>



<a name="557904657"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/557904657" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#557904657">(Nov 18 2025 at 11:26)</a>:</h4>
<p><a href="https://arxiv.org/abs/2511.13674">Here</a> and <a href="https://arxiv.org/abs/2511.13706">here</a> is some of today's slop.</p>



<a name="557908611"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/557908611" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#557908611">(Nov 18 2025 at 11:44)</a>:</h4>
<p>This author is a tenure-track professor at San Jose State with a 15-year academic career including several rather highly-cited papers, mainly in engineering journals (for reference, his "h-index" is about the same as Emily Riehl's, though citations are certainly much easier to come by in engineering fields.) Since 2021, he's been publishing at an unreasonable rate by mathematical standards, a couple dozen papers a year. I don't know whether these standards are abnormal in electrical engineering, but it does seem to start a bit too soon to be substantially LLM-supported, and some of his recent papers are fairly well-cited. But if he's posting obvious slop on Arxiv (I haven't checked myself), that's very serious academic misconduct at a university that those of us in the Western academic sphere actually know how to get in touch with, which is different from the previous cases.</p>



<a name="557908712"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/557908712" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#557908712">(Nov 18 2025 at 11:44)</a>:</h4>
<p>I see he does explicitly cite use of LLMs, but claims responsibility for all results.</p>



<a name="557910596"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/557910596" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#557910596">(Nov 18 2025 at 11:54)</a>:</h4>
<p>I guess one could desk reject this on the basis that it has no citations. But it's pretty worrying. I bet the author would tell you he sincerely believes these results are correct and valuable, and it seems at least conceivable that nothing in there is a brazen howl, so it's getting awfully close to something that a human editor would feel they need to send out for review, which really begins to break the system. If editors refuse to review any paper that's LLM-assisted, people will surely just try to be sneakier about it. What's to be done as LLM slop becomes less and less immediately disregardable?</p>



<a name="557922735"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/557922735" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#557922735">(Nov 18 2025 at 12:48)</a>:</h4>
<p>The signs of it being LLM-generated are clear (even without the admission by the author). As for the content, there is lot of correct things said, but it doesn't take long to start noticing the slop. I'd desk reject it as an editor, but I could imagine if it was sufficiently outside of one's expertise something a step above in quality of presentation with no better content would get sent to peer review. For instance, there's <a href="https://arxiv.org/abs/2511.09764">these</a>  <a href="https://arxiv.org/abs/2511.07675">two</a> papers where supposedly a major open problem is solved but the writing style screams a bit less like LLM to me (the author does admit using one to help) and as it's outside my expertise, I'd have to send it forward.</p>



<a name="558088136"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/558088136" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#558088136">(Nov 18 2025 at 23:51)</a>:</h4>
<p>For this behavior to make sense, tenure committees would need to actually be fooled by having lots of bad papers - are they really?  (I understand lots of mediocre papers may be considered part of publish or perish, but bad papers!?). Maybe the way to stop the AI slop would be to give tenure committees LLM detectors</p>



<a name="558091885"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/558091885" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#558091885">(Nov 19 2025 at 00:19)</a>:</h4>
<p>Given that people in certain countries are making successful careers out of sheer volume of papers in journals supported by citation cartels, "having lots of bad papers" fooling committees is not a new thing</p>



<a name="558146500"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/558146500" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#558146500">(Nov 19 2025 at 09:03)</a>:</h4>
<p>I think an EE tenure committee would likely have a lot of trouble telling whether these are bad or just mediocre papers. They'd probably have to rely on the judgment of mathematicians in where and whether to publish. This stuff wouldn't get published in our real journals, but can EE tenure committees distinguish between our real journals and ones we'd consider mostly fake?</p>



<a name="558173738"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/558173738" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#558173738">(Nov 19 2025 at 11:11)</a>:</h4>
<p>Bad universities have bad administrators who don't care much that bad professors are publishing bad papers in bad journals.  I hope only bad students go to these universities!</p>



<a name="558196000"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/558196000" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#558196000">(Nov 19 2025 at 13:00)</a>:</h4>
<p>Well, San Jose State is the kind of place you go if you're a pretty good student in California who can't quite make it to Berkeley or UCLA, so I don't think that's likely the case. And there are plausibly people not writing such bad papers who don't get a job because they don't have such a bewildering pile of publications...</p>



<a name="558209078"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/558209078" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> JR <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#558209078">(Nov 19 2025 at 13:55)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/558146500">said</a>:</p>
<blockquote>
<p>I think an EE tenure committee would likely have a lot of trouble telling whether these are bad or just mediocre papers. They'd probably have to rely on the judgment of mathematicians in where and whether to publish. This stuff wouldn't get published in our real journals, but can EE tenure committees distinguish between our real journals and ones we'd consider mostly fake?</p>
</blockquote>
<p>What would they make of <em>Tunisian Journal of Mathematics</em> for example?</p>



<a name="558209881"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/558209881" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#558209881">(Nov 19 2025 at 13:58)</a>:</h4>
<p>Do you think the Tunisian Journal is fake? It has some quite senior people as editors and a glance at a few titles doesn't throw up immediate red flags.</p>



<a name="558209989"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/558209989" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> JR <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#558209989">(Nov 19 2025 at 13:59)</a>:</h4>
<p>Exactly. It is unexpectedly strong to those not already in the know.</p>



<a name="558210443"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/558210443" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#558210443">(Nov 19 2025 at 14:01)</a>:</h4>
<p>Ah, I get it, you were saying they might give it a false negative because of bias against the base being in Tunisia, or something.</p>



<a name="558252648"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/558252648" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#558252648">(Nov 19 2025 at 16:35)</a>:</h4>
<p>I guess my point here is that tenure is a socially mediated process by the very communities that (should) care, whereas arxiv publishing is much less so, so solving the AI tenure problem somehow seems easier than the arxiv AI slop problem.  But if tenure committees were already granting tenure on bad papers, I'm all out of ideas.</p>



<a name="560064651"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560064651" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Bryce Clarke <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560064651">(Nov 25 2025 at 04:11)</a>:</h4>
<p><a href="https://arxiv.org/abs/2511.17544">https://arxiv.org/abs/2511.17544</a></p>



<a name="560064690"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560064690" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Bryce Clarke <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560064690">(Nov 25 2025 at 04:11)</a>:</h4>
<p>This too unfortunately looks to be AI-generated.</p>



<a name="560065040"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560065040" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560065040">(Nov 25 2025 at 04:15)</a>:</h4>
<p>That looks like the same person who earlier put up a lot of AI-gen papers: <a href="https://arxiv.org/search/?query=Reizi&amp;searchtype=author&amp;source=header">https://arxiv.org/search/?query=Reizi&amp;searchtype=author&amp;source=header</a></p>



<a name="560066313"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560066313" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Bryce Clarke <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560066313">(Nov 25 2025 at 04:30)</a>:</h4>
<p>I thought the name was familiar, but it doesn’t come up on the full name search on ArXiv. That explains why.</p>



<a name="560067443"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560067443" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560067443">(Nov 25 2025 at 04:45)</a>:</h4>
<p>The email address the papers were submitted from is the same, so it's not some peculiar coincidence, either</p>



<a name="560077070"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560077070" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560077070">(Nov 25 2025 at 06:33)</a>:</h4>
<p>F. Alpay, a different offender, has posted 38 papers since May on a bunch of different topics: <a href="https://arxiv.org/search/?searchtype=author&amp;query=Alpay%2C+F">https://arxiv.org/search/?searchtype=author&amp;query=Alpay%2C+F</a> <br>
Roughly three papers every two weeks....</p>
<p>Some of them got moved to math.GM, but only at the start...</p>



<a name="560078635"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560078635" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560078635">(Nov 25 2025 at 06:50)</a>:</h4>
<p>These papers are now in math.LO, math.OC, cs.LO, cs.AI, cs.CR, cs.IR, math.FA, cs.LG, math.NT, stat.ML, cs.PL, cs.PF and stat.ME. <br>
Poisoning the waterhole indeed. No wonder the cs part of the arXiv resorted to banning a genre of papers</p>



<a name="560083187"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560083187" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560083187">(Nov 25 2025 at 07:25)</a>:</h4>
<p>Maybe a simple rate limiter would work?  No more than one paper per month or something?</p>



<a name="560086323"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560086323" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560086323">(Nov 25 2025 at 07:47)</a>:</h4>
<p>Sadly that wouldn't work for people who want to release a multi-part project in one go. Say you hadn't published anything on the arXiv for six months, then have a two-part paper you want to upload on the same day (I've seen this before from respectable people).<br>
Also, different parts of the arXiv would have to have their own rates, because in certain parts of physics a paper every two weeks is not ludicrous. And someone who heads up a big lab who has lots of projects running under their name with a team of grad students and postdocs is likely to run afoul of this too.</p>



<a name="560086490"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560086490" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560086490">(Nov 25 2025 at 07:49)</a>:</h4>
<p>Also, Saharon Shelah wouldn't have been allowed to put up a couple of articles a month, as he did when working at his top pace: <a href="https://shelah.logic.at/paper-list/#published_papers_withoutaccepted">https://shelah.logic.at/paper-list/#published_papers_withoutaccepted</a></p>



<a name="560087777"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560087777" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> James Deikun <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560087777">(Nov 25 2025 at 07:57)</a>:</h4>
<p>A manually-curated whitelist might work for these issues though.</p>



<a name="560106091"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560106091" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560106091">(Nov 25 2025 at 09:37)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276693">Bryce Clarke</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/560064651">said</a>:</p>
<blockquote>
<p><a href="https://arxiv.org/abs/2511.17544">https://arxiv.org/abs/2511.17544</a></p>
</blockquote>
<p>Does the math look wrong, or is it just the style that's AI-ish?   (The math certainly looks badly explained; I haven't decided yet whether it's actually <em>wrong.</em>)</p>
<p>In one passage it says</p>
<blockquote>
<p>In the original draft, the final statement was phrased as “equivalently.” We have rephrased it to clarify that the Λ-compatibility of θ is a <em>consequence</em> of the monoidal axioms and the strict Λ-compatibility of F and G, rather than an independent axiom.</p>
</blockquote>
<p>It also makes another reference to a mistake in the "original draft", and it thanks "anonymous reviewers", as if it's been published.</p>



<a name="560108007"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560108007" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560108007">(Nov 25 2025 at 09:46)</a>:</h4>
<p><span class="user-mention silent" data-user-id="275920">John Baez</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/560106091">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="276693">Bryce Clarke</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/560064651">said</a>:</p>
<blockquote>
<p><a href="https://arxiv.org/abs/2511.17544">https://arxiv.org/abs/2511.17544</a></p>
</blockquote>
<p>Does the math look wrong, or is it just the style that's AI-ish?   (The math certainly looks badly explained; I haven't decided yet whether it's actually <em>wrong.</em>)</p>
<p>In one passage it says</p>
<blockquote>
<p>In the original draft, the final statement was phrased as “equivalently.” We have rephrased it to clarify that the Λ-compatibility of θ is a <em>consequence</em> of the monoidal axioms and the strict Λ-compatibility of F and G, rather than an independent axiom.</p>
</blockquote>
<p>It also makes another reference to a mistake in the "original draft", and it thanks "anonymous reviewers", as if it's been published.</p>
</blockquote>
<p>Reizi is literally the same guy we discussed months ago, just under a slightly different name <a href="https://arxiv.org/search/math?query=Reizi&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50">https://arxiv.org/search/math?query=Reizi&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50</a></p>



<a name="560108199"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560108199" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560108199">(Nov 25 2025 at 09:48)</a>:</h4>
<p>now, the trusting, well-mannered person inside me thinks "maybe in the meantime they got married"</p>
<p>As for the other person in me...</p>



<a name="560142608"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560142608" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560142608">(Nov 25 2025 at 12:37)</a>:</h4>
<p><a href="/user_uploads/21317/u86o4KAHtc3Ug-3Dqc5csFxV/image.png">image.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/u86o4KAHtc3Ug-3Dqc5csFxV/image.png" title="image.png"><img data-original-content-type="image/png" data-original-dimensions="749x500" src="/user_uploads/thumbnail/21317/u86o4KAHtc3Ug-3Dqc5csFxV/image.png/840x560.webp"></a></div><p>The title of the paper could legitimately be Merzbow new drop</p>



<a name="560142704"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560142704" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560142704">(Nov 25 2025 at 12:38)</a>:</h4>
<p>(should have posted in #memes...)</p>



<a name="560299070"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560299070" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560299070">(Nov 26 2025 at 04:33)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276422">David Michael Roberts</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/560086490">said</a>:</p>
<blockquote>
<p>Also, Saharon Shelah wouldn't have been allowed to put up a couple of articles a month, as he did when working at his top pace: <a href="https://shelah.logic.at/paper-list/#published_papers_withoutaccepted">https://shelah.logic.at/paper-list/#published_papers_withoutaccepted</a></p>
</blockquote>
<p>Shelah has really slowed down a lot, only 12 papers on the arXiv so far this year. In 2019 he had 33 papers. In 2023 he had 40 (!)</p>



<a name="560302613"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560302613" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ryan Wisnesky <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560302613">(Nov 26 2025 at 04:52)</a>:</h4>
<p>if we assume arxiv endorsement is similar to block-chain endorsement, then the reaction should be not just against the bad ai-enabled authors, but against whoever endorsed those authors.</p>



<a name="560350032"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560350032" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560350032">(Nov 26 2025 at 10:06)</a>:</h4>
<p>Very good point.  I reject all requests for arXiv endorsements from people I don't know, because I don't have time to investigate them.  (Many are obvious crackpots, but for others I can't quickly tell.)</p>



<a name="560724327"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560724327" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Graham Manuell <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560724327">(Nov 28 2025 at 08:01)</a>:</h4>
<p>The endorsement process currently only asks endorsers to read the paper the person is submitting, not vouch that they will never produce nonsense, so the whole policy would need to change before the reaction can be back-propagated.</p>



<a name="560737761"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/560737761" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#560737761">(Nov 28 2025 at 09:13)</a>:</h4>
<p>Good point. However, some of these authors seem to have only produced AI-generated papers, so in those cases the endorser did drop the ball.</p>



<a name="564634934"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/564634934" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Amar Hadzihasanovic <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#564634934">(Dec 19 2025 at 07:38)</a>:</h4>
<p><span class="user-mention silent" data-user-id="277285">Martti Karvonen</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/557904657">said</a>:</p>
<blockquote>
<p><a href="https://arxiv.org/abs/2511.13674">Here</a> and <a href="https://arxiv.org/abs/2511.13706">here</a> is some of today's slop.</p>
</blockquote>
<p>More from the same person today: <a href="https://arxiv.org/abs/2512.15951">https://arxiv.org/abs/2512.15951</a></p>



<a name="564638418"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/564638418" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Amar Hadzihasanovic <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#564638418">(Dec 19 2025 at 08:05)</a>:</h4>
<p>One of the first things in the paper is defining a "multiadjoint" operation which appears to be <em>cyclic</em> in (inputs + output), that is, takes <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">H</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">m</mi></mrow><mo stretchy="false">(</mo><msub><mi>H</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>H</mi><mi>n</mi></msub><mo separator="true">;</mo><mi>K</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{Hom}(H_1, \ldots, H_n; K)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">Hom</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mclose">)</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">H</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">m</mi></mrow><mo stretchy="false">(</mo><msup><mi>K</mi><mo>†</mo></msup><mo separator="true">,</mo><msubsup><mi>H</mi><mn>1</mn><mo>†</mo></msubsup><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>H</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><mo>†</mo></msubsup><mo separator="true">;</mo><msubsup><mi>H</mi><mi>n</mi><mo>†</mo></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{Hom}(K^\dagger, H^\dagger_1, \ldots, H^\dagger_{n-1}; H_n^\dagger)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2917em;vertical-align:-0.3246em;"></span><span class="mord"><span class="mord mathrm">Hom</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">†</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.967em;"><span style="top:-2.4337em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">†</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2663em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.967em;"><span style="top:-2.4337em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">†</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3246em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-2.453em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">†</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> and then claiming that this is an <em>involution</em>, which seems immediately nonsense</p>



<a name="565749827"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/565749827" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#565749827">(Dec 30 2025 at 06:04)</a>:</h4>
<p>Okay then. <em>THIS</em> is why so many AI junk papers were turning up on the maths section of the arXiv, including the person mentioned in my very first post in this thread: <a href="https://blog.arxiv.org/2025/12/10/updated-endorsement-policy-for-arxiv-mathematics/">https://blog.arxiv.org/2025/12/10/updated-endorsement-policy-for-arxiv-mathematics/</a> </p>
<blockquote>
<p>In the past most arXiv sections, including Math, auto-endorsed first-time submitters with affiliation to an academic or research institute. arXiv felt this was a fair way to allow new researchers to submit to arXiv while still preserving the corpus and maintaining a standard of scientific integrity. </p>
</blockquote>
<p>so (probably) any student with an academic affiliation, or a researcher with a university address who wasn't a mathematician but felt their genAI conversation solved some cool problem could get auto-endorsement from the arXiv and then just be allowed free reign. It wasn't, it seems, rogue academics endorsing crackpots with access to an LLM. </p>
<p>The new standard is:</p>
<blockquote>
<p>Automatic endorsement for new submitters in all Math categories will now require both 1) an email address from an academic/research institution, and 2) previous authorship on an existing paper which has been accepted to the arXiv Mathematics section (see <a href="https://info.arxiv.org/help/authority.html">paper ownership</a>). New submitters who cannot meet these requirements will no longer be automatically endorsed, but can still <a href="https://info.arxiv.org/help/endorsement.html#who-can-endorse">obtain a personal endorsement</a> directly from an established arXiv author in the same research area.</p>
</blockquote>
<p>Caveat for <code>math-ph</code>:</p>
<blockquote>
<p>This policy update only applies to arXiv Mathematics, and <a href="https://arxiv.org/category_taxonomy">all the Math categories contained in the section</a>. Please note, <a href="https://arxiv.org/list/math-ph/recent">Mathematical Physics</a> (math-ph) is a sub-section of Physics and will follow endorsement rules for Physics. Each category of arXiv has different moderators, who are subject matter experts with a terminal degree in their particular subject, to best serve the scholarly pursuits, goals, and standards of their category. While this policy update is currently only being implemented for the arXiv Mathematics section, other arXiv sections may wish to update their endorsement policies in the future. We will make these updates public if and when they do occur.</p>
</blockquote>



<a name="565771879"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/565771879" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#565771879">(Dec 30 2025 at 10:39)</a>:</h4>
<p>Starting a few months ago I've noticed a great increase in how many people ask me to endorse them for the arXiv.   Now I'm getting roughly 1 such request per day, almost all from people with completely wacky ideas.   But I don't think it's solely because of the new arXiv policy, first because of the timing and second because a lot of these people don't have academic affiliations and thus the policy doesn't apply to them.</p>
<p>Many of these people don't even have the common sense to send me the paper that they are trying to put on the arXiv, so I can evaluate it!    But usually it's obvious that the paper would be bad, so I don't even answer most of these emails.  Here's a typical one:</p>
<blockquote>
<p>Dear Professor Baez,</p>
<p>I hope you are doing well. I am an independent researcher preparing my first arXiv submission to the quant-ph category and am writing to request possible endorsement.</p>
<p>My manuscript, entitled “The Awareness Theory of Everything: A Recursive Observer-Primitive Framework for Spacetime and Quantum Phenomena,” presents a foundational, observer-primitive approach in which awareness is taken as the minimal starting point, from which spacetime, quantum phase, entanglement, and particle identity are derived via recursion.</p>
<p>I understand endorsement does not imply agreement with the content, only that the work is appropriate for the category. I would be grateful if you would consider endorsing me, or let me know if further information would be helpful.</p>
<p>Thank you very much for your time.</p>
<p>(Please see below for endorsement links)</p>
</blockquote>
<p>Here's another:</p>
<blockquote>
<p>Dear Prof. Baez,</p>
<p>I hope this email finds you well. My name is [---], an independent researcher in theoretical physics. I am writing to respectfully request your endorsement for submitting my preprint to arXiv in the math-ph category.</p>
<p>The paper, titled "Law of Absolute Kinematics: The Ultimate Equation of the Universe Driven by the α₀ Postulate," proposes a single kinematic postulate—that the proper four-acceleration modulus of all observers (with zero non-gravitational three-acceleration) is a universal constant α₀ &gt; 0. This forces spacetime to emerge as de Sitter space and three-dimensional hyperbolic space (H³), analytically determining all fundamental constants (G, ℏ, c, Λ) and unifying inertia, gravity, and quantum topology via worldline knots and Kähler geometry. A companion paper, "Topological Generation of the Fermion Mass Spectrum: From Worldline Knots to Fundamental Particle Inertia," details how fermion masses arise from topological invariants in H³.</p>
<p>Your work on knot theory, category theory in TQFT, and mathematical physics intersections (e.g., your writings on knots in quantum mechanics and higher categories) has been inspirational. My framework's knot algebra mapping fermions to 12 minimal-volume knots in H³, enforcing CPT invariance, aligns closely with your explorations of topological structures in physical paradigms.</p>
<p>The preprint is openly available [....]</p>
<p>I would be deeply grateful if you could review and endorse this submission, as your expertise in these areas makes you an ideal endorser. Please let me know if you need any additional information.</p>
<p>Thank you for your time and consideration.</p>
<p>Best regards, </p>
</blockquote>



<a name="565772471"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/565772471" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#565772471">(Dec 30 2025 at 10:44)</a>:</h4>
<p>Some of these people are obviously using AI, so I feel that's part of the reason for this upswing.   Maybe they're asking an LLM who could endorse their paper?</p>



<a name="565775481"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/565775481" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#565775481">(Dec 30 2025 at 11:14)</a>:</h4>
<p>Maybe an LLM is asking you to be endorsed</p>



<a name="566004811"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566004811" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Cole Comfort <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566004811">(Jan 01 2026 at 23:15)</a>:</h4>
<p><span class="user-mention silent" data-user-id="282822">fosco</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/565775481">said</a>:</p>
<blockquote>
<p>Maybe an LLM is asking you to be endorsed</p>
</blockquote>
<p>Given the frequency, that is definitely possible. Some of the forums I used to visit seem to be full on bots talking to each other and devoid of real humans.</p>



<a name="566047963"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566047963" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566047963">(Jan 02 2026 at 11:31)</a>:</h4>
<p>I'm not sure if that's more or less depressing than the thought of actual people creating these crackpot papers.    Either way, I never endorse <strong>anyone</strong> for the arXiv except my students - so I'm not letting any LLMs onto the arXiv.</p>



<a name="566514774"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566514774" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Martti Karvonen <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566514774">(Jan 06 2026 at 10:17)</a>:</h4>
<p><a href="https://arxiv.org/abs/2601.00803">Another one</a></p>



<a name="566516055"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566516055" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566516055">(Jan 06 2026 at 10:24)</a>:</h4>
<p>It definitely is, but I have to admit it wasn't so obvious to my eye, do you exclude it was just heavily AI - <em>aided</em> and instead it was fully <em>generated</em> ?</p>



<a name="566518324"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566518324" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566518324">(Jan 06 2026 at 10:37)</a>:</h4>
<blockquote>
<p>The author thanks colleagues and anonymous reviewers for their helpful comments on earlier versions of this work</p>
</blockquote>
<p>It amuses but also saddens me when people are so ignorant of how to write a paper that v1 of their LLM-generated preprint includes this type of post-refereeing remark and they don't know it's entirely fake, or don't notice.<br>
Also, from this paper, what is the definition of a "tunnel"? And how does example 6.1 work? What is the "inference" output when two edges share a common vertex? From the incomplete info it should be another edge....</p>



<a name="566521217"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566521217" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566521217">(Jan 06 2026 at 10:54)</a>:</h4>
<p>I didn't get that far before concluding it was robot work <span aria-label="smile" class="emoji emoji-1f604" role="img" title="smile">:smile:</span></p>



<a name="566521453"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566521453" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566521453">(Jan 06 2026 at 10:55)</a>:</h4>
<p>what saddens me is to see how much of this slop comes from people at philosophy departments... it's like some people now think "ah, before it was a prohibitive effort but <em>now</em> I can do research in category theory!"</p>



<a name="566549304"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566549304" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566549304">(Jan 06 2026 at 14:02)</a>:</h4>
<p>This also reflects poorly on the departments who have hired these people. I can't believe Ghent University would be pleased to know that one of its researchers is producing AI slop (assuming the affiliation is legitimate).</p>



<a name="566603305"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566603305" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> James Deikun <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566603305">(Jan 06 2026 at 18:54)</a>:</h4>
<p>Dmytro Sukhov doesn't actually have a faculty page at Ghent, it's quite possible this person is only a student, maybe even an undergraduate, although the picture on <a href="https://www.researchgate.net/profile/Dmytro-Sukhov">ResearchGate</a> looks perhaps a bit old for an undergraduate.  ResearchGate reports over 40,000 "members" of UGhent which seems way too much for just faculty no matter what time span they have collected data over, and depending on the time frame is possibly too much even for graduate students.</p>
<p>The <a href="https://www.slavistiek.ugent.be/contact/">page</a> for what seems like the most likely subdepartment Sukhov would be in doesn't list any such person.</p>



<a name="566630543"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566630543" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ivan Di Liberti <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566630543">(Jan 06 2026 at 22:18)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276092">Nathanael Arkor</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/566549304">said</a>:</p>
<blockquote>
<p>This also reflects poorly on the departments who have hired these people. I can't believe Ghent University would be pleased to know that one of its researchers is producing AI slop (assuming the affiliation is legitimate).</p>
</blockquote>
<p>Unfortunately I feel like guidelines from universities will push for despicable use of AI tools. Maybe they will lead to rejections in pure mathematics, but they will definitely lead to more publications in other fields.</p>



<a name="566630797"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566630797" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ivan Di Liberti <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566630797">(Jan 06 2026 at 22:20)</a>:</h4>
<p><span class="user-mention silent" data-user-id="282822">fosco</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/566516055">said</a>:</p>
<blockquote>
<p>It definitely is, but I have to admit it wasn't so obvious to my eye, do you exclude it was just heavily AI - <em>aided</em> and instead it was fully <em>generated</em> ?</p>
</blockquote>
<p>I think one can guess it essentially by the structure of the text: the ways it uses bullet points, the way it organises phrases. It's not just AI-aided, I can totally tell it's precisely <em>chatgpt</em>.</p>



<a name="566662785"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566662785" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566662785">(Jan 07 2026 at 05:00)</a>:</h4>
<p><span class="user-mention" data-user-id="438995">@James Deikun</span>  people who work at my university but aren't academics have an "institutional email", in that it has the university domain in it. The OP LLM-generated paper person up top has a student email. That said, the image you link on RG is not that old, and people who are mature age students do exist! In my second postdoc I supervised an undergrad for a summer project who was probably a decade older than me!</p>



<a name="566728369"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566728369" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> James Deikun <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566728369">(Jan 07 2026 at 12:58)</a>:</h4>
<p>Yes, I know I was only going by averages there, both in terms of appearances (I knew a guy in high school who would pretend to be my father to get me into movies, but he was actually younger than I was) and people's actual ages, which is why I said "perhaps".  There's a lot of guesswork involved and guesswork is far from perfectly reliable.</p>



<a name="566767752"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/566767752" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#566767752">(Jan 07 2026 at 16:05)</a>:</h4>
<p>I would not be shocked if this Dmytro Sukhov was the same as one who wrote <a href="https://medium.com/@dmitrihazak/parmenides-was-not-afraid-of-motion-77b7f137397b">this blog article about Parmenides</a>.</p>



<a name="570432542"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/570432542" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#570432542">(Jan 27 2026 at 22:11)</a>:</h4>
<ul>
<li><a href="https://www.technologyreview.com/2026/01/27/1131793/openais-latest-product-lets-you-vibe-code-science/">OpenAI's latest project lets you vibe-code science</a>, <em>Technology Review</em>, January 27, 2026.</li>
</ul>
<blockquote>
<p>OpenAI just revealed what its new in-house team, <a href="https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/">OpenAI for Science</a>, has been up to. The firm has released a <a href="https://openai.com/index/introducing-prism/">free LLM-powered tool for scientists called Prism</a>, which embeds ChatGPT in a text editor for writing scientific papers.</p>
</blockquote>
<blockquote>
<p>The idea is to put ChatGPT front and center inside software that scientists use to write up their work in much the same way that chatbots are now embedded into popular programming editors. It’s <a href="https://www.technologyreview.com/2025/04/16/1115135/what-is-vibe-coding-exactly/">vibe coding</a>, but for science.</p>
</blockquote>



<a name="570439076"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/570439076" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#570439076">(Jan 27 2026 at 23:02)</a>:</h4>
<blockquote>
<p>Designed as an AI-enhanced word processor and research tool for scientific papers, Prism is deeply integrated with GPT-5.2, which can be used to assess claims, revise prose, or search for prior research.</p>
<p>Prism isn’t designed to conduct research on its own and without human guidance. Executives believe it will accelerate the work being done by human scientists, and compared Prism to coding interfaces like Cursor and Windsurf.</p>
</blockquote>
<p>"Assess claims"? This is very close to "hey LLM I had the idea that functors could prove the Collatz conjecture, please tell me how good this idea is". (Quote is from TechCrunch, but the article reads like a paid placement, I'm not going to link to it)</p>



<a name="570551698"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/570551698" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Bruno Gavranović <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#570551698">(Jan 28 2026 at 13:07)</a>:</h4>
<p>I watched the video demonstration of Prism and I find it funny that they're actually using something category theory related in their demonstration: they ask their model to insert a random pullback diagram into a paper on black holes.</p>



<a name="570659239"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/570659239" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#570659239">(Jan 28 2026 at 21:02)</a>:</h4>
<p>A sign of how it's going to be used, no doubt... :-/</p>



<a name="573379721"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573379721" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573379721">(Feb 11 2026 at 20:21)</a>:</h4>
<p><a href="https://www.youtube.com/watch?v=fbP8N44s3Ao">https://www.youtube.com/watch?v=fbP8N44s3Ao</a> not a paper, but gives off AI slop vibes</p>
<div class="youtube-video message_inline_image"><a data-id="fbP8N44s3Ao" href="https://www.youtube.com/watch?v=fbP8N44s3Ao"><img src="https://uploads.zulipusercontent.net/03725c454fcecee90541163b7236ae0e46c3046b/68747470733a2f2f692e7974696d672e636f6d2f76692f666250384e34347333416f2f6d7164656661756c742e6a7067"></a></div>



<a name="573380056"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573380056" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573380056">(Feb 11 2026 at 20:23)</a>:</h4>
<p>I can't exclude some CT is mentioned before, but "Kan extensions", "ends and coends" appear at around minute 20 and it's absolute nonsense</p>



<a name="573380154"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573380154" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573380154">(Feb 11 2026 at 20:23)</a>:</h4>
<p><a href="/user_uploads/21317/fpBScNFfoamiQw2jvfiztMWD/image.png">image.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/fpBScNFfoamiQw2jvfiztMWD/image.png" title="image.png"><img data-original-content-type="image/png" data-original-dimensions="1303x256" src="/user_uploads/thumbnail/21317/fpBScNFfoamiQw2jvfiztMWD/image.png/840x560.webp"></a></div>



<a name="573382081"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573382081" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573382081">(Feb 11 2026 at 20:35)</a>:</h4>
<p>"Unified field theory" like that is an immediate flag</p>



<a name="573382268"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573382268" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573382268">(Feb 11 2026 at 20:36)</a>:</h4>
<p>yep</p>



<a name="573394130"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573394130" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573394130">(Feb 11 2026 at 21:54)</a>:</h4>
<p>Even when the author was Einstein, "unified field theory" is a big red flag - none of that stuff of his worked, or even could have.  For everyone else who uses that term, it's even worse.</p>



<a name="573394944"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573394944" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573394944">(Feb 11 2026 at 22:00)</a>:</h4>
<p>I wonder if there's any clever sense in which coends are left adjoint to ends? That's certainly not normally how it works.</p>



<a name="573401113"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573401113" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573401113">(Feb 11 2026 at 22:41)</a>:</h4>
<p>Or that those formulae describe functors? Or that the (co)end formula on either side even makes sense?</p>



<a name="573462914"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573462914" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573462914">(Feb 12 2026 at 08:44)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/573394944">said</a>:</p>
<blockquote>
<p>I wonder if there's any clever sense in which coends are left adjoint to ends? That's certainly not normally how it works.</p>
</blockquote>
<p>at face value the functor <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>↦</mo><msup><mo>∫</mo><mi>C</mi></msup><mi>T</mi></mrow><annotation encoding="application/x-tex">T\mapsto \int^C T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6943em;vertical-align:-0.011em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">↦</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3424em;vertical-align:-0.3061em;"></span><span class="mop"><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0006em;">∫</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0362em;"><span style="top:-3.2579em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>↦</mo><msub><mo>∫</mo><mi>C</mi></msub><mi>T</mi></mrow><annotation encoding="application/x-tex">T\mapsto \int_C T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6943em;vertical-align:-0.011em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">↦</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1608em;vertical-align:-0.3558em;"></span><span class="mop"><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0006em;">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1225em;"><span style="top:-2.3442em;margin-left:-0.1945em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3558em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span> are parallel, so they cannot be adjoint. One might suspect that, similarly to what happens for colimit <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊣</mo></mrow><annotation encoding="application/x-tex">\dashv</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mrel">⊣</span></span></span></span> constant <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊣</mo></mrow><annotation encoding="application/x-tex">\dashv</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mrel">⊣</span></span></span></span> limit, we should have coend <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊣</mo></mrow><annotation encoding="application/x-tex">\dashv</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mrel">⊣</span></span></span></span> constant (?) <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊣</mo></mrow><annotation encoding="application/x-tex">\dashv</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mrel">⊣</span></span></span></span> end, but this is not the case. The coend is left adjoint to a functor <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>, and the end is right adjoint to a functor <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>, but <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>≇</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\not\cong B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span></span><span class="base"><span class="strut" style="height:0.589em;"></span><span class="mrel">≅</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>, nor does <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊣</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\dashv B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⊣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>.</p>
<p>There is a sense in which a functor resembling “taking the coend” is left adjoint to a functor that itself has a further right adjoint, and this latter functor resembles “taking an end.” Working this out required some effort a couple of years ago, and I can elaborate if you are interested. I suspect the author of the video does not know the definition of coend — for instance, what is called “a coend” there does not even typecheck (there is no dependence on two <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>’s in the integral) — and I would be more inclined to interpret it as a directed integral. Which, probably, they do not know the proper definition of either...</p>



<a name="573477504"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573477504" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Josselin Poiret <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573477504">(Feb 12 2026 at 09:56)</a>:</h4>
<p><span class="user-mention silent" data-user-id="282822">fosco</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/573462914">said</a>:</p>
<blockquote>
<p>there is no dependence on two <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>’s in the integral</p>
</blockquote>
<p>there does not need to be a dependence on two <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex"> x </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> in the integral for it to be a coend, although it just reduces to a colimit in that case.</p>



<a name="573480591"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573480591" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573480591">(Feb 12 2026 at 10:11)</a>:</h4>
<p>well, then call it a colimit</p>



<a name="573520699"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/573520699" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> JR <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#573520699">(Feb 12 2026 at 13:26)</a>:</h4>
<p>AI;DR</p>



<a name="574919052"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/574919052" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#574919052">(Feb 20 2026 at 11:26)</a>:</h4>
<p>I don't even know what this is trying to be or say: <a href="https://arxiv.org/abs/2602.17160">https://arxiv.org/abs/2602.17160</a></p>



<a name="574950767"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/574950767" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#574950767">(Feb 20 2026 at 14:17)</a>:</h4>
<p>I'll put my neck on the line: I am confident that this submission is not AI-generated, and was in fact written without any AI assistance.</p>
<p>One <em>can</em> in fact get the main idea by reading the writeup. A decade ago Michael Robinson already had the same idea and actually worked it out in detail. Ironically, something an LLM would have pointed out if one had been used to draft the paper!</p>



<a name="574950901"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/574950901" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#574950901">(Feb 20 2026 at 14:17)</a>:</h4>
<p>It still probably doesn't belong on the arXiv, mostly because it's not really a preprint but a short "I noticed something / had an idea" writeup. But these kinds of submissions used to get through moderation long before AI even became a concern.</p>



<a name="575031228"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575031228" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575031228">(Feb 20 2026 at 21:32)</a>:</h4>
<p>Yeah, I think I agree. Some of the short content-free sections though are the kind of output AI gives, but usually it's more verbose.</p>



<a name="575062759"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575062759" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575062759">(Feb 21 2026 at 05:36)</a>:</h4>
<p>In my view, the primary criterion is a manuscript’s quality, not whether it was produced with AI. Authors are accountable for their publications regardless of the tools or methods they use. Trivial or erroneous work, whether AI-assisted or not, should harm an author’s reputation, while papers that genuinely advance the state of the art deserve recognition, regardless of how they were produced.</p>
<p>Remark: Note that this point of view excludes plagiarism, because it does not advance the state of the art.</p>



<a name="575086244"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575086244" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575086244">(Feb 21 2026 at 13:01)</a>:</h4>
<p><span class="user-mention silent" data-user-id="1030017">José Manuel Rodríguez Caballero</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/575062759">said</a>:</p>
<blockquote>
<p>In my view, the primary criterion is a manuscript’s quality, not whether it was produced with AI. Authors are accountable for their publications regardless of the tools or methods they use.</p>
</blockquote>
<p>I for one agree with this.</p>
<p>This thread is mostly about a specific phenomenon where LLM-generated content started defeating the heuristics that preprint archive moderators previously used to exclude crackpot papers. Last year, when <span class="user-mention" data-user-id="276422">@David Michael Roberts</span> created this thread, we had such a deluge of crackpot papers getting through moderation that they outnumbered good preprints in the daily postings of certain categories on the arXiv!</p>
<p>This thread allowed us to coordinate, discuss and report on this phenomenon. The arXiv eventually deployed some countermeasures, so the thread is a bit slower these days, but the purpose is still the same.</p>



<a name="575123672"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575123672" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575123672">(Feb 22 2026 at 00:37)</a>:</h4>
<p>In response to the ongoing challenge of LLM-generated crackpot papers bypassing static moderation heuristics, we should shift our defense from evaluating the submitted document to evaluating the author's actual comprehension. To filter these bad-faith submissions, the arXiv could implement a dynamic, LLM-moderated proof-of-knowledge oral examination that builds directly upon the remote proctoring infrastructure already normalized by the online Canadian citizenship test. Just as the Canadian government leverages webcam monitoring, identity verification, and strict time limits to secure a high-stakes remote exam, this academic protocol would escalate that exact technological precedent into a continuous audio-visual feed where the author's hands must remain explicitly visible to prevent unauthorized AI assistance.</p>
<p>When an author attempts to upload a preprint, an efficient model would parse the paper's claimed domain to generate ten foundational, open-ended questions. For example, the system might ask the candidate to explain why the Kullback-Leibler divergence fails to be a true distance metric. Given a strict three-minute window per question, the author must verbally defend their conceptual reasoning directly to the camera. This setup substitutes the citizenship exam's static format and human fallback interviews with a dynamic, real-time challenge that automated generators cannot fake.</p>
<p>Instead of relying on human moderators, a real-time LLM would transcribe the spoken answers and evaluate them against established academic consensus. Authors who successfully demonstrate their internalized domain knowledge by passing a fifty percent accuracy threshold would automatically receive cryptographic submission privileges to finalize their upload. This solution scales efficiently without adding human administrative overhead, neutralizing bad-faith submissions by targeting the core inability of automated tools to perform a real-time conceptual defense under pressure. To see how this existing proctoring technology functions in practice, you can view this <a href="https://www.youtube.com/watch?v=EBIf0YuXzyA">overview on preparing for the online Canadian citizenship test</a>, which details the real-world application of webcam monitoring and identity verification procedures.</p>
<div class="youtube-video message_inline_image"><a data-id="EBIf0YuXzyA" href="https://www.youtube.com/watch?v=EBIf0YuXzyA"><img src="https://uploads.zulipusercontent.net/3ecbae198c130357bba0df8254121740121f5f09/68747470733a2f2f692e7974696d672e636f6d2f76692f45424966305975587a79412f6d7164656661756c742e6a7067"></a></div>



<a name="575124559"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575124559" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575124559">(Feb 22 2026 at 00:58)</a>:</h4>
<p>That seems like A Modest Proposal....</p>



<a name="575144048"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575144048" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575144048">(Feb 22 2026 at 07:26)</a>:</h4>
<p>I get the Swift allusion, but comparing this to the Canadian citizenship test is a serious critique of our gatekeeping norms. If we accept remote proctoring as a legitimate and fair way to grant something as fundamental as national citizenship, why is applying that same rigor to academic publishing considered satire?</p>
<p>The arXiv endorsement system is built entirely on social capital and networking privilege. A dynamic proof-of-knowledge exam shifts the burden from <em>who you know</em> to <em>what you understand</em>, replacing an opaque social hierarchy with transparent meritocracy. Calling an exam "ridiculous" just highlights how much we rely on social gatekeeping instead of objective competence.</p>
<p>Consider: Srinivasa Ramanujan, Évariste Galois, János Bolyai, Oliver Heaviside, Sophie Germain, George Boole, and Ludwig Schläfli.</p>
<p><strong>What do they all have in common?</strong> Poor social capital.</p>
<p>Under the current system, who would have endorsed them to submit to arXiv?</p>



<a name="575151928"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575151928" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575151928">(Feb 22 2026 at 10:11)</a>:</h4>
<p>When I was a math student, I received an endorsement to submit papers to arXiv that I honestly hadn’t earned (according to my current opinion). Because of that, I ended up posting a lot of low-quality work that was accepted. Later, as I matured as a mathematician, I realized how weak those preprints were and asked for them to be withdrawn. Even today, I asked to withdraw three preprints.</p>
<p>For that reason, I think having some kind of online test of basic math or physics knowledge for the author before submitting to arXiv could help authors better understand where they stand, especially in terms of the Dunning–Kruger effect. I’m not ashamed to mention this effect, because it happens to all students at some point. It’s a normal part of the learning process, and it’s important to keep it in mind when thinking about submitting preprints.</p>



<a name="575189989"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575189989" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575189989">(Feb 22 2026 at 20:33)</a>:</h4>
<p><a href="/user_uploads/21317/JBw0jJg7VbEKp43orHbpnW6D/Screenshot-2026-02-22-at-15-34-43-AH-178872-arXiv-important-notification-regarding-submit_7290117-josephcmacgmail.com-Gmail.png">Screenshot 2026-02-22 at 15-34-43 AH-178872 arXiv - important notification regarding submit_7290117 - josephcmac@gmail.com - Gmail.png</a></p>
<div class="message_inline_image"><a href="/user_uploads/21317/JBw0jJg7VbEKp43orHbpnW6D/Screenshot-2026-02-22-at-15-34-43-AH-178872-arXiv-important-notification-regarding-submit_7290117-josephcmacgmail.com-Gmail.png" title="Screenshot 2026-02-22 at 15-34-43 AH-178872 arXiv - important notification regarding submit_7290117 - josephcmac@gmail.com - Gmail.png"><img data-original-content-type="image/png" data-original-dimensions="575x567" src="/user_uploads/thumbnail/21317/JBw0jJg7VbEKp43orHbpnW6D/Screenshot-2026-02-22-at-15-34-43-AH-178872-arXiv-important-notification-regarding-submit_7290117-josephcmacgmail.com-Gmail.png/840x560.webp"></a></div><p>I asked arXiv to withdraw three preprints because I believed they were not written with sufficient rigor (all beyond my field of expertise), but their support team declined to remove them. I wanted to let you know that any low-quality material you see there under my name in arXiv is because the platform would not accept my request to withdraw it. I wish to correct my mistakes in good faith.</p>



<a name="575196390"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575196390" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> fosco <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575196390">(Feb 22 2026 at 22:13)</a>:</h4>
<p>The practice to withdraw the paper is discouraged, but updating the current version with a revised one does not require any previous authorization. This is what the arXiv is for, after all, sharing works that might be in progress...</p>



<a name="575201161"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575201161" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575201161">(Feb 22 2026 at 23:17)</a>:</h4>
<p>The false negative problem, of excellent authors unable to get an ArXiv endorsement, is as far as I can tell very rare, though of course one can hypothesize there are many Ramanujans falling through the cracks. The AI exam you suggest would be more relevant for preventing false positives, but seems dehumanizing to legitimate posters, extremely burdensome to those tasked with designing, deploying, and supporting it, and vulnerable to a partly disjoint set of false positives and negatives from the current system. I think most ArXiv authors would vigorously oppose it.</p>



<a name="575207287"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575207287" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575207287">(Feb 23 2026 at 01:00)</a>:</h4>
<p><span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/575201161">said</a>:</p>
<blockquote>
<p>...seems dehumanizing to legitimate posters, extremely burdensome to those tasked with designing, deploying, and supporting it, and vulnerable to a partly disjoint set of false positives and negatives from the current system. I think most ArXiv authors would vigorously oppose it.</p>
</blockquote>
<p>I couldn't agree more, hence my earlier comment. What next, AI-driven video-interview submission checks to journals? No doubt powered by outsourcing to companies like Palantir....</p>



<a name="575209516"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575209516" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575209516">(Feb 23 2026 at 01:15)</a>:</h4>
<p><span class="user-mention" data-user-id="282822">@fosco</span> Some preprints can’t reasonably be updated because they are fundamentally mistaken or trivial. Allowing authors to delete such preprints gives them a public way to acknowledge their error. A “crank” is someone who can’t admit they were wrong. So the first step toward reducing crank submissions on arXiv is to let authors remove work they no longer stand behind, or, if they choose to keep it, require them to add a clear notice that the author no longer supports the results.</p>



<a name="575210941"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575210941" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575210941">(Feb 23 2026 at 01:27)</a>:</h4>
<p>You can replace a paper on the arXiv by the empty set, you know. Even if you can't delete the entire entry.</p>



<a name="575211650"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575211650" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575211650">(Feb 23 2026 at 01:37)</a>:</h4>
<p><a href="/user_uploads/21317/Rwf3dMZn3qBvpGngL9EcG4T6/Screenshot-2026-02-22-at-20-38-15-view.png">Screenshot 2026-02-22 at 20-38-15 view.png</a><br>
<span class="user-mention" data-user-id="276422">@David Michael Roberts</span></p>
<div class="message_inline_image"><a href="/user_uploads/21317/Rwf3dMZn3qBvpGngL9EcG4T6/Screenshot-2026-02-22-at-20-38-15-view.png" title="Screenshot 2026-02-22 at 20-38-15 view.png"><img data-original-content-type="image/png" data-original-dimensions="990x718" src="/user_uploads/thumbnail/21317/Rwf3dMZn3qBvpGngL9EcG4T6/Screenshot-2026-02-22-at-20-38-15-view.png/840x560.webp"></a></div><p>I’ve done it. I replaced the preprint with an empty file (except for a message telling that I do not endorse anymore that preprint) since I couldn’t figure out how to delete the entire entry. I’m hoping they’ll treat it as a violation of the site’s rules and, as a result, remove all my contributions and delete my account. I really don’t like the idea of arXiv keeping something I wrote without my consent.</p>



<a name="575212304"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575212304" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575212304">(Feb 23 2026 at 01:45)</a>:</h4>
<p><span class="user-mention" data-user-id="609515">@Kevin Carlson</span> </p>
<blockquote>
<p>The AI exam you suggest would be more relevant for preventing false positives, but seems dehumanizing to legitimate posters, extremely burdensome to those tasked with designing, deploying, and supporting it, and vulnerable to a partly disjoint set of false positives and negatives from the current system.</p>
</blockquote>
<p>I didn’t feel dehumanized when I took the online exam for Canadian citizenship. I can imagine an elderly professor submitting a paper and being asked by the platform to complete a short exam. It might take him just ten minutes and feel completely trivial because he has the knowledge and experience. But for someone relying on AI and lacking the underlying skills, it wouldn’t be trivial at all.</p>



<a name="575213797"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575213797" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575213797">(Feb 23 2026 at 02:04)</a>:</h4>
<p><span class="user-mention silent" data-user-id="1030017">José Manuel Rodríguez Caballero</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/575211650">said</a>:</p>
<blockquote>
<p>I’ve done it. I replaced the preprint with an empty file (except for a message telling that I do not endorse anymore that preprint) since I couldn’t figure out how to delete the entire entry. </p>
</blockquote>
<p>Did you follow the instructions here: <a href="https://info.arxiv.org/help/withdraw.html#article-has-been-announced">https://info.arxiv.org/help/withdraw.html#article-has-been-announced</a> ? I don't mean replace by an empty document, I mean literally replace with the empty set. There's no document at all if you do it properly. See eg <a href="https://www.arxiv.org/abs/2512.23436v2">https://www.arxiv.org/abs/2512.23436v2</a> (to give a random paper I just found). You can also blank the abstract as well, and edit the title to be something boring and untraceable, like [paper retracted]</p>



<a name="575213873"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575213873" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575213873">(Feb 23 2026 at 02:05)</a>:</h4>
<p><span class="user-group-mention" data-user-group-id="2511">@moderators</span> can the discussion about arXiv withdrawal here please be moved to a different thread?</p>



<a name="575214175"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575214175" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575214175">(Feb 23 2026 at 02:09)</a>:</h4>
<p><span class="user-mention" data-user-id="276422">@David Michael Roberts</span>  Thanks, I am reading the link. The question of removing AI-generated articles from arXiv is relevant here (not my case). More specifically, does arXiv allow the easy deletion of these articles?</p>



<a name="575214215"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575214215" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575214215">(Feb 23 2026 at 02:09)</a>:</h4>
<blockquote>
<p>Articles that have been announced and made public cannot be completely removed. A withdrawal creates a new version of the paper marked as withdrawn.</p>
</blockquote>



<a name="575214320"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575214320" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575214320">(Feb 23 2026 at 02:11)</a>:</h4>
<p>In my case, they don't even accept that for some preprints for which I provided the reason: "These preprints were not developed with enough rigor". (that is normal for students, this is why, I think, to be endorsed to ArXiv, candidate should have some mathematical maturity).</p>



<a name="575214384"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575214384" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575214384">(Feb 23 2026 at 02:12)</a>:</h4>
<p>Is the online exam for Canadian citizenship proctored by AI?</p>



<a name="575214643"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575214643" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575214643">(Feb 23 2026 at 02:16)</a>:</h4>
<p>The online Canadian citizenship test is not proctored by a live human or advanced AI tracking eye movements, but it is strictly monitored using an automated webcam system. Before you begin, the system requires you to take a photo of your face and your ID, and then it automatically captures random snapshots of you while you take the exam. Immigration, Refugees and Citizenship Canada (IRCC) reviews these photos after the test to verify your identity and ensure you are completely alone, your face is fully visible, and you are not using outside study materials or other devices. The testing platform also monitors your browser activity to prevent the use of other tabs, programs, or VPNs, and any suspicious behavior or webcam issues will cause your test to be flagged for a manual review by a human officer.<br>
<a href="https://www.citizenshipsupport.ca/canadian-citizenship/canadian-citizenship-test-ircc-new-online-test">https://www.citizenshipsupport.ca/canadian-citizenship/canadian-citizenship-test-ircc-new-online-test</a></p>



<a name="575214697"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575214697" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575214697">(Feb 23 2026 at 02:17)</a>:</h4>
<p>The point I made about dehumanization was in regard more to having a large language model assess my mathematical expertise.</p>



<a name="575214796"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575214796" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575214796">(Feb 23 2026 at 02:19)</a>:</h4>
<p>The test I am proposing is not intended to measure your mathematical skills, but to demonstrate that your mathematical skills exceed a certain threshold allowing you to know what you are talking about. For example, if you want to submit a paper on analytic number theory, the question what is zeta(2)/pi^2 is easy and relevant.</p>



<a name="575214845"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575214845" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575214845">(Feb 23 2026 at 02:19)</a>:</h4>
<p>Yes, lower bounding your mathematical skills is an example of measuring them.</p>



<a name="575214874"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575214874" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575214874">(Feb 23 2026 at 02:20)</a>:</h4>
<p>A lower bound is not a complete estimation.</p>



<a name="575214885"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575214885" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575214885">(Feb 23 2026 at 02:20)</a>:</h4>
<p>it is a minimum requirement</p>



<a name="575215021"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575215021" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ben Eltschig <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575215021">(Feb 23 2026 at 02:22)</a>:</h4>
<p>I think the point still stands that having to prove that you know what you're talking about to the same kind of system that made this higher level of scrutiny necessary to begin with would not exactly be a great experience</p>



<a name="575215100"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575215100" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575215100">(Feb 23 2026 at 02:23)</a>:</h4>
<p>What is your solution?</p>



<a name="575215552"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575215552" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575215552">(Feb 23 2026 at 02:29)</a>:</h4>
<p>The ArXiv has already rescinded the policy of auto-endorsement for submissions of Math papers from academic email addresses, which means submitters must now be endorsed by an existing author with a bit of “credit” on ArXiv, as discussed above. This seems to have substantially alleviated the problem.</p>



<a name="575216191"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575216191" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575216191">(Feb 23 2026 at 02:37)</a>:</h4>
<p>Consequence: <strong>Endorsement Fatigue</strong><br>
Existing authors with "credit" on arXiv often find themselves inundated with cold emails from desperate, unknown researchers begging for endorsements.</p>



<a name="575216465"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575216465" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575216465">(Feb 23 2026 at 02:42)</a>:</h4>
<p>I’ve gotten something like two in my life. This is a problem that’s largely constrained to people like John who are very well-known, and it’s not that hard to handle with a policy of no endorsements for unknown requesters.</p>



<a name="575219767"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575219767" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Valeria de Paiva <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575219767">(Feb 23 2026 at 03:33)</a>:</h4>
<p>hi <span class="user-mention" data-user-id="609515">@Kevin Carlson</span> , I think you're underestimating the problem here </p>
<blockquote>
<p>The false negative problem, of excellent authors unable to get an ArXiv endorsement, is as far as I can tell very rare</p>
</blockquote>
<p>To give you an example, I have publishing papers since before the arxiv was founded (do you know about Hypatia?) and still the arxiv did not want to publish a paper of mine recently, with a new student, goodness knows why. There's an awful of privilege that accrues from having an academic American email.</p>
<p>On the other hand I do agree with you that a test, as suggested by @Jose Manuel, is not a good thing. We want more people, more Ramanujans doing maths, not less.</p>



<a name="575220455"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575220455" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575220455">(Feb 23 2026 at 03:41)</a>:</h4>
<p>I think someone like Srinivasa Ramanujan could easily pass an online exam in number theory. It wouldn’t even need to be especially difficult, just basic material, like asking which type of partial differential equation the theta function satisfies: elliptic, parabolic, or hyperbolic.</p>



<a name="575220547"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575220547" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575220547">(Feb 23 2026 at 03:43)</a>:</h4>
<p>In fact, in India there are many bright young people who have no connection to the academic world.</p>



<a name="575220648"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575220648" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575220648">(Feb 23 2026 at 03:44)</a>:</h4>
<p>What should a new Ramanujan do? Send a cold email requesting an endorsement to use ArXiv, or simply pass an online exam to demonstrate his skills?</p>



<a name="575221133"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575221133" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Valeria de Paiva <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575221133">(Feb 23 2026 at 03:51)</a>:</h4>
<p>I'm sure someone like Ramanujan could easily pass an online exam. I do not think they, or anyone else for that matter,  should need to.</p>



<a name="575221369"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575221369" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575221369">(Feb 23 2026 at 03:54)</a>:</h4>
<p>So how do we solve the problem of some people submitting AI-generated articles whose context they don't understand?</p>



<a name="575222942"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575222942" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575222942">(Feb 23 2026 at 04:18)</a>:</h4>
<p><span class="user-mention" data-user-id="276422">@David Michael Roberts</span> </p>
<blockquote>
<p>What next, AI-driven video-interview submission checks to journals? No doubt powered by outsourcing to companies like Palantir....</p>
</blockquote>
<p>There is no need to AI-driven video-interview submission checks to journals, because of peer review. However, the lack of peer review on ArXiv makes it vulnerable to exploitation by cranks. Incidentally, I've participated in several AI-powered video interviews for jobs in Canada: this is the future.</p>



<a name="575228350"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575228350" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> John Baez <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575228350">(Feb 23 2026 at 05:40)</a>:</h4>
<p><span class="user-mention silent" data-user-id="1030017">José Manuel Rodríguez Caballero</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/575220648">said</a>:</p>
<blockquote>
<p>What should a new Ramanujan do? Send a cold email requesting an endorsement to use ArXiv, or simply pass an online exam to demonstrate his skills?</p>
</blockquote>
<p>A new Ramanujan could contact a good number theorist just like the old Ramanujan did.   He started by saying</p>
<blockquote>
<p><em>Dear Sir,</em><br>
<em>I beg to introduce myself to you as a clerk in the Accounts Department of the Port Trust Office at Madras on a salary of only £20 per annum. I am now about 23 years of age. I have had no University education but I have undergone the ordinary school course. After leaving school I have been employing the spare time at my disposal to work at Mathematics. I have not trodden through the conventional regular course which is followed in a University course, but I am striking out a new path for myself. I have made a special investigation of divergent series in general and the results I get are termed by the local mathematicians as "startling".</em></p>
</blockquote>
<p>Then he gave some examples; then he concluded his 1-page letter by saying</p>
<blockquote>
<p><em>Being poor, if you are convinced that there is anything of value I would like to have my theorems published. I have not given the actual investigations nor the expressions that I get but I have indicated the lines on which I proceed. Being inexperienced I would very highly value any advice you give me. Requesting to be excused for the trouble I give you.</em></p>
</blockquote>
<p>Then he attached 11 more pages of results, some of which startled Hardy (and Littlewood).</p>
<p>I get lots of emails like this, about 3-7 a week - mostly not about number theory.   Unfortunately almost all of them contain results that are either nonsense or just not very difficult.    I encourage the ones who seem to show promise, but ignore most of them.</p>



<a name="575228429"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575228429" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575228429">(Feb 23 2026 at 05:41)</a>:</h4>
<p>Besides, these days there are of course tens of thousands of Indian academics well-connected to the global research world.</p>



<a name="575228678"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575228678" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575228678">(Feb 23 2026 at 05:45)</a>:</h4>
<p><span class="user-mention silent" data-user-id="276656">Valeria de Paiva</span> <a href="#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/575219767">said</a>:</p>
<blockquote>
<p>hi <span class="user-mention silent" data-user-id="609515">Kevin Carlson</span> , I think you're underestimating the problem here </p>
<blockquote>
<p>The false negative problem, of excellent authors unable to get an ArXiv endorsement, is as far as I can tell very rare</p>
</blockquote>
<p>To give you an example, I have publishing papers since before the arxiv was founded (do you know about Hypatia?) and still the arxiv did not want to publish a paper of mine recently, with a new student, goodness knows why. There's an awful of privilege that accrues from having an academic American email.</p>
</blockquote>
<p>Hi Valeria, well, that does sound worrying, and I don’t really understand ArXiv well enough to know how it’s at all possible unless you were publishing in a new area. But of course you aren’t a good example of a researcher who would have trouble getting an endorsement from an established researcher! So I’m not sure this lies exactly inside the false negative problem I was sketching, rather than the general problem that <em>any</em> mechanical filtering process will inevitably make a range of errors which will (perhaps not inevitably?) be subject to certain biases in favor of the actors considered central by the designers of the system. I hope you two got the paper up without too much mess.</p>



<a name="575419906"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575419906" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> José Manuel Rodríguez Caballero (he/him/his) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575419906">(Feb 23 2026 at 23:50)</a>:</h4>
<p><span class="user-mention" data-user-id="275920">@John Baez</span> wrote:</p>
<blockquote>
<p>I get lots of emails like this, about 3-7 a week - mostly not about number theory. Unfortunately almost all of them contain results that are either nonsense or just not very difficult. I encourage the ones who seem to show promise, but ignore most of them.</p>
</blockquote>
<p>A hypothetical Gmail extension (empowered by Gemini Pro) could automate technical screening by triggering an auto-response containing a fundamental competency test for incoming inquiries. Any sender who fails to provide a correct answer within one week would have their original message automatically deleted. For instance, an inability to compute the dimension of the real Lie algebra su(23) would serve as a proxy for the sender's capacity to discuss a Theory of Everything. This functionality would streamline inbox management by filtering out unqualified submissions before they require manual review. Automated encouragement emails are also an option.</p>
<p>For example: A friend claimed to have proven the Riemann hypothesis. When asked to provide the proof formalized in Isabelle/HOL or Coq, he complained that learning those languages was too difficult. My response was simple: for a mind truly capable of solving the Riemann hypothesis, mastering a proof assistant should be trivial.</p>



<a name="575499876"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575499876" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Morgan Rogers (he/him) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575499876">(Feb 24 2026 at 10:02)</a>:</h4>
<p><a href="https://arxiv.org/abs/2602.18033">This one</a> got a high score in my arXiv digest. The main "independence theorem" is mostly nonsense. :(</p>



<a name="575507377"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575507377" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David Michael Roberts <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575507377">(Feb 24 2026 at 10:36)</a>:</h4>
<p><span class="user-mention" data-user-id="277473">@Morgan Rogers (he/him)</span>  <a class="message-link" href="/#narrow/channel/229111-community.3A-general/topic/AI-generated.20papers/near/574919052">#community: general &gt; AI-generated papers @ 💬</a>  was originally pointing to the author's previous preprint. And there's another one today. That's three preprints in two days....</p>



<a name="575625339"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575625339" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Kevin Carlson <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575625339">(Feb 24 2026 at 19:29)</a>:</h4>
<p>This one's nice insofar as from the abstract it's immediately clear that the author is claiming to do something that is a category error.</p>



<a name="575690711"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575690711" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Zoltan A. Kocsis (Z.A.K.) <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575690711">(Feb 25 2026 at 04:33)</a>:</h4>
<p>Oh no <span aria-label="rage" class="emoji emoji-1f621" role="img" title="rage">:rage:</span> I'm fairly sure that this one's also not ai-generated, but it's embarrassing bullshit nonetheless, to the extent that it might as well be.</p>



<a name="575717476"></a>
<h4><a href="https://categorytheory.zulipchat.com#narrow/stream/229111-community%3A%20general/topic/AI-generated%20papers/near/575717476" class="zl"><img src="https://mattecapu.github.io/ct-zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Nathanael Arkor <a href="https://mattecapu.github.io/ct-zulip-archive/stream/229111-community.3A-general/topic/AI-generated.20papers.html#575717476">(Feb 25 2026 at 08:43)</a>:</h4>
<p><span class="user-mention" data-user-id="510824">@Zoltan A. Kocsis (Z.A.K.)</span>: I'm curious what gives you so much confidence it's not LLM-generated?</p>



<footer class="site-footer">

<hr><p>Last updated: Feb 28 2026 at 12:12 UTC</p>
This archive runs on a customization of <a href="https://github.com/zulip/zulip-archive">zulip-archive</a>
</footer>
</body>

</html>